<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Matt Pharr's Homepage</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.png" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Matt Pharr</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" width="307" height="307" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#pbr">PBR Book</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#papers">Papers</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link" href="blog">Blog</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0"><div class="text-primary">Matt Pharr</div></h1>
                    <div class="subheading mb-5">
                        <e-mail><a href="mailto:matt@pharr.org">matt@pharr.org</a></e-mail>
                    </div>
                    <p class="lead mb-3">I am a distinguished research scientist at NVIDIA, where I am working on a variety of projects related to path tracing on GPUs.</p>
                    <p class="lead mb-3">
                        My book on rendering,
                        <a href="https://pbrt.org">Physically Based Rendering: From Theory to Implementation,</a>
                        is widely used in university courses and by graphics researchers and developers. Greg Humphreys, Pat Hanrahan, and I were awarded a
                        <a href="https://www.oscars.org/sci-tech/ceremonies/2014">Scientific and Technical Academy Award</a>
                        in recognition of the book's impact on CGI in movies—never before has a book received an Academy Award. The third edition of the book was released in the Fall of 2016 and the fourth edition was released on March 28, 2023.
                    </p>
                    <p class="lead mb-3">I have previously worked at Google, co-founded Neoptica (which was acquired by Intel), and co-founded Exluna (which was acquired by NVIDIA). I have a Ph.D. from the Stanford Graphics Lab under the supervision of Pat Hanrahan and a B.S. in Computer Science from Yale.</p>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="pbr">
                <div class="resume-section-content">
                    <h2 class="mb-5">PBR Book</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <p>
                                Greg Humphreys, Wenzel Jakob, and I are the authors of the book
                                <i>Physically Based Rendering: From Theory to Implementation.</i>
                                The book dates to the late 1990s, when Greg and I were graduate students and Pat Hanrahan suggested that we write a book on how to write a ray tracer. He offered to use it in the rendering class he was teaching at Stanford and Greg and I agreed, having no idea what we were getting ourselves into. The manuscript greatly benefitted from both Pat's approach to rendering as well as being battle tested in the course. The first edition was published in 2004 and the second edition came out in 2010. Wenzel joined for the third edition in 2016, and the fourth edition was recently released.
                            </p>
                            <span class="d-lg-none">
                                <div class="text-center">
                                    <a href="assets/img/cover-full.jpg"><img class="img-fluid img-paper" src="assets/img/cover.jpg" alt="" width="384" height="495" /></a>
                                </div>
                            </span>
                            <p>
                                The book is distinguished by being a
                                <i>literate program.</i>
                                Literate programming is an approach invented by Donald Knuth where programs are broken down into short sections of code that are intermixed with text describing them. Writing the book in this way has allowed us to start from the theory and mathematics of rendering and go all the way to its implementation. Doing so makes it possible to discuss subtle implementation issues that aren't often mentioned in papers, for example, and also lets us say something about the design and structure of a moderately complex software system.
                                <tt>pbrt,</tt>
                                the rendering system described in the book, is capable of rendering highly realistic imagery, such as the image shown here.
                            </p>
                            <span class="d-lg-none">
                                <div class="text-center">
                                    <a href="assets/img/kitchen.png"><img class="img-fluid img-paper" src="assets/img/kitchen-thumb.jpg" alt="" width="384" height="240" /></a>
                                </div>
                            </span>
                            <p>
                                As its title suggests, the book's focus is on physically-based approaches to rendering. It implements a ray-optics based simulation of light transport and scattering; all of the light transport algorithms it describes are based on Monte Carlo path tracing. For more information, see both
                                <a href="https://pbrt.org">the book Website,</a>
                                as well as the
                                <a href="http://pbr-book.org">online verision of the book.</a>
                            </p>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div>
                                    <a href="assets/img/cover-full.jpg"><img class="img-fluid img-paper" src="assets/img/cover.jpg" alt="" width="384" height="495" /></a>
                                </div>
                                <div>
                                    <a href="assets/img/kitchen.png"><img class="img-fluid img-paper" src="assets/img/kitchen-thumb.jpg" alt="" width="384" height="240" /></a>
                                </div>
                            </span>
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Papers-->
            <section class="resume-section" id="papers">
                <div class="resume-section-content">
                    <h2 class="mb-5">Papers</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">
                                SZ Sequences: Binary-Constructed (0,2
                                <sup>q</sup>
                                )-Sequences
                            </h3>
                            <div class="authors">Abdalla G. M. Ahmed, Matt Pharr, Victor Ostromoukhov, and Hui Huang</div>
                            <div>ACM Transactions on Graphics (SIGGRAPH Asia) 2025</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/sz-sequences.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/sz-sequences-supplemental.pdf">[Supplemental]</a>
                                <a href="https://arxiv.org/abs/2505.20434">[arXiv]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/sz-sequences.png" alt="" width="344" height="344" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Low-discrepancy sequences have seen widespread adoption in computer graphics thanks to the superior rates of convergence that they provide. Because rendering integrals often are comprised of products of lower-dimensional integrals, recent work has focused on developing sequences that are also well-distributed in lower-dimensional projections. To this end, we introduce a novel construction of binary-based (0, 4)-sequences; that is, progressive fully multi-stratified sequences of 4D points, and extend the idea to higher power-of-two dimensions. We further show that not only it is possible to nest lower-dimensional sequences in higher-dimensional ones—for example, embedding a (0, 2)-sequence within our (0, 4)-sequence—but that we can ensemble two (0, 2)-sequences into a (0, 4)-sequence, four (0, 4)-sequences into a (0, 16)-sequence, and so on. Such sequences can provide excellent rates of convergence when integrals include lower-dimensional integration
                                problems in 2, 4, 16,… dimensions. Our construction is based on using 2×2 block matrices as symbols to construct larger matrices that potentially generate a sequence with the target (0,
                                <i>s</i>
                                )-sequence in base
                                <i>s</i>
                                property. We describe how to search for suitable alphabets and identify two distinct, cross-related alphabets of block symbols, which we call
                                <i>s</i>
                                and
                                <i>z</i>
                                , hence
                                <i>SZ</i>
                                for the resulting family of sequences. Given the alphabets, we construct candidate generator matrices and search for valid sets of matrices. We then infer a simple recurrence formula to construct full-resolution (64-bit) matrices. Because our generator matrices are binary, they allow highly-efficient implementation using bitwise operations and can be used as a drop-in replacement for Sobol matrices in existing applications. We compare SZ sequences to state-of-the-art low discrepancy sequences, and demonstrate mean relative squared error improvements up to 1.93× in common rendering applications.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Abdalla G. M. Ahmed, Matt Pharr, Victor Ostromoukhov, and Hui Huang. 2025. SZ Sequences: Binary-Constructed (0,2
                                <sup>q</sup>
                                )-Sequences. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 44(6).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/sz-sequences.png" alt="" width="344" height="344" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Collaborative Texture Filtering</h3>
                            <div class="authors">Tomas Akenine-Möller, Pontus Ebelin, Matt Pharr, and Bart Wronski</div>
                            <div>ACM/EG Symposium on High Performance Graphics (HPG), 2025</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/collaborative_texfilt.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/collaborative_texfilt_video1.mp4">[Video 1]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/collaborative_texfilt_video2.mp4">[Video 2]</a>
                                <a href="https://arxiv.org/abs/2506.17770">[arXiv]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/collaborative-texfilt.png" alt="" width="344" height="344" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Recent advances in texture compression provide major improvements in compression ratios, but cannot use the GPU's texture units for decompression and filtering. This has led to the development of stochastic texture filtering (STF) techniques to avoid the high cost of multiple texel evaluations with such formats. Unfortunately, those methods can give undesirable visual appearance changes under magnification and may contain visible noise and flicker despite the use of spatiotemporal denoisers. Recent work substantially improves the quality of magnification filtering with STF by sharing decoded texel values between nearby pixels (Wronski et al. 2025). Using GPU wave communication intrinsics, this sharing can be performed inside actively executing shaders without memory traffic overhead. We take this idea further and present novel algorithms that use wave communication between lanes to avoid repeated texel decompression prior to filtering. By distributing
                                unique work across lanes, we can achieve zero-error filtering using ≤1 texel evaluations per pixel given a sufficiently large magnification factor. For the remaining cases, we propose novel filtering fallback methods that also achieve higher quality than prior approaches.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Tomas Akenine-Möller, Pontus Ebelin, Matt Pharr, and Bart Wronski. 2025. Collaborative Texture Filtering. Proceedings of the ACM/EG Symposium on High Performance Graphics (HPG).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/collaborative-texfilt.png" alt="" width="344" height="344" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Improved Stochastic Texture Filtering Through Sample Reuse</h3>
                            <div class="authors">Bartlomiej Wronski, Matt Pharr, and Tomas Akenine-Möller</div>
                            <div>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D) 2025</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/stf_sample_reuse.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/stf_sample_reuse_supplemental.pdf">[Supplemental]</a>
                                <a href="https://research.nvidia.com/labs/rtr/publication/wronski2025stfreuse/stf_sample_reuse_video.mp4">[Video]</a>
                                <a href="https://arxiv.org/abs/2504.05562">[arXiv]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/quadcomm-stf.png" alt="" width="344" height="343" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Stochastic texture filtering (STF) has re-emerged as a technique that can bring down the cost of texture filtering of advanced texture compression methods, e.g., neural texture compression. However, during texture magnification, the swapped order of filtering and shading with STF can result in aliasing. The inability to smoothly interpolate material properties stored in textures, such as surface normals, leads to potentially undesirable appearance changes. We present a novel method to improve the quality of stochastically-filtered magnified textures and reduce the image difference compared to traditional texture filtering. When textures are magnified, nearby pixels filter similar sets of texels and we introduce techniques for sharing texel values among pixels with only a small increase in cost (0.04-0.14 ms per frame). We propose an improvement to weighted importance sampling that guarantees that our method never increases error beyond single-sample
                                stochastic texture filtering. Under high magnification, our method has >10 dB higher PSNR than single-sample STF. Our results show greatly improved image quality both with and without spatiotemporal denoising.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                For better or for worse, we developed significantly improved techniques for sharing texels among pixels shortly after the publication of this paper; those were published in
                                <i>Collaborative Texture Filtering</i>
                                . Nevertheless, this paper also includes a novel Monte Carlo estimator, improvements to blue noise masks for sample sharing, as well as new techniques for generating stochastic footprints for sharing among nearby pixels, all of which remain useful.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Bartlomiej Wronski, Matt Pharr, and Tomas Akenine-Möller. 2025. Improved Stochastic Texture Filtering Through Sample Reuse. Proceedings of the ACM on Computer Graphics and Interactive Techniques (ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/quadcomm-stf.png" alt="" width="344" height="343" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A Generalized Ray Formulation For Wave-Optics Light Transport</h3>
                            <div class="authors">Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Eugene d'Eon, Ling-Qi Yan, and Matt Pharr</div>
                            <div>ACM Transactions on Graphics (SIGGRAPH Asia) 2024</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/rtplt_paper.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/rtplt_lowres.pdf">[Paper (lowres)]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/rtplt_supplemental.pdf">[Supplemental]</a>
                                <a href="https://www.youtube.com/watch?v=94ZAG7o5gvY">[Teaser video]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/spaceship-lowres.png" alt="" width="344" height="177" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Ray optics is the foundation of modern path tracing and sampling algorithms for computer graphics; crucially, it allows high-performance implementations based on ray tracing. However, many applications of interest in computer graphics and computational optics demand a more precise understanding of light: as waves. For example, accurately modelling scattering effects like diffraction or interference requires a model that provides the coherence of light waves arriving at surfaces. While recent work in Physical Light Transport has introduced such a model, it requires tracing light paths starting from the light sources, which is often less efficient than tracing them from the sensor, and does not allow the use of many effective importance sampling techniques.
                            </div>
                            <div class="abstract mb-2">
                                We introduce a new model for wave optical light transport that is based on the fact that sensors aggregate the measurement of many light waves when capturing an image. This allows us to compactly represent the statistics of light waves in a
                                <em>generalized ray</em>
                                . Generalized rays allow sampling light paths starting from the sensor and applying sophisticated path tracing sampling techniques while still accurately modelling the wave nature of light. Our model is computationally efficient and straightforward to add to an existing path tracer; this offers the prospect of wave optics becoming the foundation of most renderers in the future. Using our model, we show that it is possible to render complex scenes under wave optics with high performance, which has not been possible with any existing method.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None yet!
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Eugene d'Eon, Ling-Qi Yan, and Matt Pharr. 2024. A Generalized Ray Formulation For Wave-Optics Light Transport. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2024), 43(6).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/spaceship-lowres.png" alt="" width="344" height="177" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A Free-Space Diffraction BSDF</h3>
                            <div class="authors">Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Arshiya Mollazainali, Eugene d'Eon, and Matt Pharr</div>
                            <div>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2024)</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/steinberg2024_fsd_paper.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/steinberg2024_fsd_paper_lowres.pdf">[Paper (lowres)]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/steinberg2024_fsd_supplemental.pdf">[Supplemental]</a>
                                <a href="https://github.com/ssteinberg/fsdBSDFpaper">[Code]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/diffraction.png" alt="" width="428" height="286" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Free-space diffractions are an optical phenomenon where light appears to "bend" around the geometric edges and corners of scene objects. In this paper we present an efficient method to simulate such effects. We derive an edge-based formulation of Fraunhofer diffraction, which is well suited to the common (triangular) geometric meshes used in computer graphics. Our method dynamically constructs a free-space diffraction BSDF by considering the geometry around the intersection point of a ray of light with an object, and we present an importance sampling strategy for these BSDFs. Our method is unique in requiring only ray tracing to produce free-space diffractions, works with general meshes, requires no geometry preprocessing, and is designed to work with path tracers with a linear rendering equation. We show that we are able to reproduce accurate diffraction lobes, and, in contrast to any existing method, are able to handle complex, real-world geometry.
                                This work serves to connect free-space diffractions to the efficient path tracing tools from computer graphics.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                (None).
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Arshiya Mollazainali, Eugene d'Eon, and Matt Pharr. 2024. A Free-Space Diffraction BSDF. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2024), 43(4).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/diffraction.png" alt="" width="428" height="286" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Filtering After Shading With Stochastic Texture Filtering</h3>
                            <div class="authors">Matt Pharr, Bartlomiej Wronski, Marco Salvi, and Marcos Fajardo</div>
                            <div>Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D '24)</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/stochtex.pdf">[Paper]</a>
                                <a href="https://arxiv.org/abs/2407.06107">[arXiv]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/stochtex.png" alt="" width="388" height="218" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                2D texture maps and 3D voxel arrays are widely used to add rich detail to the surfaces and volumes of rendered scenes, and filtered texture lookups are integral to producing high-quality imagery. We show that applying the texture filter after evaluating shading generally gives more accurate imagery than filtering textures before BSDF evaluation, as is current practice. These benefits are not merely theoretical, but are apparent in common cases. We demonstrate that practical and efficient filtering after shading is possible through the use of stochastic sampling of texture filters.
                            </div>
                            <div class="abstract mb-2">Stochastic texture filtering offers additional benefits, including efficient implementation of high-quality texture filters and efficient filtering of textures stored in compressed and sparse data structures, including neural representations. We demonstrate applications in both real-time and offline rendering and show that the additional error from stochastic filtering is minimal. We find that this error is handled well by either spatiotemporal denoising or moderate pixel sampling rates.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                An additional previous example of stochastic texture filtering that we didn't have in previous work: stochastic bilinear in
                                <a href="https://momentsingraphics.de/HPG2017.html">Non-linearly Quantized Moment Shadow Maps,</a>
                                by Christoph Peters in HPG 2017.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr, Bartlomiej Wronski, Marco Salvi, and Marcos Fajardo. 2024. Stochastic Texture Filtering. Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D '24).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/stochtex.png" alt="" width="388" height="218" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Decorrelating ReSTIR Samplers via MCMC Mutations</h3>
                            <div class="authors">Rohan Sawhney, Daqi Lin, Markus Kettunen, Benedikt Bitterli, Ravi Ramamoorthi, Chris Wyman, Matt Pharr</div>
                            <div>ACM Transactions on Graphics</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ReSTIRMCMC.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ReSTIRMCMC-supplemental.pdf">[Supplemental]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ReSTIRMCMC-video.m4v">[Video]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/restir-mutations.png" alt="" width="388" height="370" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Monte Carlo rendering algorithms often utilize correlations between pixels to improve efficiency and enhance image quality. For real-time applications in particular, repeated reservoir resampling offers a powerful framework to reuse samples both spatially in an image and temporally across multiple frames. While such techniques achieve equal-error up to 100× faster for real-time direct lighting [Bitterli et al. 2020] and global illumination [Ouyang et al. 2021; Lin et al. 2021], they are still far from optimal. For instance, unchecked spatiotemporal resampling often introduces noticeable correlation artifacts, while reservoirs holding more than one sample suffer from impoverishment in the form of duplicate samples. We demonstrate how interleaving Markov Chain Monte Carlo (MCMC) mutations with reservoir resampling helps alleviate these issues, especially in scenes with glossy materials and difficult-to-sample lighting. Moreover, our approach does not
                                introduce any bias, and in practice we find considerable improvement in image quality with just a single mutation per reservoir sample in each frame.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None so far.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Rohan Sawhney, Daqi Lin, Markus Kettunen, Benedikt Bitterli, Ravi Ramamoorthi, Chris Wyman, Matt Pharr. 2024. Decorrelating ReSTIR Samplers via MCMC Mutations, ACM Transactions on Graphics, January 2024. 43(1), 10:1-15.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/restir-mutations.png" alt="" width="388" height="370" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">ART-Owen Scrambling</h3>
                            <div class="authors">Abdalla G. M. Ahmed, Matt Pharr, Peter Wonka</div>
                            <div>ACM Transactions on Graphics (SIGGRAPH Asia)</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ahmed2023artowen.pdf">[Paper]</a>
                                <a href="https://arxiv.org/abs/2311.11664">[arXiv]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/artowen.png" alt="" width="388" height="274" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We present a novel algorithm for implementing Owen-scrambling, combining the generation and distribution of the scrambling bits in a single self-contained compact process. We employ a context-free grammar to build a binary tree of symbols, and equip each symbol with a scrambling code that affects all descendant nodes. We nominate the grammar of adaptive regular tiles (ART) derived from the repetition-avoiding Thue-Morse word, and we discuss its potential advantages and shortcomings. Our algorithm has many advantages, including random access to samples, fixed time complexity, GPU friendliness, and scalability to any memory budget. Further, it provides two unique features over known methods: it admits optimization, and it is invertable, enabling screen-space scrambling of the high-dimensional Sobol sampler.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None other than that I love the naming of this paper (which is not due to me) and it's incredible that the two Arts lined up.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Abdalla G. M. Ahmed, Matt Pharr, Peter Wonka. 2023. ART-Owen Scrambling. ACM Transactions on Graphics (SIGGRAPH Asia), 42(6) 258:1-11.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/artowen.png" alt="" width="388" height="274" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">ReSTIR GI: Path Resampling for Real-Time Path Tracing</h3>
                            <div class="authors">Yaobin Ouyang, Shiqiu Liu, Markus Kettunen, Matt Pharr, and Jacopo Pantaleoni</div>
                            <div>High Performance Graphics 2021 (Computer Graphics Forum)</div>
                            <div class="mb-3"><a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/RESTIR-GI.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/restir-gi.jpg" alt="" width="486" height="486" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Even with the advent of hardware-accelerated ray tracing in modern GPUs, only a small number of rays can be traced at each pixel in real-time applications. This presents a significant challenge for path tracing, even when augmented with state-of-the art denoising algorithms. While the recently-developed ReSTIR algorithm [Bitterli et al. 2020] enables high-quality renderings of scenes with millions of light sources using just a few shadow rays at each pixel, there remains a need for effective algorithms to sample indirect illumination.
                            </div>
                            <div class="abstract mb-2">We introduce an effective path sampling algorithm for indirect lighting that is suitable to highly parallel GPU architectures. Building on the screen-space spatio-temporal resampling principles of ReSTIR, our approach resamples multi-bounce indirect lighting paths obtained by path tracing. Doing so allows sharing information about important paths that contribute to lighting both across time and pixels in the image. The resulting algorithm achieves a substantial error reduction compared to path tracing: at a single sample per pixel every frame, our algorithm achieves MSE improvements ranging from 9.3x to 166x in our test scenes. In conjunction with a denoiser, it leads to high-quality path traced global illumination at real-time frame rates on modern GPUs</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                ReSTIR: the gift that keeps on giving.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Yaobin Ouyang, Shiqiu Liu, Markus Kettunen, Matt Pharr, and Jacopo Pantaleoni. 2021. ReSTIR GI: Path Resampling for Real-Time Path Tracing. Computer Graphics Forum (High Performance Graphics 2021).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/restir-gi.jpg" alt="" width="486" height="486" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Spatiotemporal Reservoir Resampling for Real-Time Ray Tracing with Dynamic Direct Lighting</h3>
                            <div class="authors">Benedikt Bitterli, Chris Wyman, Matt Pharr, Peter Shirley, Aaron Lefohn, and Wojciech Jarosz</div>
                            <div>SIGGRAPH 2020</div>
                            <div class="mb-3"><a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ReSTIR.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/restir.jpg" alt="" width="338" height="440" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Efficiently rendering direct lighting from millions of dynamic light sources using Monte Carlo integration remains a challenging problem, even for off-line rendering systems. We introduce a new algorithm—ReSTIR—that renders such lighting interactively, at high quality, and without needing to maintain complex data structures. We repeatedly resample a set of candidate light samples and apply further spatial and temporal resampling to leverage information from relevant nearby samples. We derive an unbiased Monte Carlo estimator for this approach, and show that it achieves equal-error 6-60× faster than state-of-the-art methods. A biased estimator reduces noise further and is 35-65× faster, at the cost of some energy loss. We implemented our approach on the GPU, rendering complex scenes containing up to 3.4 million dynamic, emissive triangles in under 50ms per frame while tracing at most 8 rays per pixel.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                Temporal reuse has been an important part of the real-time rendering toolbox in recent years, notably with techniques like TAA. What I like most about this paper is that it applies temporal reuse to
                                <i>samples</i>
                                rather than final shaded pixel colors. In doing so, it can deliver an unbiased Monte Carlo estimate of the scene's lighting and in turn, avoids some of the issues that come with temporal reuse of final shaded pixel values. The algorithm is spooky effective at sampling direct lighting from up to millions of light sources thanks to massive sharing of light samples across both space and time.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Benedikt Bitterli, Chris Wyman, Matt Pharr, Peter Shirley, Aaron Lefohn, and Wojciech Jarosz. 2020. Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting.
                                <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH), 39</i>
                                (4).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block">
                                <div><img class="img-fluid img-paper" src="assets/img/restir.jpg" alt="" width="338" height="440" loading="lazy" /></div>
                            </span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Practical Product Sampling by Fitting and Composing Warps</h3>
                            <div class="authors">David Hart, Matt Pharr, Thomas M&uuml;ller, Ward Lopes, Morgan McGuire, and Peter Shirley.</div>
                            <div>Eurographics Symposium on Rendering 2020</div>
                            <div class="mb-3"><a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/practical-product-sampling.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/dragon.jpg" alt="" width="350" height="350" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We introduce a Monte Carlo importance sampling method for integrands composed of products and show its application to rendering where direct sampling of the product is often difficult. Our method is based on warp functions that operate on the primary samples in [0,1)^n, where each warp approximates sampling a single factor of the product distribution. Our key insight is that individual factors are often well‐behaved and inexpensive to fit and sample in primary sample space, which leads to a practical, efficient sampling algorithm. Our sampling approach is unbiased, easy to implement, and compatible with multiple importance sampling. We show the results of applying our warps to projected solid angle sampling of spherical triangles, to sampling bilinear patch light sources, and to sampling glossy BSDFs and area light sources, with efficiency improvements of over 1.6× on real‐world scenes.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                This paper grew out of a clever idea from Pete Shirley about warping uniform samples before using them with traditional inversion-method based approaches for importance sampling. It turns out that you can do a surprisingly decent approximation to product sampling by doing so; we show results for incorporating the cosine factor in sampling techniques for light sources as well as for sampling the product of BSDF and illumination. A nice thing about this technique is that it's roughly 50 lines of code and can easily be added to incorporated into Monte Carlo rendering systems;
                                <a href="https://github.com/mmp/pbrt-v4">pbrt-v4</a>
                                has an implementation of it.
                            </div>
                            <div class="comments mb-2">
                                Subsequent to publication, I realized that both
                                <a href="http://www.lafortune.eu/publications/Dublin.html">Lafortune And Willems (1995)</a>
                                and
                                <a href="http://graphics.ucsd.edu/~henrik/papers/importance_driven_path_tracing_using_the_photon_map">Jensen (1995)</a>
                                used a related technique for product sampling of BRDFs with indirect lighting: they both took indirect lighting samples, applied the inverse of the importance sampling mapping, and then tabularized sample counts in primary sample space. Samples in [0,1]^2 were then generated based on that distribution before being used for BRDF importance sampling. This work should have been included in our previous work discussion. (It does, however, share some similarities with the earlier work of Booth (1986), which is cited.)
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                David Hart, Matt Pharr, Thomas M&uuml;ller, Ward Lopes, Morgan McGuire, and Peter Shirley. 2020. Practical product sampling by fitting and composing warps.
                                <i>Computer Graphics Forum (Proceedings of EGSR 2020), 39</i>
                                (4), 149—158.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/dragon.jpg" alt="" width="350" height="350" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Dynamic Many-Light Sampling for Real-Time Ray Tracing</h3>
                            <div class="authors">Pierre Moreau, Matt Pharr, and Petrik Clarberg</div>
                            <div>High Performance Graphics 2019</div>
                            <div class="mb-3"><a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/dynamic-manylight.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/dynamic-manylight.jpg" alt="" width="352" height="326" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Monte Carlo ray tracing offers the capability of rendering scenes with large numbers of area light sources—lights can be sampled stochastically and shadowing can be accounted for by tracing rays, rather than using shadow maps or other rasterization-based techniques that do not scale to many lights or work well with area lights. Current GPUs only afford the capability of tracing a few rays per pixel at real-time frame rates, making it necessary to focus sampling on important light sources. While state-of-the-art algorithms for offline rendering build hierarchical data structures over the light sources that enable sampling them according to their importance, they lack efficient support for dynamic scenes. We present a new algorithm for maintaining hierarchical light sampling data structures targeting real-time rendering. Our approach is based on a two-level BVH hierarchy that reduces the cost of partial hierarchy updates. Performance is further improved by
                                updating lower-level BVHs via refitting, maintaining their original topology. We show that this approach can give error within 6% of recreating the entire hierarchy from scratch at each frame, while being two orders of magnitude faster, requiring less than 1 ms per frame for hierarchy updates for a scene with thousands of moving light sources on a modern GPU. Further, we show that with spatiotemporal filtering, our approach allows complex scenes with thousands of lights to be rendered with ray-traced shadows in 16.1 ms per frame.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                This paper brought Alex Conty Estevez and Chris Kulla's BVH-based many light sampling technique to the GPU for real-time rendering. The main contribution was showing that the light hierarchy could be efficiently updated for dynamic scenes by maintaining a two-level BVH and applying refitting. Not too long afterward, ReSTIR showed much better results for this problem, though only for first-hit rays from the camera.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Pierre Moreau, Matt Pharr, and Petrik Clarberg. 2019. Dynamic many-light sampling for real-time ray tracing.
                                <i>High Performance Graphics (Short Papers),</i>
                                21—26.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/dynamic-manylight.jpg" alt="" width="352" height="326" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Efficient Generation of Points that Satisfy Two-Dimensional Elementary Intervals</h3>
                            <div class="authors">Matt Pharr</div>
                            <div>Journal of Computer Graphics Techniques</div>
                            <div class="mb-3"><a href="assets/efficient-gen-elmentary-intervals.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/4k-pmj02bn.jpg" alt="" width="302" height="299" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Precomputing high-quality sample points has been shown to be a useful technique for Monte Carlo integration in rendering; doing so allows optimizing properties of the points without the performance constraints of generating samples during rendering. A particularly useful property to incorporate is stratification across elementary intervals, which has been shown to reduce error in Monte Carlo integration. This is a key property of the recently-introduced progressive multi-jittered, pmj02 and pmj02bn points [Christensen et al. 2018]. For generating such sets of sample points, it is important to be able to efficiently choose new samples that are not in elementary intervals occupied by existing samples. Random search, while easy to implement, quickly becomes infeasible after a few thousand points. We describe an algorithm that efficiently generates 2D sample points that are stratified with respect to sets of elementary intervals. If a total of n sample
                                points are being generated, then for each sample, our algorithm uses O(n^1/2) time to build a data structure that represents the regions where a next sample may be placed. Given this data structure, valid samples can be generated in O(1) time. We demonstrate the utility of our method by generating much larger sets of pmj02bn points than were feasible previously
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                I really liked the 2018 paper by Per Christensen, Andrew Kensler, and Charlie Kilpatrick on progressive multi-jittered sample sets that also satisfied the 2D elementary intervals and had blue noise characteristics. Their approach used a relatively slow search for valid points and so I spent some time thinking about whether that part of their algorithm could be done more efficiently. After this paper was published, I realized that the paper
                                <a href="http://gruenschloss.org/netsearch/netsearch.pdf">(t,m,s)-nets and maximized minimum distance</a>
                                by Leo Gr&uuml;schlo&szlig; et al. introduces a very similar trick to the one this paper described.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr. 2019. Efficient generation of points that satisfy two-dimensional elementary intervals.
                                <i>Journal of Computer Graphics Techniques (JCGT), 8</i>
                                (1) 56—68.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/4k-pmj02bn.jpg" alt="" width="302" height="299" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">A System for Acquiring, Processing, and Rendering Panoramic Light Field Stills for Virtual Reality</h3>
                            <div class="authors">Ryan Overbeck, Daniel Erickson, Daniel Evangelakos, Matt Pharr, and Paul Debevec</div>
                            <div>SIGGRAPH Asia 2018</div>
                            <div class="mb-3"><a href="assets/vr-lightfield.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/lf-shuttle.jpg" alt="" width="400" height="400" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We present a system for acquiring, processing, and rendering panoramic light field still photography for display in Virtual Reality (VR). We acquire spherical light field datasets with two novel light field camera rigs designed for portable and efficient light field acquisition. We introduce a novel real-time light field reconstruction algorithm that uses a per-view geometry and a disk-based blending field. We also demonstrate how to use a light field prefiltering operation to project from a high-quality offline reconstruction model into our real-time model while suppressing artifacts. We introduce a practical approach for compressing light fields by modifying the VP9 video codec to provide high quality compression with real-time, random access decompression.
                            </div>
                            <div class="abstract mb-2">We combine these components into a complete light field system offering convenient acquisition, compact file size, and high-quality rendering while generating stereo views at 90Hz on commodity VR hardware. Using our system, we built a freely available light field experience application called Welcome to Light Fields featuring a library of panoramic light field stills for consumer VR which has been downloaded over 15,000 times.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                There is an entertaining retrospective to write about Google's VR efforts and the light field project specifically, but this space is too narrow to contain it.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Ryan Overbeck, Daniel Erickson, Daniel Evangelakos, Matt Pharr, and Paul Debevec. 2018. A system for acquiring, processing, and rendering panoramic light field stills for virtual reality.
                                <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2018) 37</i>
                                (6).
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/lf-shuttle.jpg" alt="" width="400" height="400" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">View-Region Optimized Image-Based Scene Simplification</h3>
                            <div class="authors">Puneet Lall, Silviu Borac, Dave Richardson, Matt Pharr, and Manfred Ernst</div>
                            <div>High Performance Graphics 2018</div>
                            <div><b>Third Place, Best Paper Award</b></div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/seurat.pdf">[Paper]</a>
                                <a href="https://github.com/googlevr/seurat">[Source]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/seurat.jpg" alt="" width="337" height="310" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We present a new algorithm for image-based simplification of complex scenes for virtual reality (VR). The algorithm transforms geometrically-detailed environments into a layered quad tile representation that is optimized for a specified viewing region and is renderable on low-power mobile-class VR devices. A novel constrained optimization formulation ensures that the scene can be rendered within a predetermined compute budget, with limits on both primitive count and fill rate. Furthermore, we introduce a new method for texturing from point samples of the original scene geometry that generates high-quality silhouettes without the drawbacks of traditional point splatting.
                            </div>
                            <div class="abstract mb-2">The resulting representation achieves a visual fidelity that was previously impossible on mobile graphics hardware; our algorithm can typically generate a high-quality representation of visually-rich scenes with billions of triangles using just 72k triangles and a single high-resolution texture map (with generally only about 50% more texels than a stereo panorama). The effectiveness of the approach is demonstrated with a set of challenging test cases.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                Google's VR effort was focused on battery-powered HMDs, which in turn implied limited power and thermal budgets and mobile-class GPUs. Under these constraints, rendering high-quality content was especially challenging. This paper came out of the Seurat project, which was working on finding ways to efficiently render complex static models while still maintaining correct parallax when viewed from some specified head box. We were lucky to be able to
                                <a href="https://www.roadtovr.com/preview-google-seurat-ilm-xlab-mobile-vr-rendering/">work with ILM</a>
                                and show off Star Wars assets rendered in VR on a battery-powered headset.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Puneet Lall, Silviu Borac, Dave Richardson, Matt Pharr, and Manfred Ernst. 2018. View-region optimized image-based scene simplification.
                                <i>Proceedings of the ACM on Computer Graphics and Interactive Techniques.</i>
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/seurat.jpg" alt="" width="337" height="310" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Sequences With Low‐Discrepancy Blue‐Noise 2‐D Projections</h3>
                            <div class="authors">Hélène Perrier, David Coeurjolly, Feng Xie, Matt Pharr, Pat Hanrahan, and Victor Ostromoukhov</div>
                            <div>Eurographics 2018</div>
                            <div class="mb-3">
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ldbn-sequences.pdf">[Paper]</a>
                                <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ldbn-sequences-supplemental.pdf">[Supplemental]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/ldbn.jpg" alt="" width="307" height="645" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Distributions of samples play a very important role in rendering, affecting variance, bias and aliasing in Monte-Carlo and Quasi-Monte Carlo evaluation of the rendering equation. In this paper, we propose an original sampler which inherits many important features of classical low-discrepancy sequences (LDS): a high degree of uniformity of the achieved distribution of samples, computational efficiency and progressive sampling capability. At the same time, we purposely tailor our sampler in order to improve its spectral characteristics, which in turn play a crucial role in variance reduction, anti-aliasing and improving visual appearance of rendering. Our sampler can efficiently generate sequences of multidimensional points, whose power spectra approach so-called Blue-Noise (BN) spectral property while preserving low discrepancy (LD) in certain 2-D projections.
                            </div>
                            <div class="abstract mb-2">In our tile-based approach, we perform permutations on subsets of the original Sobol LDS. In a large space of all possible permutations, we select those which better approach the target BN property, using pair-correlation statistics. We pre-calculate such “good” permutations for each possible Sobol pattern, and store them in a lookup table efficiently accessible in runtime. We provide a complete and rigorous proof that such permutations preserve dyadic partitioning and thus the LDS properties of the point set in 2-D projections. Our construction is computationally efficient, has a relatively low memory footprint and supports adaptive sampling. We validate our method by performing spectral/discrepancy/aliasing analysis of the achieved distributions, and provide variance analysis for several target integrands of theoretical and practical interest.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                (none)
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Hélène Perrier, David Coeurjolly, Feng Xie, Matt Pharr, Pat Hanrahan, and Victor Ostromoukhov. 2018. Sequences with Low-Discrepancy Blue-Noise 2-D Projections
                                <i>Computer Graphics Forum (Proceedings of Eurographics) 37</i>
                                (2), 339—353.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/ldbn.jpg" alt="" width="307" height="645" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">ispc: A SPMD compiler for high-performance CPU programming</h3>
                            <div class="authors">Matt Pharr and William R. Mark</div>
                            <div>Innovative Parallel Computing (InPar) 2012</div>
                            <div><b>Best Paper Award</b></div>
                            <div class="mb-3">
                                <a href="assets/ispc.pdf">[Paper]</a>
                                <a href="https://ispc.github.io/">[Website]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/ispc.jpg" alt="" width="810" height="608" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                SIMD parallelism has become an increasingly important mechanism for delivering performance in modern CPUs, due its power efficiency and relatively low cost in die area compared to other forms of parallelism. Unfortunately, languages and compilers for CPUs have not kept up with the hardware's capabilities. Existing CPU parallel programming models focus primarily on multi-core parallelism, neglecting the substantial computational capabilities that are available in CPU SIMD vector units. GPU-oriented languages like OpenCL support SIMD but lack capabilities needed to achieve maximum efficiency on CPUs and suffer from GPU-driven constraints that impair ease of use on CPUs.
                            </div>
                            <div class="abstract mb-2">We have developed a compiler, the Intel SPMD Program Compiler (ispc), that delivers very high performance on CPUs thanks to effective use of both multiple processor cores and SIMD vector units. ispc draws from GPU programming languages, which have shown that for many applications the easiest way to program SIMD units is to use a single-program, multiple-data (SPMD) model, with each instance of the program mapped to one SIMD lane. We discuss language features that make ispc easy to adopt and use productively with existing software systems and show that ispc delivers up to 35x speedups on a 4-core system and up to 240x speedups on a 40-core system for complex workloads (compared to serial C++ code).</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                None beyond
                                <a href="blog/2018/04/30/ispc-all.html">12 blog posts</a>
                                about the history of ispc.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr and William R. Mark. 2012. ispc: A SPMD compiler for high-performance CPU programming.
                                <i>Proceedings of Innovative Parallel Computing (InPar) 2012.</i>
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/ispc.jpg" alt="" width="810" height="608" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Monte Carlo evaluation of non-linear scattering equations for subsurface reflection</h3>
                            <div class="authors">Matt Pharr and Pat Hanrahan</div>
                            <div>SIGGRAPH 2000</div>
                            <div class="mb-3"><a href="assets/scatteringequations.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/scatteringequations.jpg" alt="" width="932" height="932" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We describe a new mathematical framework for solving a wide variety of rendering problems based on a non-linear integral scattering equation. This framework treats the scattering functions of complex aggregate objects as first-class rendering primitives; these scattering functions accurately account for all scattering events inside them. We also describe new techniques for computing scattering functions from the composition of scattering objects. We demonstrate that solution techniques based on this new approach can be more efficient than previous techniques based on radiance transport and the equation of transfer and we apply these techniques to a number of problems in rendering scattering from complex surfaces.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                (None).
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr and Pat Hanrahan. 2000. Monte Carlo evaluation of non-linear scattering equations for subsurface reflection.
                                <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques (SIGGRAPH '00),</i>
                                75–84.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/scatteringequations.jpg" alt="" width="932" height="932" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Realistic modeling and rendering of plant ecosystems</h3>
                            <div class="authors">Oliver Deussen, Pat Hanrahan, Bernd Lintermann, Radomir Mech, Matt Pharr, and Przemyslaw Prusinkiewicz.</div>
                            <div>SIGGRAPH 1998</div>
                            <div class="mb-3"><a href="assets/ecosys.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/ecosys.jpg" alt="" width="368" height="248" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Modeling and rendering of natural scenes with thousands of plants poses a number of problems. The terrain must be modeled and plants must be distributed throughout it in a realistic manner, reflecting the interactions of plants with each other and with their environment. Geometric models of individual plants, consistent with their positions within the ecosystem, must be synthesized to populate the scene. The scene, which may consist of billions of primitives, must be rendered efficiently while incorporating the subtleties of lighting in a natural environment.
                            </div>
                            <div class="abstract mb-2">We have developed a system built around a pipeline of tools that address these tasks. The terrain is designed using an interactive graphical editor. Plant distribution is determined by hand (as one would do when designing a garden), by ecosystem simulation, or by a combination of both techniques. Given parameterized procedural models of individual plants, the geometric complexity of the scene is reduced by approximate instancing, in which similar plants, groups of plants, or plant organs are replaced by instances of representative objects before the scene is rendered. The paper includes examples of visually rich scenes synthesized using the system.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                As I remember it, after the 1997 paper on memory-coherent ray tracing, Pat and Przemek got to talking. The combination of Przemek's ability to generate complex scenes full of plants and our work on rendering complex scenes led to this collaboration and soon enough I was off to Calgary for a few weeks in November 1997. I remember that in Calgary it got dark awfully early and that it was mighty chilly on walks back to my hotel in the evenings.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Oliver Deussen, Pat Hanrahan, Bernd Lintermann, Radomir Mech, Matt Pharr, and Przemyslaw Prusinkiewicz. 1998. Realistic modeling and rendering of plant ecosystems.
                                <i>Proceedings of the 25th annual conference on Computer graphics and interactive techniques (SIGGRAPH '98),</i>
                                275–286.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/ecosys.jpg" alt="" width="368" height="248" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Rendering complex scenes with memory-coherent ray tracing</h3>
                            <div class="authors">Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan</div>
                            <div>SIGGRAPH 1997</div>
                            <div class="mb-3"><a href="assets/coherentrt.pdf">[Paper]</a></div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/cathedral.jpg" alt="" width="384" height="576" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                Simulating realistic lighting and rendering complex scenes are usually considered separate problems with incompatible solutions. Accurate lighting calculations are typically performed using ray tracing algorithms, which require that the entire scene database reside in memory to perform well. Conversely, most systems capable of rendering complex scenes use scan-conversion algorithms that access memory coherently, but are unable to incorporate sophisticated illumination. We have developed algorithms that use caching and lazy creation of texture and geometry to manage scene complexity. To improve cache performance, we increase locality of reference by dynamically reordering the rendering computation based on the contents of the cache. We have used these algorithms to compute images of scenes containing millions of primitives, while storing ten percent of the scene description in memory. Thus, a machine of a given memory capacity can render realistic scenes
                                that are an order of magnitude more complex than was previously possible.
                            </div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                The follow-on to the EGSR geometry caching paper, where we showed that caching worked really well if you also reorder the ray tracing work to access the cache coherently. It is a fairly disruptive change to a renderer to reorder the traced rays and as it has turned out, available RAM has grown enough that it is generally possible to store whatever scene you want to render in memory, perhaps with a bit of work to ensure it fits and perhaps with some caching but no reordering. Nevertheless, this is probably my favorite paper, both for being my first SIGGRAPH paper and for the complexity of the scenes we were path tracing, given available memory at the time.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. 1997. Rendering complex scenes with memory-coherent ray tracing.
                                <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques (SIGGRAPH '97),</i>
                                101–108.
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/cathedral.jpg" alt="" width="384" height="576" loading="lazy" /></span>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Geometry caching for ray-tracing displacement maps</h3>
                            <div class="authors">Matt Pharr and Pat Hanrahan</div>
                            <div>Eurographics Workshop on Rendering 1996</div>
                            <div class="mb-3">
                                <a href="assets/displace.pdf">[Paper]</a>
                                <a href="assets/img/displace-plate1.png">[Plate 1]</a>
                                <a href="assets/img/displace-plate2.png">[Plate 2]</a>
                                <a href="assets/img/displace-plate3.png">[Plate 3]</a>
                            </div>
                            <span class="d-lg-none">
                                <div class="text-center"><img class="img-fluid img-paper" src="assets/img/displace-plate2.jpg" alt="" width="374" height="562" loading="lazy" /></div>
                            </span>
                            <div class="abstract mb-2">
                                <b>Abstract:</b>
                                We present a technique for rendering displacement mapped geometry in a ray-tracing renderer. Displacement mapping is an important technique for adding detail to surface geometry in rendering systems. It allows complex geometric variation to be added to simpler geometry, without the cost in geometric complexity of completely describing the nuances of the geometry at modeling time and with the advantage that the detail can be added adaptively at rendering time.
                            </div>
                            <div class="abstract mb-2">The cost of displacement mapping is geometric complexity. Renderers that provide it must be able to efficiently render scenes that have effectively millions of geometric primitives. Scan-line renderers process primitives one at a time, so this complexity doesn't tax them, but traditional ray-tracing algorithms require random access to the entire scene database, so any part of the scene geometry may need to be available at any point during rendering. If the displaced geometry is fully instantiated in memory, it is straightforward to intersect rays with it, but displacement mapping has not yet been practical in ray-tracers due to the memory cost of holding this much geometry.</div>
                            <div class="abstract mb-2">We introduce the use of a geometry cache in order to handle the large amounts of geometry created by displacement mapping. By caching a subset of the geometry created and rendering the image in a coherent manner, we are able to take advantage of the fact that the rays spawned by traditional ray-tracing algorithms are spatially coherent. Using our algorithm, we have efficiently rendered highly complex scenes while using a limited amount of memory.</div>
                            <div class="comments mb-2">
                                <b>Comments:</b>
                                Caching a subset of the scene geometry in memory works out well when you're doing Whitted ray tracing and trying to render complex scenes with a few tens of megabytes of RAM. (Note: less RAM than a current CPU has in cache memory.) This paper was published in the olden days when you couldn't have a color image here there and everywhere in your paper. Therefore, the paper itself includes no images, but there are separate “color plates” for the reader's elucidation. At the conference, which was held in Porto, we found a restaurant named “Monte Carlo” that naturally had to be tried; Phil Dutré took a
                                <a href="https://lh5.googleusercontent.com/EV9qc4bwKDEPqkxTcc4usdpOYutvITxD8PTmplm843HCz9U3YTXKVVwcRaptukZAlBSS_EyJv0tTrtZZUb3i4Xtz545UCYlYo0Ww71osm383j-f8Mb8=w1280">wonderful picture</a>
                                of assembled rendering nerds there.
                            </div>
                            <div class="citation mb-2">
                                <b>Citation:</b>
                                Matt Pharr and Pat Hanrahan. 1996. Geometry caching for ray-tracing displacement maps.
                                <i>Proceedings of the Eurographics Workshop on Rendering.</i>
                            </div>
                        </div>
                        <div class="flex-shrink-0">
                            <span class="d-none d-lg-block"><img class="img-fluid img-paper" src="assets/img/displace-plate2.jpg" alt="" width="374" height="562" loading="lazy" /></span>
                        </div>
                    </div>
                </div>
            </section>
            <!-- Projects-->
            <section class="resume-section" id="projects">
                <div class="resume-section-content">
                    <h2 class="mb-5">Projects</h2>
                    <h3 class="mb-1">Committees</h3>
                    <ul>
                        <li>Papers Chair, Eurographics Symposium on Rendering 2020 (with Carsten Dachsbacher)</li>
                        <li>Academy of Motion Picture Arts and Sciences Digital Imaging Technology Surrogate Committee, 2015&ndash;2018, 2022&ndash;2025</li>
                        <li>Program Chair, High Performance Graphics 2014</li>
                        <li>Conference Chair, High Performance Graphics 2011 (with John Owens)</li>
                        <li>Papers Chair, High Performance Graphics 2009 (with David McAllister and Ingo Wald)</li>
                        <li>
                            Editorial board,
                            <a href="http://jcgt.org/">Journal of Computer Graphics Techniques</a>
                        </li>
                    </ul>
                    <h3 class="mb-1">TOG Production Renderers</h3>
                    <p>
                        In 2018, I guest-edited a special issue of
                        <i>ACM Transactions on Graphics</i>
                        on production rendering. The developers of five widely-used renderers wrote comprehensive systems papers, describing the challenges they face, the constraints they work under, and the solutions they have developed:
                    </p>
                    <ul>
                        <li><a href="https://graphics.pixar.com/library/RendermanTog2018/">RenderMan (Christensen et al.)</a></li>
                        <li><a href="https://jo.dreggn.org/home/2018_manuka.pdf">Manuka (Fascione et al.)</a></li>
                        <li><a href="https://fpsunflower.github.io/ckulla/data/2018_tog_spi_arnold.pdf">Sony Arnold (Kulla et al.)</a></li>
                        <li><a href="https://www.solidangle.com/research/Arnold_TOG2018.pdf">Solid Angle Arnold (Fajardo et al.)</a></li>
                        <li><a href="https://www.yiningkarlli.com/projects/hyperiondesign.html">Hyperion (Burley et al.)</a></li>
                    </ul>
                    <p>
                        I wrote a
                        <a href="assets/tog_intro.pdf">short introduction</a>
                        that discusses the industry's transition from Reyes to path tracing.
                    </p>
                    <h3 class="mb-1">Reports / Drafts</h3>
                    <p>Here are a handful of additional documents that are not formal publications as such.</p>
                    <ul>
                        <li>
                            <b>Physically Based Image Synthesis: Design and Implementation of a Rendering System (2003)</b>
                            <i>(with Greg Humphreys)</i>
                            <a href="assets/lrt-draft-2003.pdf">[PDF]</a>
                            <div></div>
                            For posterity, a 2003 snapshot of the manuscript that became the
                            <i>Physically Based Rendering</i>
                            book.
                        </li>
                        <li>
                            <b>Infinite Area Light Source With Importance Sampling (2004)</b>
                            <i>(with Greg Humphreys)</i>
                            <a href="assets/infinitesample.pdf">[PDF]</a>
                            <div></div>
                            A short literate program implementation of an importance sampling algorithm for infinite light sources, too late for the first edition of
                            <i>Physically Based Rendering,</i>
                            but included in subsequent editions.
                        </li>
                        <li>
                            <b>The Implementation of a Hair Scattering Model (2016)</b>
                            <a href="assets/hair.pdf">[PDF]</a>
                            <div></div>
                            This is an implementation of Chiang et al's
                            <i>A Practical and Controllable Hair and Fur Model for Production Path Tracing,</i>
                            implemented as a literate program. The fourth edition of
                            <i>Physically Based Rendering</i>
                            includes an
                            <a href="https://https://pbr-book.org/4ed/Reflection_Models/Scattering_from_Hair.html">updated version</a>
                            of this.
                        </li>
                        <li>
                            <b>The Implementation of a Scalable Texture Cache (2017)</b>
                            <a href="assets/texcache.pdf">[PDF]</a>
                            <div></div>
                            I spent some time trying to figure out how to implement a texture cache for CPU rendering that scaled well with many threads; this is a problem that has clearly been solved in production but one where no one has published about their approaches. This write-up this is the result of my efforts&mdash;an effective approach that is based on the "read-copy update" technique, implemented as a literate program.
                        </li>
                    </ul>
                    <h3 class="mb-1">Code</h3>
                    <ul>
                        <li>
                            <b>skicka:</b>
                            Partially as an exercise to learn go, I wrote a command-line tool that makes it easy to work with files and directories on Google Drive (including uploading/downloading, listing files in folders, etc). Google was happy to let me open source it. (“skicka” is Swedish for “to send”, which vaguely alludes to what the tool does.)
                            <a href="https://github.com/google/skicka">[source]</a>
                        </li>
                        <li>
                            <b>bk:</b>
                            After near disaster from an Apple Time Capsule that silently failed, I wrote my own backup system featuring just enough functionality for my needs while not being too complicated: it features deduplication, encryption, direct cloud storage, and Reed-Solomon encoding for local backup integrity.
                            <a href="https://github.com/mmp/bk">[source]</a>
                        </li>
                        <li>
                            <b>vice:</b>
                            In the "software with a small audience" department, I've lately been hacking on an air traffic control simulator, patterened on the STARS radar scope used in the US.
                            <a href="https://pharr.org/vice">[documentation]</a>
                            <a href="https://github.com/mmp/vice">[source]</a>
                        </li>
                        <li>
                            <b>Apple ][ works:</b>
                            Not to brag, but in middle school I wrote a pretty awesome
                            <a href="assets/dd.bas.txt">D&D character generator</a>
                            in Applesoft Basic as well as a
                            <a href="assets/diskmod.txt">disk track/sector editor</a>
                            in 6502 assembly language.
                        </li>
                    </ul>
                    <h3 class="mb-1">Selected Talks</h3>
                    <ul>
                        <li>
                            <b>Porting pbrt to the GPU While Preserving its Soul</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/pbrt-gpu-hpg2020.pdf">[slides]</a>
                            <div>Invited talk, High Performance Graphics 2020.</div>
                        </li>
                        <li>
                            <b>Path Tracing for Future Games (??!!?)</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/rtrt-open-problems-2019.pdf">[slides]</a>
                            <div>SIGGRAPH 2019 “Open Problems in Real-Time Rendering” course.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                A year after the launch of RTX GPUs, it was easier to talk about ray tracing. The “open problems” course was a great opportunity to talk about some of the early successes with real-time ray tracing and to sketch out some of the biggest unsolved challenges.
                            </div>
                        </li>
                        <li>
                            <b>Adopting Lessons from Offline Ray-Tracing to Real-Time Ray Tracing for Practical Pipelines</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/advances-adopting-lessons-2018.pdf">[slides]</a>
                            <div>SIGGRAPH 2018 “Advances in Real-Time Rendering in Games” course.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                The goal of this talk was to advance the idea of the importance of sampling well when doing Monte Carlo ray tracing in the real-time rendering community. It was a slightly tricky talk in that there was a risk of the topic seeming irrelevant to the realities of real-time rendering at that time. The public announcement of RTX GPUs half an hour after the end of the talk was helpful in that regard, at least after the fact.
                            </div>
                        </li>
                        <li>
                            <b>Warp and Effect</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/warp-and-effect.pdf">[slides]</a>
                            <div>SIGGRAPH 2018 “My Favorite Samples” course.</div>
                        </li>
                        <li>
                            <b>Mobile VR: Challenges and Opportunities</b>
                            <a href="assets/mobilevr.pdf">[slides]</a>
                            <div>Keynote, Symposium on Interactive 3D Graphics and Games (i3d) 2017.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                “Challenges” seems to have won.
                            </div>
                        </li>
                        <li>
                            <b>ispc: A SPMD Compiler for High-Performance CPU Programming</b>
                            <a href="assets/uiuc-ispc-2012.pdf">[slides]</a>
                            <div>University of Illinois Urbana-Champaign I2PC Distinguished Speaker Series, 2012.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                This is representative of a number of talks about ispc that I gave around this time.
                            </div>
                        </li>
                        <li>
                            <b>Interactive Rendering in the Post-GPU Era</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/graphicshardware.pdf">[slides]</a>
                            <div>Keynote, Graphics Hardware 2006.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                Based on the observation that some of the most effective real-time graphics algorithms were based on using sophisticated data structures in shaders, this talk argued for the importance of closely coupled CPU+GPU architectures under the premise that it was too difficult to build such data structures on the GPU. As it has turned out, enough clever people have subsequently figured out how to do that on GPUs that keeping it all on the GPU seems just fine. I believe that at least this talk coined the term “programmable graphics” and made the distinction with “programmable shading”.
                            </div>
                        </li>
                        <li>
                            <b>The Quiet Revolution in Interactive Rendering</b>
                            <a href="https://pub-3d16d67d2c68402fa2fb05197bac91f9.r2.dev/ir-ee380.pdf">[slides]</a>
                            <div>Invited talk, Stanford University ee380 Computer Systems Colloquium, 2005.</div>
                            <div class="talkcomments">
                                <b>Comments:</b>
                                Graphics got a lot better even just a few years after the arrival of programmable GPUs. This talk was an effort to break down what had happened in how people were using programmability and to argue that they would be able to reach film-quality imagery well before a naive comparison based on FLOPS required to render a image film suggested.
                            </div>
                        </li>
                    </ul>
                    <h3 class="mb-1">Miscellanea</h3>
                    <ul>
                        <li>
                            <b>Film credits:</b>
                            For work I did in Pixar's Rendering R&D group as an intern in graduate school, I have movie credits for
                            <i>A Bug's Life</i>
                            and
                            <i>Toy Story 2.</i>
                            Using a very loose definition of the Bacon number, this means that I have an
                            <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number">Erdős–Bacon</a>
                            number of 6.
                        </li>
                        <li>
                            <b>Stanford cs348b:</b>
                            I've taught
                            <a href="http://graphics.stanford.edu/courses/cs348b/">Image Synthesis Techniques,</a>
                            the graduate-level rendering class at Stanford seven times, co-teaching it with Pat Hanrahan for the past four years and doing it a few times solo in years previous.
                        </li>
                        <li>
                            <b>GPU Gems 2:</b>
                            While at NVIDIA the first time, I edited the book
                            <a href="https://developer.nvidia.com/gpugems/gpugems2/inside-front-cover">GPU Gems 2: Programming Techniques for High-Performance Graphics and General Purpose Computation.</a>
                            The first half of the book is comprised of twenty-four chapters about the state-of-the-art in interactive rendering, and the second half is devoted to general purpose computation on graphics processors (GPGPU)—the first book covering this topic.
                        </li>
                        <li>
                            <b>CACM Technical Perspective:</b>
                            The May 2013 issue of Communications of the ACM had a
                            <a href="https://cacm.acm.org/magazines/2013/5/163758-gpu-ray-tracing/fulltext">paper on OptiX</a>
                            by Steve Parker et al. for which I wrote an
                            <a href="assets/cacm_rt_engine.pdf">introduction and technical perspective,</a>
                            framing the work for a wider audience.
                        </li>
                    </ul>
                    <h3 class="mb-1">Social</h3>
                    <ul>
                        <li><a rel="me" href="https://mastodon.gamedev.place/@mattpharr">@mattpharr@mastodon.gamedev.place</a></li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>

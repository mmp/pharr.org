<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="icon" type="image/png" href="/matt/favicon.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Swallowing the Elephant (Part 12): A Postscript On Disk Bandwidth</title>
  <meta name="description" content="A few concluding notes (this time for real) about the effect of disk bandwidth when preparing to render the Moana Island scene with pbrt-v4.">

  <link rel="stylesheet" href="/matt/blog/assets/main.css">
  <link rel="canonical" href="https://pharr.org/matt/blog/2021/08/07/moana-bandwidth-note.html">
  <link rel="alternate" type="application/rss+xml" title="Matt Pharr&#39;s blog" href="/matt/blog/feed.xml">
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/matt/blog/">Matt Pharr&#39;s blog</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
          
            
            
          
        </div>

      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Swallowing the Elephant (Part 12): A Postscript On Disk Bandwidth</h1>
    <p class="post-meta">
      <time datetime="2021-08-07T00:00:00-05:00" itemprop="datePublished">
        
        Aug 7, 2021
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>At the ostensible <a href="/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html">end of these
updates</a> about
pbrt-v4’s performance when rendering <a href="https://www.disneyanimation.com/resources/moana-island-scene/">Disney’s Moana Island
scene</a>,
there was an unresolved question about why CPU utilization wasn’t better at
the very start when pbrt was parsing the scene description.  As a
refresher, with 64 threads on a 32-core AMD 3970X CPU and pbrt’s GPU-based
rendering path, it looked like this. (As before, the vertical dashed line
indicates when rendering begins.)</p>

<p align="center"> <img src="/matt/blog/images/ttfp-cpu-64-truefin-rescaled.svg" alt="CPU utilization (64 threads, with more use of Import)" /> </p>

<p>Starting at the 17 second mark, low CPU utilization isn’t necessarily bad
since that’s when the GPU starts getting involved building acceleration
structures, but before that it’s all on the CPU.  For the first twelve or so
seconds, total CPU utilization is between 0.2 and 0.4, which corresponds to
roughly 13–26 of those 64 threads actually making something of themselves;
that’s not enough of them to be satisfying.</p>

<p>It started to nag at me whether limited disk bandwidth might have something
to do with that—i.e., is the issue that threads are stalled waiting for
I/O?  I made a few measurements to try to answer that question and learned
enough along the way that here we go again.</p>

<h2 id="how-far-we-have-come">How Far We Have Come</h2>

<p>Three years ago when I <a href="/matt/blog/2018/07/08/moana-island-pbrt-1.html">first looked at pbrt’s performance with Moana
Island</a> I was using a
Google Compute Engine instance with a spinny disk for benchmarks.  Nowadays
you might hope for around 100 MB/s of read bandwidth from such a disk.
pbrt-v4 reads a total of 27,766 MB from disk when loading this
scene and it takes a lot of 100 MBs to get through all of that.  Therefore,
when I was doing benchmarks then I was careful to flush the OS’s buffer
cache between runs so that the true cost of I/O was measured
and everything didn’t come out of RAM after the first time at rates much
better than 100 MB/s.</p>

<p>This time around, I didn’t mention the disk on the system I used for
benchmarking and I didn’t worry about the buffer cache.  That wasn’t an
oversight, but was more of a “I’m pretty sure this doesn’t matter”<sup id="fnref:ps"><a href="#fn:ps" class="footnote">1</a></sup> sort of
thing, like which version of the Linux kernel was running or whether it was
DDR4 3200 or DDR4 3600 RAM in the system.  (For the record, 5.8.0 and the
former.)</p>

<p>The disk I’m using now is an NVMe disk; a quick benchmark showed that it
delivers a peak of 2,022 MB/s of read bandwidth.  I didn’t think that could
be a bottleneck, though if you distribute those 2,022 MB/s evenly to 64
threads, it’s just 32 MB/s per thread.  Thinking about it in those terms
made me worry that bandwidth might be tight, so I decided to make some
direct measurements and see what they had to show.</p>

<h2 id="starting-position">Starting Position</h2>

<p>First, I measured pbrt’s disk bandwidth use over time to get a sense of
whether it ever approached the peak and to see how disk reads were
distributed over the course of loading the scene.  (This and following
measurements were made with an empty buffer cache, just to be safe.)
<code class="highlighter-rouge">iostat</code> made that easy to do, though sadly it doesn’t seem to be able to
report I/O with less than one second granularity, which is more coarse than
one would like given 30 seconds time to first pixel.  In any case, here is
a graph of what it had to say; the disk’s measured maximum I/O bandwidth is
marked with a dashed horizontal line.</p>

<p align="center"> <img src="/matt/blog/images/moana-io.svg" alt="CPU utilization (64 threads, with more use of Import)" /> </p>

<p>For the first 20 or seconds, pbrt is mostly parsing text *.pbrt scene
description files; it starts out consuming plenty of bandwidth but then
slows as there are fewer files left to get through.  The second wave
of I/O starting at 20 seconds corresponds to reading all of the PLY files for the
object instances in the scene.  The news in this graph is mostly good: pbrt
doesn’t seem to ever top out at the maximum bandwidth, suggesting that
it’s not I/O bound, though it’s close enough at 9 seconds there that it’s
not possible to be sure from these measurements.</p>

<p>This data also makes it possible to compute an alternative speed of light
measurement for time to first pixel.  If we divide the total size of data
read, 27,766 MB, by the peak read bandwidth of 2,022 MB/s, we can see that we
can’t hope to have a time to first pixel under 13.7 seconds.  That’s already an
interesting result, as it shows that the <a href="/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html#speed-of-light">earlier speed of light
calculation</a>
that only considered the CPU didn’t tell the whole story: then, 
neglecting I/O limits, we estimated 7.2 seconds as the best possible time
to first pixel.</p>

<p>Another thing this graph shows is that pbrt is close enough to being I/O
bound at the start that there isn’t a lot of reason to worry about the
relatively low CPU utilization then.  We might improve
it some by finding more things to start reading sooner, but the benefit
would be limited since we would soon hit peak disk bandwidth and be limited
by that.  Further performance improvements would then require a better
balance of I/O requests over time.</p>

<h2 id="turning-the-bandwidth-screw">Turning The Bandwidth Screw</h2>

<p>The data already seemed fairly conclusive about not being I/O bound, but I
was curious about how performance varied with disk read bandwidth—how
crucial is that lovely abundant NVMe bandwidth to pbrt’s performance with
this scene?  One way to find out is to start reducing the amount of disk
read bandwidth available to pbrt and to see how that affects performance.</p>

<p>Once you find <a href="https://unix.stackexchange.com/a/393798">the right trick</a>
it’s surprisingly easy, at least on Linux, to use <code class="highlighter-rouge">systemd-run</code> to launch a
process that has a limited amount of disk read bandwidth available to it.
I did a quick study, dialing the bandwidth down from the 2,000 MB/s that my
NVMe drive offers to the sad 50 MB/s that a middling spinning disk today
might provide.</p>

<p>Here is a graph of pbrt-v4’s time to first pixel with the Moana Island
scene as a function of available disk bandwidth, running with both 8
threads on 4 cores and 64 threads on 32 cores. Note that the y axis has a
logarithmic scale, the better to fit the sadness that is a nearly 600
second time to first pixel given 50 MB/s.</p>

<p align="center"> <img src="/matt/blog/images/ttfp-vs-read-bandwidth.svg" alt="CPU utilization (64 threads, with more use of Import)" /> </p>

<p>There are a number of things to see in this graph.  First, it offers
further confirmation that pbrt-v4 is not bandwidth limited for this scene:
the fact that performance doesn’t immediately decrease as bandwidth starts
to decrease from 2,000 MB/s indicates that more bandwidth isn’t going to make things
faster.  Both lines seem to have hit their asymptote, though the 64 thread
one just barely so.</p>

<p>This graph also shows how much bandwidth can decrease before performance is
meaningfully affected.  With 64 threads, you only have to go to 1400 MB/s
to slow down time to first pixel by 10%, but with 8 threads you can go all
the way to 800 MB/s before there’s a 10% drop.  This isn’t surprising—the
more threads you’ve got, the more bandwidth you’re capable of
consuming—but it’s nevertheless interesting to see how much farther one
can go with fewer threads.</p>

<p>Finally, note that below 500 MB/s, the two curves are
effectively the same.  Here, too, there’s no big surprise: if you’re trying
to drink through a narrow straw, having more thirsty people waiting in line
on the end of it isn’t going to get the water through more quickly, to grossly
overstretch a metaphor.</p>

<h2 id="deflate-deflate-deflate">DEFLATE, DEFLATE, DEFLATE</h2>

<p>Compression algorithms make it possible to trade off bandwidth for
computation, so my last experiment was to look at performance with the
scene description compressed using <code class="highlighter-rouge">gzip</code>.  Thanks to a recent <a href="https://github.com/mmp/pbrt-v4/commit/6577a325a4cb934efac3b10f3b33847cf0d93ea4">patch from
Jim
Price</a>,
pbrt-v4 now supports reading <code class="highlighter-rouge">gzip</code>-compressed scene description files, and
<a href="https://w3.impa.br/~diego/software/rply/">RPly</a>, the PLY file reader by Diego Nehab that
pbrt uses, already supported <code class="highlighter-rouge">gzip</code>-compressed PLY files.
All of that made it easy to run the same experiments with a compressed
scene description.</p>

<p>With the *.pbrt and PLY files compressed using <code class="highlighter-rouge">gzip -5</code>, pbrt-v4 reads a
total of just 5,570 MB from disk—nearly 5x less than with the
uncompressed scene description.  Using <a href="https://zlib.net/">zlib</a> for
decompression with 64 threads and the full NVMe disk bandwidth, pbrt takes
40 seconds to first pixel with a compressed scene—12 seconds slower than
with everything uncompressed.  Given that it wasn’t bandwidth-limited
before, that isn’t surprising—we have just increased the amount of CPU
work that needs to be done to get the scene into memory.</p>

<p>Here is the graph of disk I/O consumption over those 40 seconds; it shows
that now there is plenty of headroom with never more than 500 MB/s of
bandwidth used.</p>

<p align="center"> <img src="/matt/blog/images/moana-io-gz.svg" alt="CPU utilization (64 threads, with more use of Import)" /> </p>

<p>As we were going to press, I saw that Aras Pranckevičius just put up a nice
series of <a href="https://aras-p.info/blog/2021/08/04/EXR-Lossless-Compression/">blog posts about compression in
OpenEXR</a>.
Those led me down all sorts of ratholes, and one of them reminded me about
<a href="https://github.com/ebiggers/libdeflate">libdeflate</a>, a highly optimized
library that can decompress gzip-encoded files (among others).  It wasn’t too
much code to swap that in for zlib in pbrt and <strong>bam</strong>: down to 34 seconds
to first pixel with a compressed scene.  And that’s actually only using
libdeflate for the *.pbrt files but still using zlib for the 1,152 MB worth
of compressed PLY files, since using libdeflate with RPly would have
required more complicated plumbing.</p>

<p>Anyway, here’s a graph that shows time to first pixel with all three
options, again with 64 threads. libdeflate gets an asterisk, since it
isn’t being used for PLY files (and there is thus presumably some
performance being left on the floor.)</p>

<p align="center"> <img src="/matt/blog/images/ttfp-vs-read-gz.svg" alt="CPU utilization (64 threads, with more use of Import)" /> </p>

<p>There’s lots of good stuff to see there.  As advertised, libdeflate is
certainly faster than zlib. It starts being the fastest option overall at
around 1,300 MB/s of bandwidth.  From there on down, the additional CPU
work to do decompression is worth the disk bandwidth savings in return. (In
contrast, zlib doesn’t make up for its computational overhead until
around 1,000 MB/s.)</p>

<p>Both decompressors have more or less constant performance all the way down
to roughly 300 MB/s from the disk.  Past there, their performance
converges: at that point, data is coming in so slowly that how quickly it’s
decompressed doesn’t make much difference.  We can also see that
compression is especially helpful way down at 50 MB/s, where it’s leads to
a spritely 127 seconds to first pixel—4.6x faster than the uncompressed
scene is with that little bandwidth.</p>

<h2 id="discussion">Discussion</h2>

<p>For once we have gotten through these investigations without finding any
surprising bottlenecks and so today has not brought any changes to pbrt’s
implementation, though I suspect pbrt will switch from zlib to libdeflate
fairly soon.</p>

<p>Perhaps the most useful result from today is a more accurate estimate of
pbrt’s best possible performance when preparing to render this scene:
13.7 seconds given the disk I/O limits of the system.  With that limit
known, it’s easier to accept the 28 seconds to first pixel that the system
delivers today—apparently only 2x off the maximum attainable
performance—and to stop fiddling with the details.</p>

<p>And yet… I hope that the attentive reader might quibble with the logic
behind that conclusion: with the compressed scene, we found ourselves with
a mere 5,570 MB of disk I/O, and that’s something this computer can deliver
in 2.75 seconds, which puts us once again 10x off the mark.  It seems that
part of speed of light is in how you define it, but nevertheless I think
it’s time to leave things where they lie for now.</p>

<h2 id="note">note</h2>
<div class="footnotes">
  <ol>
    <li id="fn:ps">
      <p>The road to disastrous performance is paved with “pretty sure” assumptions about a system’s behavior, so that assumption was admittedly not wise. <a href="#fnref:ps" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Matt Pharr&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li><a href="/matt/">homepage</a></li>
          
            <li><a href="mailto:matt.pharr@gmail.com">matt.pharr@gmail.com</a></li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>It seemed worth writing up at the time.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>

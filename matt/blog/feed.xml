<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.3.1">Jekyll</generator><link href="https://pharr.org/matt/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pharr.org/matt/blog/" rel="alternate" type="text/html" /><updated>2021-12-24T08:29:28-08:00</updated><id>https://pharr.org/matt/blog/feed.xml</id><title type="html">Matt Pharr’s blog</title><subtitle>It seemed worth writing up at the time.
</subtitle><entry><title type="html">Debugging Your Renderer (5/n): Rendering Deterministically</title><link href="https://pharr.org/matt/blog/2021/12/24/debugging-renderers-rendering-deterministically.html" rel="alternate" type="text/html" title="Debugging Your Renderer (5/n): Rendering Deterministically" /><published>2021-12-24T00:00:00-08:00</published><updated>2021-12-24T00:00:00-08:00</updated><id>https://pharr.org/matt/blog/2021/12/24/debugging-renderers-rendering-deterministically</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/12/24/debugging-renderers-rendering-deterministically.html">&lt;p&gt;Deterministic program execution has a lot going for it.  For most programs,
it’s the natural way of being: for any particular input, the program
generates the same output.  Determinism makes debugging much easier, as it
saves you from having to re-run the system repeatedly to trigger a bug that
only happens sometimes, and it’s great for end-to-end tests, since you can
safely make strict assertions about cases where the program’s output should
remain absolutely unchanged (e.g., that &lt;a href=&quot;/matt/blog/2021/12/19/debugging-renderers-end-to-end-tests.html#when-the-images-should-not-change-at-all&quot;&gt;float parser
example&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;However, deterministic execution doesn’t always come naturally when you’re
rendering, especially when you’re rendering in parallel.  Today’s post will
go into some of the ways that deterministic execution can be lost, talk
about how to maintain determinism, and then finish with some further
discussion of its benefits.&lt;/p&gt;

&lt;h2 id=&quot;the-basics&quot;&gt;The Basics&lt;/h2&gt;

&lt;p&gt;To start, let’s settle on a more precise definition of deterministic
rendering than “same input gives same output.”  It is too much to ask for
bit accuracy in output across machines; not only will we encounter
different standard math libraries with different levels of precision, but
there are a number of corners of C++ that allow for things like variation
in &lt;a href=&quot;https://en.cppreference.com/w/cpp/language/eval_order&quot;&gt;order of
evaluation&lt;/a&gt; across
compilers that can lead to innocuous differences in output.&lt;/p&gt;

&lt;p&gt;Therefore, we’ll define the observable effect of determinism as: &lt;em&gt;on a
particular system with a particular compiler, repeatedly running the
renderer on the same input always produces the same value at every pixel&lt;/em&gt;.
Implicit in that definition is that the same computations are performed to
compute each pixel’s value, though not necessarily in the same order.  That
definition is plenty for our needs; the benefits from nailing it down
further almost certainly wouldn’t be worth the trouble.&lt;/p&gt;

&lt;p&gt;A render running on a single core should naturally achieve that goal.  If
it does not, fixing that is the first order of business.  Most likely it’s
an uninitialized memory access, other memory corruption, or code somewhere
that randomly seeds a random number generator based on something that
varies like the process id or current time.  (I won’t say more about fixing
those sorts of problems here, as it’s all rendering-independent and is
regular everyday debugging.)&lt;/p&gt;

&lt;p&gt;Rendering in parallel is when things get more complicated.  Indeed, none of
the versions of pbrt before the latest,
&lt;a href=&quot;https://github.com/mmp/pbrt-v4&quot;&gt;pbrt-v4&lt;/a&gt;, was deterministic.  That was
always a minor annoyance when debugging and testing the system, though I
honestly didn’t realize what a productivity drag it was until determinism
was achieved.&lt;/p&gt;

&lt;h2 id=&quot;consistent-samples&quot;&gt;Consistent Samples&lt;/h2&gt;

&lt;p&gt;For rendering to be deterministic, the Monte Carlo sampling routines must
use exactly the same random sample points at every sample taken in every
pixel.  If they are not, then determinism is lost from the start, since
different rays will be traced each time due to slightly different rays
leaving the camera, different sampling decisions will be made at
intersections, and so forth.  One might assume that deterministic is the
natural way of being for the
&lt;a href=&quot;https://pbr-book.org/3ed-2018/Sampling_and_Reconstruction/Sampling_Interface#BasicSamplerInterface&quot;&gt;Sampler&lt;/a&gt;s
that generate those points, but that was not so prior to pbrt-v4.  There
were two issues: the placement of low discrepancy point sets and carried
state in samplers that led to nondeterminism with multithreading.&lt;/p&gt;

&lt;p&gt;When using low discrepancy point sets like Halton points, pbrt-v3 aligns
the origin of the points with the upper left pixel of the image.  That’s
normally \((0,0)\), but then if the user specifies a crop window to
render just part of the image the low discrepancy points all shift in
compensation.  That was always a bother for debugging since you couldn’t
narrow in on a problem pixel without perturbing all of the samples and
often no longer hitting the bug.  That detail was easy enough to fix given
attention to it.&lt;/p&gt;

&lt;p&gt;The other issue came from the fact that each thread maintains its own
&lt;code class=&quot;highlighter-rouge&quot;&gt;Sampler&lt;/code&gt; instance.  This way samplers can maintain state that depends on
the current pixel and pixel sample (e.g., an offset into the Halton
sequence).  Many samplers also use pseudorandom number generators (RNGs) in
their work; those, too, are per-sampler state.  (For example, the
stratified sampler uses a RNG to jitter sample locations and low
discrepancy samplers use RNGs for randomization via scrambling.)&lt;/p&gt;

&lt;p&gt;In pbrt-v3, those per-sampler RNGs are seeded once at system startup time
and then chug along, generating random numbers as requested.  Because
threads are dynamically assigned to work on regions of the image, they may
not work on the same pixels over multiple runs.  In turn, the values that a
RNG returns at a pixel both depends on which thread was assigned that pixel
as well as how many random numbers it had supplied previously for other
pixels.&lt;/p&gt;

&lt;p&gt;The fix was easy: reseed the RNG before generating sample points at a
particular pixel sample.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;Sampler&lt;/code&gt; interface includes a
&lt;code class=&quot;highlighter-rouge&quot;&gt;StartPixelSample()&lt;/code&gt; method that is called before samples are requested at
a given pixel sample, so it’s just a few lines of code to put those RNGs in
a known state.  Here’s that method in &lt;code class=&quot;highlighter-rouge&quot;&gt;IndependentSampler&lt;/code&gt;, which generates
uniform independent samples without any further nuance:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;StartPixelSample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Point2i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampleIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SetSequence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Advance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampleIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;65536ull&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There are two things to note in &lt;code class=&quot;highlighter-rouge&quot;&gt;StartPixelSample()&lt;/code&gt;’s implementation.
First, pbrt uses the &lt;a href=&quot;https://www.pcg-random.org/index.html&quot;&gt;PCG&lt;/a&gt; RNG,
which allows the specification of both a particular sequence of
pseudorandom values as well as an offset into that sequence.  Thus, we
choose a sequence according to the pixel coordinates and then offset into
it according to the index of the sample being taken in the pixel.&lt;/p&gt;

&lt;p&gt;The other thing to mention there is &lt;code class=&quot;highlighter-rouge&quot;&gt;Hash()&lt;/code&gt;, which has been useful all
over the place in pbrt-v4.  Here is its signature:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can pass a bunch of values or objects straight away to it and it
marshals them up and passes them to
&lt;a href=&quot;https://en.wikipedia.org/wiki/MurmurHash&quot;&gt;MurmurHash&lt;/a&gt; to hash
them.&lt;sup id=&quot;fnref:padding&quot;&gt;&lt;a href=&quot;#fn:padding&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; In its use in the &lt;code class=&quot;highlighter-rouge&quot;&gt;IndependentSampler&lt;/code&gt;, we also allow the
user to specify a seed for random number generation; &lt;code class=&quot;highlighter-rouge&quot;&gt;Hash()&lt;/code&gt; makes
it simple to mush that together with the current pixel coordinates to
choose a pseudorandom sequence for the current pixel.&lt;/p&gt;

&lt;p&gt;There is, needless to say, a short &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/64c0a5cc0b29d6c6ffdacc53f93bc714e047e3b0/src/pbrt/samplers_test.cpp#L15&quot;&gt;unit
test&lt;/a&gt;
that ensures all of the samplers consistently generate the same sample values.&lt;/p&gt;

&lt;h2 id=&quot;other-moments-of-randomness&quot;&gt;Other Moments of Randomness&lt;/h2&gt;

&lt;p&gt;Samplers were much of the trouble in bringing pbrt-v4 into the land of
deterministic output, though two other places in the system that made
random decisions without the involvement of a sampler needed attention.&lt;/p&gt;

&lt;p&gt;First was a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/64c0a5cc0b29d6c6ffdacc53f93bc714e047e3b0/src/pbrt/cpu/primitive.cpp#L57&quot;&gt;stochastic alpha
test&lt;/a&gt;,
deep in the primitive intersection code.  For shapes that have an alpha
texture assigned to them, we’d like to ignore any intersections where the
alpha texture is zero and randomly accept ones with fractional alpha with
probability according to their alpha value.  The sampler isn’t available in
the ray intersection routines and keeping a persistent RNG in that code has
obvious problems, so here is what we do instead:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;si&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Possibly ignore intersection based on stochastic alpha test
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;Float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashFloat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Ignore this intersection and trace a new ray
&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;[...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Given a less-than-one alpha value, a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;HashFloat()&lt;/code&gt; gives a uniform
random floating-point value between 0 and 1.  It’s a buddy of &lt;code class=&quot;highlighter-rouge&quot;&gt;Hash()&lt;/code&gt; and
is also happy to take whichever-all values you pass it to turn into a
random floating-point value.  (Above, it’s the ray origin and direction.)&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashFloat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Thus, the results are deterministic for any given ray.&lt;/p&gt;

&lt;p&gt;The second case was in pbrt-v4’s &lt;code class=&quot;highlighter-rouge&quot;&gt;LayeredBxDF&lt;/code&gt; class, which implements &lt;a href=&quot;https://shuangz.com/projects/layered-sa18/&quot;&gt;Guo
et al.’s algorithm&lt;/a&gt; for
stochastic evaluation and sampling of the BRDFs of layered materials.  That
needs an unbounded number of independent random samples, so we instantiate
an RNG for each evaluation, but &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/64c0a5cc0b29d6c6ffdacc53f93bc714e047e3b0/src/pbrt/bxdfs.h#L513&quot;&gt;seed it via the incident and outgoing
directions&lt;/a&gt;.
Thus again, for any pair of directions passed to the BRDF evaluation
method, the same set of random samples will be generated and the returned
value will be deterministic.&lt;/p&gt;

&lt;h2 id=&quot;consistent-pixel-sums&quot;&gt;Consistent Pixel Sums&lt;/h2&gt;

&lt;p&gt;With what we have so far, the same rays will be traced each
time the renderer runs and in turn, if an assertion fires along the
way, it will do so consistently.  That’s a big benefit for debugging, but
we have not yet achieved deterministic output, which is important for
making end-to-end tests maximally useful.&lt;/p&gt;

&lt;p&gt;The remaining challenge lies in summing sample values to compute each
pixel’s final value. Because floating-point addition is not associative, if
the image samples that contribute to a pixel are not accumulated carefully
the order of summation may be different across different runs of the
program and so the output may change.  That was a problem in pbrt-v3 due to
how it computed final pixel values: there, the image is decomposed into
rectangular regions that are assigned to threads and threads generate samples
within their regions, updating the pixels that each sample contributes to.&lt;/p&gt;

&lt;p&gt;This figure illustrates the problem with that, showing all of the samples
that contribute to a particular output pixel (black dot):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/matt/blog/images/thread pixel sampling.svg&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have two threads responsible for adjacent \(4 \times 4\) pixel regions of the
image (thick boxes).  For an output image pixel near the boundary of the
two regions that has a reconstruction filter that is wider than the pixel
spacing (shaded circle), some of the samples that contribute will be taken
by thread 1 (orange dots) and some will come from samples taken by thread 2
(blue dot).  Because the threads are independent, the filtered sample
values are not accumulated in a deterministic order and thus, the final
pixel value is not deterministic.&lt;/p&gt;

&lt;p&gt;pbrt-v4 addresses this issue by adopting Ernst et al.’s &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.3579&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;filter importance
sampling&lt;/a&gt;
approach.  Independent samples are taken for each output pixel, with no
sample sharing with other pixels.  If only a single thread works on a pixel
at a time, then the samples for each output pixel are naturally generated
in a consistent order, giving a consistent sum.  (Filter importance
sampling has a number of additional advantages that are detailed in the
paper, including better preservation of the benefits of high-quality
sampling patterns.)  With that tuned up, we (almost) have deterministic
output.&lt;/p&gt;

&lt;h2 id=&quot;those-pesky-splats&quot;&gt;Those Pesky Splats&lt;/h2&gt;

&lt;p&gt;One more thing… pbrt-v4’s output is not quite deterministic if a light
transport algorithm that traces paths starting from the light sources is
being used.  In that case, light path vertices are splatted into the image
at whichever pixel they are visible; if multiple threads end up splatting
into the same pixel, then we are back to nondeterminism from unordered
floating-point addition.&lt;/p&gt;

&lt;p&gt;This issue could be addressed by having each thread splat into its own
image and then summing the images at the end, though that would incur a
cost in memory use that scales with the number of threads.  Alternatively,
we might use fixed-point rather than floating-point to store those pixel
values.  For now that issue is unaddressed; it rarely causes any trouble,
especially since those splatted values are accumulated in double precision
and generally converted all the way down to half-float precision for
storage.  Most of the time that loss of precision hides any sloppy sums.&lt;/p&gt;

&lt;h2 id=&quot;the-joys-of---debugstart&quot;&gt;The Joys of --debugstart&lt;/h2&gt;

&lt;p&gt;The greatest benefit of deterministic rendering has been the ability to
quickly iterate on bugs: you can add some logging code or more assertions,
recompile, and re-render, confident that the new code will see the same
inputs as triggered the bug.  Given samplers that give exactly the same
samples at each pixel also means that you can speed things up by just
rendering a crop window or even a single pixel as you’re chasing a bug.&lt;/p&gt;

&lt;p&gt;Even better, it was easy to go even further and add support for retracing
just a single offending ray path.  pbrt-v4 has a &lt;code class=&quot;highlighter-rouge&quot;&gt;CheckCallbackScope&lt;/code&gt; class
that uses RAII to register a callback function that will run if an
assertion fails or if the renderer crashes.  Here is how it is used in most
of pbrt’s CPU integrators:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;thread_local&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Point2i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadPixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;thread_local&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadSampleIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CheckCallbackScope&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringPrintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Rendering failed at pixel (%d, %d) sample %d. Debug with &quot;&lt;/span&gt;
                        &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--debugstart %d,%d,%d&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;threadPixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadPixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadSampleIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;threadPixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadPixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threadSampleIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As rendering proceeds, each thread keeps its thread-local &lt;code class=&quot;highlighter-rouge&quot;&gt;threadPixel&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;threadSampleIndex&lt;/code&gt; variables up to date and if the renderer aborts due to
an error, you get a message like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Rendering failed at pixel (915, 249) sample 83. Debug with &quot;--debugstart 915,249,83&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;at the bottom of the crash output.  If you then rerun pbrt passing it that
&lt;code class=&quot;highlighter-rouge&quot;&gt;--debugstart&lt;/code&gt; option, a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/64c0a5cc0b29d6c6ffdacc53f93bc714e047e3b0/src/pbrt/cpu/integrators.cpp#L68&quot;&gt;specialized code
path&lt;/a&gt;
traces just that single ray path in the main thread of execution.  That
gives a simpler debugging context than launching a bunch of threads and
waiting for the bug to hit again; it’s delightfully helpful for bugs that
otherwise only happen after a substantial amount of time has gone by.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’ve made it past “detecting rendering bugs” and have made our way to
“reliably replicating those bugs.”  Next time will be a few thoughts about
performance bugs before we get into actual debugging techniques.&lt;/p&gt;

&lt;h2 id=&quot;note&quot;&gt;note&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:padding&quot;&gt;
      &lt;p&gt;The attentive reader of &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/64c0a5cc0b29d6c6ffdacc53f93bc714e047e3b0/src/pbrt/util/hash.h#L121&quot;&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;Hash()&lt;/code&gt; implementation&lt;/a&gt; will note that if a struct or class that
        has padding between elements is passed to it, the
        results may be nondeterministic since it hashes their in-memory contents
        directly. It would be nice to use a C++ SFINAE trick to get a
        compilation error in that case, but I’m not aware of a way to
        detect that at compile time. &lt;a href=&quot;#fnref:padding&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Making a renderer fully deterministic—the same input always giving exactly the same output—has a few tricky corners that were never all addressed in pbrt until the latest version. Achieving that determinism has all sorts of benefits for testing and debugging.</summary></entry><entry><title type="html">Debugging Your Renderer (4/n): End-to-end tests (or, “why did that image change?”)</title><link href="https://pharr.org/matt/blog/2021/12/19/debugging-renderers-end-to-end-tests.html" rel="alternate" type="text/html" title="Debugging Your Renderer (4/n): End-to-end tests (or, &quot;why did that image change?&quot;)" /><published>2021-12-19T00:00:00-08:00</published><updated>2021-12-19T00:00:00-08:00</updated><id>https://pharr.org/matt/blog/2021/12/19/debugging-renderers-end-to-end-tests</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/12/19/debugging-renderers-end-to-end-tests.html">&lt;p&gt;Here we are, three posts into the meat of this series, and we’re still on
the topic of determining if the renderer is buggy in the first place—the
actual craft of debugging has not yet seen much discussion.  We’re getting
there—I promise—but I’m going to finish discussing ways of detecting
bugs before getting into fixing them.&lt;/p&gt;

&lt;p&gt;Beyond unit tests, I’ve also found that having a good set of end-to-end
rendering tests is of enormous benefit.  In this context, the idea of an
end-to-end test is simple: you render an image of a scene and then check
the image to make sure it is correct.&lt;/p&gt;

&lt;p&gt;There’s plenty of nuance in that sentence: which scene?  (And not just one,
right?)  How do you check whether the output is correct?  Needless to say,
it’s “many scenes,” and as we’ll see, verifying correctness from an image
can be as much art as science.  We’ll dig into all of those questions
today.&lt;/p&gt;

&lt;h2 id=&quot;building-a-library-of-test-scenes&quot;&gt;Building a Library of Test Scenes&lt;/h2&gt;

&lt;p&gt;I’ve been collecting scenes to use for testing pbrt for at least a decade;
there are upward of 600 of them in the test suite today.  Most of them
don’t make pretty pictures and some output very low resolution images. Some
are as small as \(10 \times 10\) pixels—nothing much to look at at all.
They can be split into a few categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simple scenes with analytic solutions.&lt;/li&gt;
  &lt;li&gt;Scenes that target a single renderer feature.&lt;/li&gt;
  &lt;li&gt;Complex(ish) scenes.&lt;/li&gt;
  &lt;li&gt;Reproduction cases for user-reported bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each type is valuable.  Take the scenes with analytic
solutions: one such scene is a diffuse sphere with radius 1, a reflectance
of 0.5, and a point light with intensity \(\pi\) at its center.  Put a
camera inside that thing and render it with your path tracer: if your
pixels don’t all have a value very close to 1 (given sufficient samples),
you’ve got a bug.  Stop right there, fix it, and be happy you had such an
easy way to detect something was off.&lt;/p&gt;

&lt;p&gt;You can take that scene and easily make variations of it.  Replace that
single point light with four point lights with intensities that sum to
\(\pi\)—that should be all ones as well.  Or Take out the point light
and make the interior of the sphere emissive with spatially- and
directionally-uniform radiance of 0.5, leaving the diffuse reflectance at
0.5.  Once again, you should get pixels that are all 1.  That emissive
sphere you can make bigger or smaller; it should be all ones if you make a
variant with a different radius.&lt;/p&gt;

&lt;p&gt;Once you start thinking in terms of scenes where you can work out the
correct answer, there’s lots more you can do.  You could light a diffuse
quad with an infinite light source and then again with an emissive sphere
surrounding it.  You could test your bidirectional algorithms by putting a
glass where with an index of refraction of 1 around the quad; in principle,
that should have no effect.&lt;/p&gt;

&lt;p&gt;And then you can also make variants of those variants that exercise all of
the different sample generation algorithms and light transport algorithms;
each of those is just a small change to the scene description file, so
getting up to 600 doesn’t need to go one at a time.&lt;/p&gt;

&lt;p&gt;The analytic scenes rarely fail once you’ve gotten them working the first
time, but when they do, the debugging problem is a relatively easy
one—much nicer than “images of the Moana Island scene are too dark when
the bidirectional path tracer is used.”  For example, for the scene with a
single point light, every ray should return the same value—at each
intersection point, the reflected radiance due to direct lighting should be
0.5 and then the indirect radiance (also 0.5) should be scaled by 0.5.
(Expand out that series and you get your expected value of 1.)&lt;/p&gt;

&lt;p&gt;Of course, those scenes may all render correctly and you may well still
find that the Moana Island scene is still too dark with your bidirectional
path tracer, but you’ve at least carved off the easy-to-fix cases in a way
that makes them easy to debug.&lt;/p&gt;

&lt;p&gt;For most of the renderer’s capabilities, it’s not too hard to come up with
a simple scene that targets that feature without exercising too many other
parts of the renderer.  Those are also useful to have in end-to-end
tests. As an example, pbrt’s test suite includes a scene comprised of a
single quad with a high-frequency texture viewed at an oblique angle.  The
BSDF is diffuse, it’s lit by a directional light, there’s no complex
visibility or multiple light scattering.  This is it:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;400&quot; height=&quot;400&quot; src=&quot;/matt/blog/images/aa-perspective.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That scene is effectively a test of pbrt’s ray differentials and texture
filtering code.  If one makes a change to the renderer and then that scene
goes bad, you can make a good guess about where the bug lies from the
limited subset of the rendering code that runs in generating it.  In such a
case, if scenes without textures still render correctly then you have a
stronger hint, though if those are also broken, then you have a hint that
texture filtering isn’t your problem.  (Or, that you have multiple
problems.)&lt;/p&gt;

&lt;p&gt;Sometimes things only go wrong in the presence of complexity; a number of
scenes culled from the &lt;a href=&quot;https://github.com/mmp/pbrt-v4-scenes&quot;&gt;pbrt-v4-scenes
distribution&lt;/a&gt; and added to the
end-to-end tests take care of that.  When those scenes fail, it’s usually
the case that simpler ones do as well.  If not, it’s often worth trying to
simplify the more complex scene as much as possible while still hitting the
bug; that, too, is a source of more test scenes for the future.  (More on
that topic in a future post as well.)&lt;/p&gt;

&lt;p&gt;Finally, there are the scenes from user bug reports. I add all of those to
the test suite; not only are they all cases that testing previously wasn’t
rigorous enough to catch, but there’s no reason to risk the embarrassment
(on this end) and annoyance (on the bug reporter’s end) of that same bug
reappearing in the future due to a change to the renderer inadvertently
reintroducing it.&lt;/p&gt;

&lt;p&gt;There is a time versus coverage trade-off in assembling this collection of
scenes: the more scenes you have with the more pixels to render and the
more samples per pixel, the more you’re exercising the renderer.  Yet, the
more of all of that you have, the longer it takes to run the tests.  If
running them takes too long, you won’t run them as often as you should.
I’ve ended up tuning them to be about an hour of single-core CPU time
(though they run on multiple cores, so it’s just a few minutes of
wall-clock time).  As you add scenes and the total time to run all of them
increases, you can judiciously reduce the resolution of some of the tests
or dial down the sampling rate used when rendering them.&lt;/p&gt;

&lt;h2 id=&quot;does-everything-render-to-completion&quot;&gt;Does Everything Render to Completion?&lt;/h2&gt;

&lt;p&gt;So you have a few tens or hundreds of test scenes and, let’s hope, a script
to render all of them and save the images.  What now?  Run that script and
see what happens.&lt;/p&gt;

&lt;p&gt;Most of the corners of the renderer’s code ends up being fairly well
exercised if you have hundreds of varied scenes designed to exercise
it.&lt;sup id=&quot;fnref:coverage&quot;&gt;&lt;a href=&quot;#fn:coverage&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; That’s good news for your assertions, as far as giving them
plenty of variety to assert about.  It’s also encouragement to add more
assertions; sometimes adding a new assertion and running through all of the
existing test scenes will unearth a new failure.  You might even add
expensive assertions for a single run-through of the test scenes to see if
they find anything, planning to debug if so and to remove them or demote
them to debug-only assertions when you’re done.&lt;/p&gt;

&lt;p&gt;Finding a failing assertion in that way really is a good thing, even though
you’ve found more work for yourself.  You’ve got yourself a debugging task
ahead of you but it’s not completely open ended, and it’s on your own
terms without the panic of a user reporting a serious bug where you have
no idea what the cause may be.  It’s also likely with a simpler scene than
a user would have been rendering if they encountered the bug later.&lt;/p&gt;

&lt;p&gt;Assertions aside, the renderer may crash for some or even all of the
scenes.  Same deal with that: a crash is not fun, but better to find it
yourself while running the tests and fix it before your users are bothered
by it.&lt;/p&gt;

&lt;p&gt;A good collection of test scenes is also good fodder for tools like
&lt;a href=&quot;https://valgrind.org&quot;&gt;valgrind&lt;/a&gt;,
&lt;a href=&quot;https://valgrind.org/docs/manual/hg-manual.html&quot;&gt;helgrind&lt;/a&gt;, and assorted
&lt;a href=&quot;https://clang.llvm.org/docs/AddressSanitizer.html&quot;&gt;sanitizers&lt;/a&gt;.  There’s a
much better chance of those sorts of tools finding something if you give
them a variety of rendering computations to examine.  Chasing down any
errors those report is also something you must do before proceeding when
you find them: there’s no way to know how much havoc lies in their wake, so
you might as well fix them once you’re aware of them, lest you spend hours
chasing down some other bug that turned out to be due to one of those.&lt;/p&gt;

&lt;h2 id=&quot;are-the-images-correct&quot;&gt;Are the Images Correct?&lt;/h2&gt;

&lt;p&gt;If all of the scenes render to completion, now you have a few hundred
images sitting on disk.  How do you know if each one is correct?&lt;/p&gt;

&lt;p&gt;For pbrt’s test scenes, I maintain a set of “golden” images that provide a
reference.&lt;sup id=&quot;fnref:gengold&quot;&gt;&lt;a href=&quot;#fn:gengold&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;  The test script then checks the output from the current version
of the renderer with the golden images.  How tricky could that be?  The
first hard problem is generating golden images in the first place.  The
second is determining if a rendered image is correct.  We’ll consider both
topics in turn.&lt;/p&gt;

&lt;p&gt;Creating an initial set of golden images is a bootstrapping problem.  For
the scenes with analytic solutions you can manually verify correctness via their
pixel values, but for the rest it’s not so easy.  I have partially been
able to sidestep that issue by assuming that the last released version of
pbrt is bug free and using its output as a starting point.  While pbrt is
surely not bug free, after it has been out for a few years enough people
have spent enough time with the code that it’s reasonable to assume it’s in
pretty good shape.&lt;/p&gt;

&lt;p&gt;For a different renderer, one might try using the output of pbrt or another
renderer as an initial reference, though that’s tricky business, with
differences in BSDF models, texture filtering, and details like rendering
in RGB versus using spectra.  One can at least make sure that one’s
renderer is in the right ballpark that way, if another renderer is both
trusted and well-understood and if it’s not too hard to render scenes in
both it and your own renderer.&lt;/p&gt;

&lt;p&gt;Another option is to gain confidence in candidate golden images via
experiments.  We’ll come back to this topic in more detail once we get to
debugging techniques, but to understand the idea, let’s consider that
texture filtering test from before.  Say that you’ve implemented ray
differentials and a texture filtering algorithm and can render images that
aren’t obviously wrong.  Lacking a verified solution, how can you become
more confident that they are correct?&lt;/p&gt;

&lt;p&gt;You might render the scene with no texture filtering but with many pixel
samples to get an antialiased image that way.  That’s something to compare
to.  You know that your implementation won’t match that perfectly, but if
it’s too far off you might be suspicious of your differentials’
correctness.  Another useful technique is to explore the parameter space:
render it once with your implementation, then again with your texture
filter widths half as wide as you think they should be, then again with
them twice as wide. You should see aliasing with the narrow filters and
blurring with the wide ones.  If so, you have some more confidence in your
implementation, and if not, you have something to dig into further.&lt;/p&gt;

&lt;p&gt;Here are some images that show the results of applying that approach for
the texture filtering test above; the images are as we would expect.  (The
images are presented using &lt;a href=&quot;https://jeri.io&quot;&gt;jeri&lt;/a&gt;; click on
them and hit ‘f’ to go full screen if necessary to see the differences.)&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;aa-comparison&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('aa-comparison'), {
  title: 'aa-comparison', children: [
 { title: 'Antialiased', image: '/matt/blog/images/aa-perspective.png' },
 { title: '2x filter widths', image: '/matt/blog/images/aa-perspective-blurred.png' },
 { title: '1/2x filter widths', image: '/matt/blog/images/aa-perspective-aliased.png' },
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;At minimum one may decree that the output of the renderer at some point in
time gives the golden images.  Going forward, any deviation in them should
be explained, either from fixing a bug or from a well-understood
improvement to the renderer.&lt;/p&gt;

&lt;h2 id=&quot;when-the-images-should-not-change-at-all&quot;&gt;When the Images Should Not Change at All&lt;/h2&gt;

&lt;p&gt;Given golden images, a change to the renderer, and a run of the end-to-end
tests, we have a set of new images that may or may not the match the golden
images.  How one feels about that depends on the sort of change one has
made.  Here are a few representative cases where not a single pixel of a
single image should be different:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;A pooled memory allocator was introduced to optimize small memory
allocations.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;An optimized routine for parsing text floating-point values in the scene
description was adopted.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;The function that loads image texture maps has been parallelized to
reduce start-up time.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For all of those cases, there’s no reasonable explanation for why anything
should change in the final output, yet sometimes you make a change like
that and find differences.  If it’s major differences, then presumably
you’ve broken something fundamental; the debugging problem in those cases
is often not too bad due to the wide impact.  Choose the one of the
simplest scenes that went astray and take it from there.&lt;/p&gt;

&lt;p&gt;For minor differences, it’s also critical to understand what happened.  It
can hard to be disciplined about that: if it’s just one pixel in one scene
out of hundreds of scenes with perhaps billions of pixels changes after you
replaced the float parser, it’s easy to tell yourself that a single float
was parsed differently and hey, quite possibly you just fixed a bug you
didn’t know you had.  Yet something more serious may be lurking; it may
just be that your tests only hit a buggy case once but other scenes would
hit it often.  If you don’t understand the root cause, you’re building the
rest of the system on sand.&lt;/p&gt;

&lt;p&gt;For the case of the float parser, it’d be crucial to track down which float
(or floats) went astray and why—keep both parsers around, call both for
each float parsed, and assert that both give the same result.  When they
disagree, figure out which one was correct.  Your assertion may never fire,
which would be “interesting” as well; it may be that the pixel change was
not due to a difference in parsing floats but was due to some other bug
that was tickled by your changes.  Those sorts of bugs aren’t fun to chase
down but are equally important to understand when you encounter them.&lt;/p&gt;

&lt;p&gt;Implicit in these imperative statements about no pixels changing has been
the assumption that the renderer is &lt;em&gt;deterministic&lt;/em&gt;—that rendering the
same scene gives exactly the same output image.  For now we will take that
as given.  Making the renderer so is tricky but worthwhile; that will be
the sole topic of the next post in this series.&lt;/p&gt;

&lt;h2 id=&quot;when-the-images-may-change&quot;&gt;When the Images May Change&lt;/h2&gt;

&lt;p&gt;Whenever changes are made to code involving ray tracing, other geometric
computations, or light transport algorithms, it’s almost inevitable that
images will change.  This brings us to the tricky question of “are those
changes ok, or suggestive that there is a bug?”&lt;/p&gt;

&lt;p&gt;To motivate this case, let’s consider a (real) example: making what is
believed to be an improvement to the algorithm that makes sure that rays
leaving bilinear patches do not incorrectly reinstersect the patch.
Assuming that we had a reasonable algorithm for this previously, we
would expect very small changes in the images for every scene that has
bilinear patches in it, but would not expect any big image changes.
(Though we might hope to have a scene that shows a case where the current
algorithm is insufficient, in which case we would hope for significant
and visually evident improvement with it.)&lt;/p&gt;

&lt;p&gt;My testing script uses pbrt’s &lt;em&gt;imgtool&lt;/em&gt; program to compare the output
images to the golden images.  It prints nothing when they match exactly, so
if you’ve just changed the float parser, you might run the end-to-end
tests, wait for them to finish, and move along happily if nothing is
reported.  When there is a discrepancy, &lt;em&gt;imgtool&lt;/em&gt;’s output is like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/matt/blog/images/blp-imgtool.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That output is carefully crafted. The three lines in turn:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The news: the images are different.&lt;/li&gt;
  &lt;li&gt;The pathnames of the two images, relative to the current
directory. These are there alone and together on a line so that it’s
easy to triple click that line to select it, then type the name of an
image viewer in the shell, paste the selection, hit return, and then view the two
images.&lt;/li&gt;
  &lt;li&gt;Numerical details about the images and how they differ.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Those details include the average value of all of the pixels in each image,
their percentage difference, and their mean squared difference.
Often those numbers alone are enough to indicate what’s going on.
If we saw something like the above for all of the test scenes that had
bilinear patches in them (minuscule differences in average pixel values and
MSE), we could be fairly confident that all was well.  It would still be
worth a quick glance at a few of the images, but there would be no need to
view all of them to feel good about the change.&lt;/p&gt;

&lt;p&gt;With that workflow in mind, &lt;em&gt;imgtool&lt;/em&gt; offers some color in its output to
make it easier to see higher levels of error.  Here’s what it said about
another scene after I made that change:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/matt/blog/images/splash-imgtool.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That red text says “this seems a little high”, and indeed it is so—here
are the corresponding images:&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; width: 100%; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;bad-splash&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('bad-splash'), {
  title: 'bad-splash', children: [
 { title: 'Test Image', image: '/matt/blog/images/run-splash.pbrt.png' },
 { title: 'Golden Image', image: '/matt/blog/images/golden-splash.pbrt.png' },
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Something funny is happening at the boundary of the liquid at the top of
the cup; it is evident that one of two images must be wrong, though it
isn’t obvious which one is.  Time to start debugging.&lt;/p&gt;

&lt;h2 id=&quot;using-statistics-to-your-advantage&quot;&gt;Using Statistics to Your Advantage&lt;/h2&gt;

&lt;p&gt;For the bilinear patch intersection example, the image statistics are
useful for giving a good first indicator of “all is well”, “something may
be fishy here”, or “things are Not Good.”  That is plenty useful, but when
one is making changes to Monte Carlo sampling code, those numbers have even
greater value.  Consider improving a BRDF importance sampling routine 
to better match the BRDF.  In that case, we hope for significant
image changes for the better thanks to lower error.  How do we distinguish
between an improvement in error and an incorrect result?&lt;/p&gt;

&lt;p&gt;Just looking at the images may not be enough.  Consider these three images
of the San Miguel scene where the first is the baseline reference and the
others correspond to two different changes to the renderer, one correct and
one buggy.  It’s not evident from just looking at the images which one
is wrong.&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; width: 100%; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;sanmiguel&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('sanmiguel'), {
  title: 'sanmiguel', children: [
 { title: 'Reference', image: '/matt/blog/images/sanmiguel-ref.exr' },
 { title: 'Change A', image: '/matt/blog/images/sanmiguel-a.exr' },
 { title: 'Change B', image: '/matt/blog/images/sanmiguel-b.exr' },
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;However, &lt;em&gt;imgtool&lt;/em&gt; has something interesting to report: the average pixel
value of “Change A” is 0.17% higher than the reference image, but the
average pixel value of “Change B” is 4.03% higher.  In the context of
unbiased Monte Carlo, a 4% change is most definitely a sign of something
going wrong.&lt;/p&gt;

&lt;p&gt;One way to think about why this is so is that if you’re using unbiased
Monte Carlo algorithms, rendering images of thousands of pixels, each with
tens of samples, then you have hundreds of thousands or even millions of
sample values that feed into that average.  If you have changed your
importance sampling routines (and your estimators don’t have ridiculously
high variance), then those average image values should be well locked
in if both “before” and “after” are bug-free.&lt;/p&gt;

&lt;p&gt;That idea also explains why that San Miguel test has a fairly low sampling
rate—just 16 samples per pixel.  You often don’t need to render the whole
image to convergence to tell if the Monte Carlo bits have gone wrong; the
statistics over all of the pixels often tell the tale.&lt;/p&gt;

&lt;p&gt;But how do you know how much of a change is acceptable?  Is that 0.17%
something to worry about?  In practice, it depends; the answer depends on
how many samples you’re taking and how much variance there is in your
estimators.  For pbrt’s tests, I’ve learned to have a sense of what’s
expected, but that’s admittedly imprecise.  A much better way would be to
follow the ideas presented in Kartic Subr and Jim Arvo’s &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/K.Subr/research.html#HypothesisMCEstimators&quot;&gt;paper on
applying proper statistical tests to these
tasks&lt;/a&gt;.
They show not only the right way to decide if two images have the same
mean, accounting for the number of samples taken in setting a threshold,
but also showing how to robustly determine the answer to questions like
“does image a have lower variance than image b?”&lt;/p&gt;

&lt;p&gt;For all of these evaluations of images, it’s crucial that images are stored
in floating point, not clamped, and without any tone mapping or gamma
correction.  When you’re making images for people to look at, you’re more
than welcome to use 8-bit PNGs and run your pixels through the ACES curve
for a “filmic” look.  For the purposes of end-to-end tests, maintaining
good old linear values with their full dynamic range is the only thing that
allows you to reason about what’s going on with them statistically.&lt;/p&gt;

&lt;p&gt;Finally, even if the numbers look good, it’s still important to view the
images, or at least all of those ones with the greatest reported
differences.  A shortcoming of those image-wide statistics is that they
don’t indicate whether the error has some unsightly structure to it that is
sneaking under the radar.  One way to better automate that test would be to
also use a perceptual error metric like
&lt;a href=&quot;https://research.nvidia.com/publication/2020-07_FLIP&quot;&gt;ꟻlip&lt;/a&gt;, though
that requires high-quality reference images, which pbrt’s end-to-end tests
currently avoid in the interests of running more quickly.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This has turned into a longer post than I intended and there’s still plenty
more to say, especially about the tricky problem of having two rendered
images and trying to figure out what their differences signify.  We will
most certainly come back to that in following posts since it is frequently
integral to the renderer debugging process.&lt;/p&gt;

&lt;p&gt;The best thing about having a good set of tests—both unit and
end-to-end—is being able to iterate on code with confidence.  You can
refactor swaths of the system, you can cleanup things that are a little
grungy, and if the tests are clear, you can feel confident about committing
those changes.  Sometimes you can try out speculative ideas—things where
you’re not sure if the idea is right—and quickly gather some empirical
data about whether the idea works or not.  If those indicators are
promising and you pursue your idea you should still find better ways to
validate it, but I’ve found that a quick yes/no can be a helpful guide.&lt;/p&gt;

&lt;p&gt;Next time we’ll go into the details of making a renderer deterministic,
which is one of the foundations of everything discussed today.  That post
will certainly be less to digest than this one was.&lt;/p&gt;

&lt;h1 id=&quot;notes&quot;&gt;notes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:coverage&quot;&gt;
      &lt;p&gt;The Right Thing to do would be to use a tool that measures code
         coverage, see which parts of the renderer never or rarely run
         given your test scenes, and to introduce new scenes
         intentionally to exercise that code.  Admittedly, I have not
         yet found that discipline for pbrt. &lt;a href=&quot;#fnref:coverage&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gengold&quot;&gt;
      &lt;p&gt;Note that the golden images must be generated from scratch for
        each operating system and compiler used, as differences in
        details like precision in the system math library usually leads
        to minor image differences across different systems. &lt;a href=&quot;#fnref:gengold&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Still in the thick of the task of detecting the presence of bugs in a renderer in the first place, this time the focus is on the value of a large suite of test scenes. Soon soon we will turn to what to do about all of these bugs when we find them.</summary></entry><entry><title type="html">Debugging Your Renderer (3/n): Assertions (and on not sweeping things under the rug)</title><link href="https://pharr.org/matt/blog/2021/12/02/debugging-renderers-assertions.html" rel="alternate" type="text/html" title="Debugging Your Renderer (3/n): Assertions (and on not sweeping things under the rug)" /><published>2021-12-02T00:00:00-08:00</published><updated>2021-12-02T00:00:00-08:00</updated><id>https://pharr.org/matt/blog/2021/12/02/debugging-renderers-assertions</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/12/02/debugging-renderers-assertions.html">&lt;p&gt;Today we’ll keep the discussion to the topic of runtime assertions in
renderers; next time it’ll be on to end-to-end tests, which will start
to lead us into a more image-focused view of graphics debugging that will
keep us busy for a while.&lt;/p&gt;

&lt;p&gt;A principle in the last post on &lt;a href=&quot;/matt/blog/2021/11/26/debugging-renderers-unit-tests.html&quot;&gt;unit testing for
renderers&lt;/a&gt; was
the idea that you’d like your debugging problem to be as simple as
possible; one way to achieve that is if bugs manifest themselves in a way
other than “some of these pixels don’t look right…”  While there will
always be plenty of that sort of bug, those are usually a much harder
debugging problem than a conventional one like “the program printed an
error and crashed.”  A good set of runtime assertions can be an effective
way to turn obscure bugs into more obvious ones.&lt;/p&gt;

&lt;p&gt;An assertion is a simple thing: a statement that a condition is always true
at some point in the execution of a program.  It seems that the original
idea of them dates to Goldstine and von Neumann in 1947.&lt;sup id=&quot;fnref:firstassert&quot;&gt;&lt;a href=&quot;#fn:firstassert&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; If
such a statement is ever found to be false, then a fundamental assumption
underlying the system’s implementation has been violated.  The
implications—to the performance of the program or to the correctness of
its output—may be wide-ranging and possibly impossible to recover from.
Assertions a great way to catch little things early before they turn into
big things that are only evident much later.&lt;/p&gt;

&lt;p&gt;In contrast to unit tests, which just have to be fast enough to not be
annoying to run often, assertions must be efficient, since they often run
in the innermost loops of the renderer.  In return, they have the advantage
that they can check many more situations than a unit test. It turns
out that a myriad of unexpected edge cases come up as you trace billions of
rays in many different scenes.  Yet an assertion that has no chance of
firing is only a drag on overall performance without offering any value.
The art is to write the ones that you don’t think will ever fire but yet
sometimes do so.&lt;/p&gt;

&lt;p&gt;For a well-written general discussion of assertions, see &lt;a href=&quot;https://blog.regehr.org/archives/1091&quot;&gt;John Regehr’s
blog post on the topic&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-basics&quot;&gt;The Basics&lt;/h2&gt;

&lt;p&gt;While C++ provides an &lt;a href=&quot;https://en.cppreference.com/w/cpp/error/assert&quot;&gt;assert
macro&lt;/a&gt; in the standard
library, it has a few shortcomings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Assertions are either enabled or disabled, via the &lt;code class=&quot;highlighter-rouge&quot;&gt;NDEBUG&lt;/code&gt; macro. Often,
they are disabled completely for optimized builds, which in turn means that
they run rarely and do not catch many bugs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When an assertion fails, only the text of the assertion (e.g., “x &amp;gt; 0”)
and its location in the source code is printed without any further
context.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pbrt-v4 therefore has its &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/c4cfd6679e436d512bed5b03fed33a1971d8ee6d/src/pbrt/util/check.h#L36&quot;&gt;own set of assertion
macros&lt;/a&gt;,
which are also integrated with pbrt’s runtime logging system.  pbrt’s
assertion macros are based on &lt;a href=&quot;https://github.com/google/glog#check-macros&quot;&gt;those in Google’s glog
package&lt;/a&gt;.  It includes
assertions that are always included, even in release builds, and those that
are only for debug builds, where more costly checks may be acceptable.
They also provide much more helpful information than &lt;code class=&quot;highlighter-rouge&quot;&gt;assert()&lt;/code&gt; does when
an assertion fails.&lt;/p&gt;

&lt;p&gt;Beyond a basic Boolean assertion (&lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK()&lt;/code&gt;), there are separate assertions
for checking equality, inequality, and greater-than/less-than.  For
example, &lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK_GE()&lt;/code&gt; checks that the first value provided to it is greater
than or equal to the second.  Here is an example of its use in pbrt:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CHECK_GE(1 - pAbsorb - pScatter, -1e-6);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There’s a bit of context packed into that simple check: we have two
probabilities, &lt;code class=&quot;highlighter-rouge&quot;&gt;pAbsorb&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pScatter&lt;/code&gt;, and if you look at the code
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/c4cfd6679e436d512bed5b03fed33a1971d8ee6d/src/pbrt/cpu/integrators.cpp#L999&quot;&gt;before
it&lt;/a&gt;
you can see that the light transport algorithm has just computed three probabilities
where the third, &lt;code class=&quot;highlighter-rouge&quot;&gt;pNull&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;1 - pAbsorb - pScatter&lt;/code&gt;.  Thus, the assertion is
effectively making sure that we are using valid probabilities when
computing &lt;code class=&quot;highlighter-rouge&quot;&gt;pNull&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;More broadly, that check is in the context of pbrt’s code for sampling
volumetric scattering.  That code requires that the volumetric
representation provide a majorant that bounds the density of the volume
over a region of space.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK_GE()&lt;/code&gt; then is effectively checking that
the majorant is a valid bound.  Thus, it’s really a check on the
validity of the code that computes those bounds, which is &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/c4cfd6679e436d512bed5b03fed33a1971d8ee6d/src/pbrt/media.cpp#L552&quot;&gt;far away in the
system&lt;/a&gt;
from where the check is made.&lt;/p&gt;

&lt;p&gt;While that decoupling has the disadvantage that a failing assertion may
require searching to find the code actually responsible for the bug, the
advantage is that the check is made at every sample taken in every
volumetric medium that is provided to pbrt for rendering; it gives the
majorant computations a thorough workout.  That check has found many bugs
in that code since it was introduced; there are plenty of corner cases in
the majorant computations, especially when you’re doing trilinear
interpolation, which requires considering a larger footprint, and also
using the nested grid representation of
&lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3450623.3464653&quot;&gt;NanoVDB&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If that assertion fails, pbrt dumps more information than just the text of
the assertion:&lt;sup id=&quot;fnref:digits&quot;&gt;&lt;a href=&quot;#fn:digits&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ tid 12129819 @     1.252s cpu/integrators.cpp:1004 ]
    FATAL Check failed: 1 - pAbsorb - pScatter &amp;gt;= -1e-6
        with 1 - pAbsorb - pScatter = -0.3336507, -1e-6 = -0.000001
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In addition to the id of the thread in which the assertion failed, we have
the elapsed time since rendering began (about 1.25 seconds here), the
location of the assertion in the source code, what was asserted, as well as
both of the values that were passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK_GE()&lt;/code&gt;.  Having those values
immediately at hand is often helpful.  In the best case, one can understand
the bug immediately, for example by seeing that an edge case that had been
assumed to be impossible actually happens in practice.  For this one,
knowing whether the value was slightly outside of the limit or far outside
of the limit (as it was here) may be a good starting point for further
investigation.&lt;/p&gt;

&lt;p&gt;A full stack trace then follows; that, too, can give a useful first pointer
for understanding the issue.  It is especially useful in still getting
something from bug reports from users when it’s not possible to reproduce a
bug locally as well as when pbrt is used for assignments in classes.  In
the latter case, the conversation often goes something like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“pbrt is buggy! It crashes when I call the function to normalize a vector.”&lt;/li&gt;
  &lt;li&gt;“That’s interesting–what does it print when it crashes?”&lt;/li&gt;
  &lt;li&gt;(pbrt’s output)&lt;/li&gt;
  &lt;li&gt;“That’s not a crash; it’s a failing assertion. The problem is that the
&lt;code class=&quot;highlighter-rouge&quot;&gt;foo()&lt;/code&gt; function that you added there is passing a degenerate vector to
the vector normalization routine.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given that students often don’t seem to read that output in the first
place, I’m not sure if any lessons are being learned about the value of
assertions through that exercise, but you can at least work through that
cycle much more quickly if it doesn’t require the student to fire up the
debugger to provide more information.&lt;/p&gt;

&lt;h2 id=&quot;resilience-versus-rigidity&quot;&gt;Resilience Versus Rigidity&lt;/h2&gt;

&lt;p&gt;When an assertion fails, a program generally terminates.  That’s a harsh
punishment, especially if the program is well into a lengthy computation.
One can treat failed assertions as exceptions and terminate just part of
the computation (and maybe just a small part, like a single ray path), or
one can also try to recover from the failing case and go on.  How to
approach all this is something of a philosophical question.&lt;/p&gt;

&lt;p&gt;A widely-accepted principle about assertions is that they should not be
used for error handling: invalid input from the user should never lead to
an assertion failure but rather should be caught sooner (and a helpful
error message printed, even if the program then terminates).  An assertion
failure should only represent an actual bug in the system: a mistake on the
programmer’s side, not on the user’s, even if something goofy provided by
the user is what tripped up the program.  That to me seems like an
unquestionably good principle.&lt;/p&gt;

&lt;p&gt;But even with assertions limited to errors in the implementation, what else
might one do when one fails?  One might try to recover, patching over the
underlying issue (for example, forcing the third probability to zero in the
majorant case), but that approach isn’t fully satisfying.  One issue is that the
code paths for the error cases will only run rarely, so they won’t be well
tested—it’s then hard to have confidence in their correctness.&lt;/p&gt;

&lt;p&gt;For a commercial product (or one that is not open source), not annoying
your users with an unexpected program termination is probably a good idea,
though I have to say that in my experience the error handling you get is
often not much better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/matt/blog/images/illustrator.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More optimistically, assertion failures represent useful data points.
Papering over them is ignoring evidence of a deeper issue.  Perhaps your
code for recovering from the failed assertion is running all the time and
there’s a massive bug lurking but you have no idea it exists in the first
place.&lt;/p&gt;

&lt;p&gt;So I have come to believe that the best approach is to be strict, at least
for a system like pbrt.  Include error handling code to deal with invalid
user input, add cases as necessary to make your algorithms general-purpose
and robust, but when things go wrong in a way that you hadn’t thought was
possible, don’t try to muddle through it—fail if a null vector is to be
normalized and abort if the majorants are seriously off.  Those sorts of
unexpected cases merit investigation and resolution.  By making them
impossible to ignore you reduce the chance of letting something serious
fester for a long time.  It’s an annoyance in the moment, but it makes the
system much more robust in the end.&lt;/p&gt;

&lt;h2 id=&quot;track-down-rare-failures&quot;&gt;Track Down Rare Failures(!)&lt;/h2&gt;

&lt;p&gt;About not letting things fester…  One of the reasons I’ve come to the
rigidity view is an experience I had with the &lt;a href=&quot;https://github.com/mmp/pbrt-v1&quot;&gt;first version of
pbrt&lt;/a&gt;.  That version was more on the
resilience side of things, or perhaps it was just negligence.  Over the
course of rendering the image below it would always print a handful of
warnings about rays having &lt;a href=&quot;https://en.wikipedia.org/wiki/NaN&quot;&gt;not-a-number
(NaN)&lt;/a&gt; values in their direction
vectors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.pbrt.org/gallery/a22.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I expected that something obscure was occasionally going wrong in the
middle of BSDF sampling but I didn’t dig in for years after first seeing
those warnings.  Part of my laziness came from the (correct) assumption
that it would be painful debugging since the warnings didn’t appear until
rendering had gone on for some time.  The underlying bug didn’t seem
important to fix since it happened so rarely.&lt;/p&gt;

&lt;p&gt;Eventually I chased it down. As with many difficult bugs, &lt;a href=&quot;https://github.com/mmp/pbrt-v1/commit/024ef868cedb4c6adf9bc5bdbca1e4c759b950c3&quot;&gt;the
fix&lt;/a&gt;
was a single-character change: a greater or equals that should have been a
greater than—“equals” being a case that otherwise led to a division by
zero.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        // Handle total internal reflection for transmission
-       if (sint2 &amp;gt; 1.) return 0.;
+       if (sint2 &amp;gt;= 1.) return 0.;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When I rendered that scene afterward, not only were the warnings gone, but
the entire rendering computation was \(1.25\times\) faster than it was
before.  I couldn’t understand why that would be so and spent hours trying
to figure out what was going on.  At first I assumed the speedup must be
due to something else, like a different setting for compiler optimizations,
but I found that it truly was entirely due to that one-character fix.&lt;/p&gt;

&lt;p&gt;Eventually I got to the bottom of it.  Here is where thing were going
catastrophically wrong—with a few lines of code elided, this is the heart
of the &lt;a href=&quot;https://github.com/mmp/pbrt-v1/blob/9d361637cafcc9e6d82c2f3440e5f7e7279254df/accelerators/kdtree.cpp#L337&quot;&gt;kd-tree traversal code in
pbrt-v1&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int axis = node-&amp;gt;SplitAxis();
float tplane = (node-&amp;gt;SplitPos() - ray.o[axis]) * invDir[axis];
// ...
if (tplane &amp;gt; tmax || tplane &amp;lt;= 0) {
    // visit first child node next
} else if (tplane &amp;lt; tmin) {
    // visit second child node next
else {
    // enqueue second child to visit later and visit first child next
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Consider that code with the lens of not-a-number. There are two rules
to keep in mind: a calculation that includes a NaN will yield a NaN, and
any comparison that includes a NaN evaluates to false.  (Thus, the fun
idiom of testing &lt;code class=&quot;highlighter-rouge&quot;&gt;x == x&lt;/code&gt; as a way to check for a NaN.)  Above, &lt;code class=&quot;highlighter-rouge&quot;&gt;tplane&lt;/code&gt; will
be NaN since the inverse ray direction is NaN.  The condition in the first
“if” test will be false, since both comparisons include a NaN.  The
condition in the second “if” test will also be false.  In turn, the third
case is always taken and &lt;em&gt;every node of the kd-tree will be visited&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Thus, a NaN-direction ray is intersected with each and every primitive in
the scene.  For a complex scene, that’s a lot of intersection tests and
thus, the performance impact of just a handful of those rays was
substantial.  Good times.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Here we are with two posts in a row that are comprised of me arguing for a
particular way of doing things and then ending with a story about me not
practicing what I’m preaching.  One could take this to mean that I don’t
know what I’m talking about, or one could take it to mean that my pain has
the potential to be your gain.  Either way works for me.&lt;/p&gt;

&lt;p&gt;More generally, I’ve come to learn that if something seems a little stinky
or uncertain in code, it really is worth stopping to take the time to chase
down whether there is in fact something wrong.  You have in hand evidence
of a problem in a particular place in a system—that’s valuable.  If you
ignore it and there is a bug there, often that bug will later manifest
itself in a way that’s much more obscure, maybe not evidently connected to
that part of the system at all.  You end up spending hours chasing it down
just to discover that if you had investigated the questionable behavior
when you first encountered it, you’d have fixed the underlying issue much
earlier and much more easily.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;notes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:firstassert&quot;&gt;
      &lt;p&gt;Goldstine and von Neumann. 1948. &lt;a href=&quot;https://www.ias.edu/sites/default/files/library/pdfs/ecp/planningcodingof0103inst.pdf&quot;&gt;Planning and Coding of problems
for an Electronic Computing
Instrument&lt;/a&gt;. Technical
Report, Institute of Advanced Study. &lt;a href=&quot;#fnref:firstassert&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:digits&quot;&gt;
      &lt;p&gt;To my previous frequent frustration, the &lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK&lt;/code&gt; macros in
       Google’s glog package do not print floating-point values with
       their full precision, which leads to error messages like &lt;code class=&quot;highlighter-rouge&quot;&gt;Check
       failed: x != 0 with x = 0&lt;/code&gt; bring printed when &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is very small
       but not actually zero.  This is another reason pbrt provides its
       own &lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK&lt;/code&gt; macros. &lt;a href=&quot;#fnref:digits&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Some notes on productively detecting bugs when they occur during the course of rendering and a cautionary tale about what can happen when you ignore runtime errors.</summary></entry><entry><title type="html">Debugging Your Renderer (2/n): Unit Tests</title><link href="https://pharr.org/matt/blog/2021/11/26/debugging-renderers-unit-tests.html" rel="alternate" type="text/html" title="Debugging Your Renderer (2/n): Unit Tests" /><published>2021-11-26T00:00:00-08:00</published><updated>2021-11-26T00:00:00-08:00</updated><id>https://pharr.org/matt/blog/2021/11/26/debugging-renderers-unit-tests</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/11/26/debugging-renderers-unit-tests.html">&lt;p&gt;Here we are, a year and a half after I posted an &lt;a href=&quot;/matt/blog/2020/04/26/debugging-intro.html&quot;&gt;introduction that was
full of talk&lt;/a&gt; about a
forthcoming series of blog posts about debugging renderers.  When I posted
that I already had a text file full of notes and had the idea that I’d get
through a series of 8 or so posts over the following few weeks.&lt;/p&gt;

&lt;p&gt;…and it’s been nothing but crickets after that setup.&lt;/p&gt;

&lt;p&gt;There’s no good reason for my poor follow-through, though this series did
turn into one of those things that got more daunting to return to the
longer time went by; I felt like the bar kept getting higher and that my
eventual postings would have to make up for the bait and switch.&lt;/p&gt;

&lt;p&gt;Now that I’m at it again, I can’t promise that these posts will make up for
the wait; in general, you get what you pay for around here.  But let’s
reset and try getting back into it.&lt;/p&gt;

&lt;p&gt;To get back in the right mood, here are a pair of images back from the
first time I tried to implement Greg Ward’s irradiance caching algorithm
back when I was in grad school:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/irradCacheInfinity.png&quot; width=&quot;288&quot; height=&quot;192&quot; /&gt;
&lt;img src=&quot;/matt/blog/images/irradCache.png&quot; width=&quot;288&quot; height=&quot;192&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In the left image (which was rendered from right to left for some reason),
there was a bug that caused energy to grow without bound as the cache was
populated (no doubt a missing factor of \(1/\pi\) that led to a feedback
loop).  I always liked how that image went from ok to a little too bright
to thermonuclear by the time it was halfway through.  The image on the
right is my eventual success, with a slightly different scene layout.&lt;/p&gt;

&lt;h2 id=&quot;avoiding-the-bad-place&quot;&gt;Avoiding The Bad Place&lt;/h2&gt;

&lt;p&gt;There’s nothing fun about an image that starts out ok and then goes bad or
your renderer crashing after its been running for an hour with a stack
trace 20 levels deep.  There’s lots to be unhappy about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Things are broken, but they’re not utterly broken, which suggests that
the underlying bug will be subtle and thus difficult to track down.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There’s an enormous amount of state to reason about—the scene in all
its complexity, all of the derived data structures, and everything that
happened since the start of rendering until things evidently went wrong.
Any bit of it may hold the problem that led to disaster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More specifically, the actual bug may be in code that ran long before the
bug became evident; some incorrect value computed earlier that messed
things up later, possibly in an indirect way.  This is a particular
challenge with algorithms that reuse earlier results, be it spatially,
temporally or otherwise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It may be minutes or even hours into rendering before the bug manifests
itself; each time you think you’ve fixed it, you’ve got to again wait
that much longer to confirm that you’re right.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anything you can do to avoid that sad situation reduces the amount of time
you spend on gnarly debugging problems, and in turn, the more productive
you’ll be (and the more fun you’ll have, actually implementing fun new
things rather than trying to make the old things work correctly.)  That
goal leads to the first principle of renderer debugging:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Try to make it a conventional debugging problem (“given these inputs,
this function produces this incorrect output”) and not an unbounded “this
image is wrong and I don’t know why” problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the best ways to have more bugs be in the first category is to
have a good suite of unit tests. There’s nothing glamorous about writing
unit tests, at least in the moment, but they can give you a lot in return
for not too much work.  Not only does failing unit test immediately narrow
down the source of a bug to the few things that the test exercises, but it
generally gives you an easier debugging problem than a failure in the
context of the full renderer.&lt;/p&gt;

&lt;h2 id=&quot;starting-simple&quot;&gt;Starting Simple&lt;/h2&gt;

&lt;p&gt;A good unit test is crisp—easy to understand and just testing one thing.
Writing tests becomes more fun if you embrace that way of going about
it—it’s easy coding since the whole goal is to not be tricky, with the
idea that you want to minimize the chance that your test itself has bugs.
A good testing framework helps by making it easy to add tests; I’ve been
using &lt;a href=&quot;https://github.com/google/googletest&quot;&gt;googletest&lt;/a&gt; for years, but
there are plenty of others.&lt;/p&gt;

&lt;p&gt;It’s good to start out by testing the most obvious things you can think of.
That may be counter-intuitive—it’s tempting to start with devious tests
that poke all the edge cases.  However, if you think about it from the
perspective of encountering a failing test, then the simpler the test is,
the easier it is to reason about the correct behavior, and the easier
debugging will be.  (There is an analogy here to the old joke about the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Streetlight_effect&quot;&gt;drunk searching for his car keys under the street
light&lt;/a&gt;.)  Only once the
basics are covered in your tests is it worth getting more clever.  If your
simpler tests pass and only the more complex ones fail, then at least you
can assume that simple stuff is functioning correctly; that may help you
reason about why the harder cases have gone wrong.&lt;/p&gt;

&lt;p&gt;Here is an example of a simple one from pbrt-v4. pbrt provides an
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/util/parallel.h#L126&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AtomicFloat&lt;/code&gt;&lt;/a&gt;
class that can atomically add values to a floating-point
variable.&lt;sup id=&quot;fnref:atomicfloat&quot;&gt;&lt;a href=&quot;#fn:atomicfloat&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; This test ensures that &lt;code class=&quot;highlighter-rouge&quot;&gt;AtomicFloat&lt;/code&gt; isn’t utterly
broken.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TEST(FloatingPoint, AtomicFloat) {
    AtomicFloat af(0);
    Float f = 0.;
    EXPECT_EQ(f, af);

    af.Add(1.0251);
    f += 1.0251;
    EXPECT_EQ(f, af);

    af.Add(2.);
    f += 2.;
    EXPECT_EQ(f, af);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The test is as simple as it could be: it performs a few additions and makes
sure that the result is the same as if a regular &lt;code class=&quot;highlighter-rouge&quot;&gt;float&lt;/code&gt; had been used.
It’s hard to imagine that this test would ever fail, but if it
did, jackpot! We have an easy case to reason about and trace through.&lt;/p&gt;

&lt;p&gt;Here’s another example of a not-very-clever test from pbrt-v4. Most
of the sampling functions there now provide an inversion function that goes
from sampled values back to the original \([0,1]^n\) sample space.  Thus,
it’s worth checking that a round-trip brings you back to (more or less)
where you started.  The following test takes a bunch of random samples &lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt;,
warps them to directions &lt;code class=&quot;highlighter-rouge&quot;&gt;dir&lt;/code&gt; on the hemisphere, then warps the directions
back to points &lt;code class=&quot;highlighter-rouge&quot;&gt;up&lt;/code&gt; in the canonical \([0,1]^2\) square, before checking
the result is pretty much back where it started.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TEST(Sampling, InvertUniformHemisphere) {
    for (Point2f u : Uniform2D(1000)) {
        Vector3f dir = SampleUniformHemisphere(u);
        Point2f up = InvertUniformHemisphereSample(dir);

        EXPECT_LT(std::abs(u.x - up.x), 1e-3);
        EXPECT_LT(std::abs(u.y - up.y), 1e-3);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There’s not much to that test, but it’s a nice one to have in the bag.
Once it passes, you can feel pretty good about your
&lt;code class=&quot;highlighter-rouge&quot;&gt;InvertUniformHemisphereSample&lt;/code&gt; function, at least if you have independent
confidence that &lt;code class=&quot;highlighter-rouge&quot;&gt;SampleUniformHemisphere&lt;/code&gt; works.  And how long does it take
to write?  No more than a minute or two.  Once it is passing, you can 
more confidently make improvements to the implementations of either of
those functions knowing that this test has a good chance of failing if you
mess something up.&lt;/p&gt;

&lt;p&gt;About succinctness in tests: that &lt;code class=&quot;highlighter-rouge&quot;&gt;Uniform2D&lt;/code&gt; in that test is a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/util/sampling.h#L1075&quot;&gt;little
thing&lt;/a&gt;
I wrote purely to make unit tests more concise.  It’s crafted to be used
with C++ range-based &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; loops and here generates 1000 uniformly
distributed 2D sample values to be looped over.  It and a handful of other
sample point generators save a few lines of code in each test that
otherwise needs a number of random values of some dimensionality and
pattern.  I’ve found that just about anything that reduces friction when
writing tests ends up being worthwhile in that each of those things
generally leads to more tests being written in the end.&lt;/p&gt;

&lt;h2 id=&quot;the-challenge-of-sampling&quot;&gt;The Challenge of Sampling&lt;/h2&gt;

&lt;p&gt;One of the challenges in implementing a Monte Carlo renderer is that the
computation is statistical in nature; sometimes it’s hard to tell if a
given sample value is incorrect or if it’s a valid outlier.  Bugs often
only become evident in the aggregate with many samples.  That challenge
extends to writing unit tests—for example, given a routine to draw
samples from some distribution, how can we be sure the samples are in fact
from the expected distribution?&lt;/p&gt;

&lt;p&gt;The Right Thing to do is to apply proper statistical tests.  For example,
&lt;a href=&quot;http://rgl.epfl.ch/people/wjakob/&quot;&gt;Wenzel&lt;/a&gt; has written code that applies a
\(\chi^2\)-test to pbrt’s &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/bsdfs_test.cpp#L280&quot;&gt;BSDF sampling
routines&lt;/a&gt;.
Those tests recently helped him chase down and fix &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/dfa1107459745b4d276c9bbdae73941cb269e077&quot;&gt;a tricky bug in pbrt’s
rough dielectric sampling
code&lt;/a&gt;. Much
respect for doing it the right way.&lt;/p&gt;

&lt;p&gt;My discipline is not always as strong as Wenzel’s, though there are some
more straightforward alternatives that are also effective.
For example, pbrt has many little sampling functions that
draw samples from some distribution.  An easy way to test them is to
evaluate the underlying function to create a tabularized distribution and
to confirm that both it and the sampling method to be tested more or less
generate the same samples with same probabilities.  As an example, here is
an excerpt from the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/util/sampling_test.cpp#L815&quot;&gt;test for sampling a trimmed
Gaussian&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    auto exp = [&amp;amp;](Float x) { return std::exp(-c * x); };
    auto values = Sample1DFunction(exp, 32768, 16, 0, xMax);
    PiecewiseConstant1D distrib(values, 0, xMax);

    for (Float u : Uniform1D(100)) {
        Float sampledX = SampleTrimmedExponential(u, c, xMax);
        Float sampledProb = TrimmedExponentialPDF(sampledX, c, xMax);

        Float discreteProb;
        Float discreteX = distrib.Sample(u, &amp;amp;discreteProb);
        EXPECT_LT(std::abs(sampledX - discreteX), 1e-2);
        EXPECT_LT(std::abs(sampledProb - discreteProb), 1e-2);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;Sample1DFunction&lt;/code&gt; utility routine takes a function and evaluates it in
a specified number of buckets covering a specified range, returning a
vector of values. &lt;code class=&quot;highlighter-rouge&quot;&gt;PiecewiseConstant1D&lt;/code&gt; then computes the corresponding
piecewise-constant 1D distribution.  We then take samples using the exact
sampling routine and the piecewise-constant routine and ensure that each
sample value is approximately the same and each returned sample probability
is close as well.  (This test implicitly depends on both sampling
approaches warping uniform samples to samples from the function with values
of &lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt; close to zero at the lower end of the exponential and &lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt; close to
one at the upper end, which is the case here.)&lt;/p&gt;

&lt;p&gt;To be clear: &lt;code class=&quot;highlighter-rouge&quot;&gt;SampleTrimmedExponential&lt;/code&gt; could still be buggy even when that
test passes.  One might fret about those fairly large &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-2&lt;/code&gt; epsilons used
for the quality test, for example.  It is possible that the looseness of
those epsilons might mask something subtly wrong, but we can at least trust
that the function isn’t completely broken, off by a significant constant
factor or the like.&lt;/p&gt;

&lt;p&gt;Writing this sort of test requires trusting your functions for sampling
tabularized distributions, but those too have their own tests;
eventually one can be confident in all of the foundations.  For example,
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/util/sampling_test.cpp#L216&quot;&gt;this
one&lt;/a&gt;
compares those results to a case where the expected result can be worked
out by hand and ensures that they match.&lt;/p&gt;

&lt;h2 id=&quot;preserving-the-evidence&quot;&gt;Preserving the Evidence&lt;/h2&gt;

&lt;p&gt;Another good use for unit tests is for isolating bugs, both for debugging
them when they first occur and for ensuring that a subsequent change to the
system doesn’t inadvertently reintroduce them.&lt;/p&gt;

&lt;p&gt;Disney’s &lt;em&gt;Moana Island&lt;/em&gt; scene helped surface all sorts of bugs in pbrt;
many were fairly painful to debug since many were of the form of “render
for a few hours before the crash happens.” For those, I found it useful to
turn them into small unit tests as soon as I could narrow down what was
going wrong.&lt;/p&gt;

&lt;p&gt;Here’s one for a ray-triangle intersection that went bad.  We have a
degenerate triangle (note that the x and z coordinates are all equal), and
so the intersection test should never return true. But for the specific ray
here, it once did, and then things went south from there.  Trying potential
fixes with a small test like this was a nice way to work through the issue
in the first place—it was easy to try a fix, recompile, and quickly see
if it worked.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TEST(Triangle, BadCases) {
    Transform identity;
    std::vector&amp;lt;int&amp;gt; indices{ 0, 1, 2 };
    std::vector&amp;lt;Point3f&amp;gt; p { Point3f(-1113.45459, -79.0496140, -56.2431908),
                             Point3f(-1113.45459, -87.0922699, -56.2431908),
                             Point3f(-1113.45459, -79.2090149, -56.2431908) };
    TriangleMesh mesh(identity, false, indices, p, {}, {}, {}, {});
    auto tris = Triangle::CreateTriangles(&amp;amp;mesh, Allocator());

    Ray ray(Point3f(-1081.47925, 99.9999542, 87.7701111),
            Vector3f(-32.1072998, -183.355865, -144.607635), 0.9999);

    EXPECT_FALSE(tris[0].Intersect(ray).has_value());
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;One thing to note when extracting failure cases like this is that it’s
critical to get &lt;a href=&quot;https://randomascii.wordpress.com/2013/02/07/float-precision-revisited-nine-digit-float-portability/&quot;&gt;every last
digit&lt;/a&gt;
of floating-point values: if the floats you test with aren’t precisely the
same as the ones that led to the bug, you may not hit the bug at all in a
test run.&lt;/p&gt;

&lt;h2 id=&quot;never-defer-looking-into-a-failing-test&quot;&gt;Never Defer Looking into a Failing Test&lt;/h2&gt;

&lt;p&gt;A cautionary tale to wrap up: a few months ago a &lt;a href=&quot;Https://github.com/mmp/pbrt-v4/issues/177&quot;&gt;bug
report&lt;/a&gt; about a failing unit
test in pbrt-v4 came in.  It had the following summary:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;gcc-8.4 has stuck forever on ZSobolSampler.ValidIndices test&lt;/li&gt;
    &lt;li&gt;gcc-9.3 passed all tests&lt;/li&gt;
    &lt;li&gt;gcc-10.3 gives me the following message (in an eternal cycle) during tests&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;tt&gt;/src/pbrt/samplers_test.cpp:182: Failure&lt;/tt&gt;&lt;br /&gt;
&lt;tt&gt;Value of: returnedIndices.find(index) == returnedIndices.end()&lt;/tt&gt;&lt;br /&gt;
&lt;tt&gt;  Actual: false&lt;/tt&gt;&lt;br /&gt;
&lt;tt&gt;  Expected: true&lt;/tt&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;ZSobolSampler&lt;/code&gt; implements &lt;a href=&quot;http://abdallagafar.com/publications/zsampler/&quot;&gt;Ahmed and Wonka’s blue noise
sampler&lt;/a&gt;, which is based on
permuting a set of low-discrepancy samples in a way that improves their
blue noise characteristics.  pbrt’s &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/792aaaa08d97dbedf11a3bb23e246b6443d847b4/src/pbrt/samplers_test.cpp#L167&quot;&gt;ZSobolSampler.ValidIndices
test&lt;/a&gt;
essentially just checks that the permutation is correct by verifying that
the same sample isn’t returned for two different pixels.  That test had been
helpful when I first implemented that sampler, but it had been no trouble
for months when that bug report arrived.&lt;/p&gt;

&lt;p&gt;When the bug report came in, I took a quick look at that test and couldn’t
imagine how it would ever run forever.  No one else had reported anything
similar and so, to my shame, I assumed it must be a problem with the
compiler installation on the user’s system or some other one-off error.  I
didn’t look at it again for almost two months.&lt;/p&gt;

&lt;p&gt;When I gave it more attention, I immediately found that I could reproduce
the bug using those compilers, just as reported.  It was a gnarly bug—one
that disappeared when I recompiled with debugging symbols and even
disappeared with an optimized build with debugging symbols.  The bug would
randomly disappear if I added print statements to log the program’s
execution.  Eventually I thought to try
&lt;a href=&quot;https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html&quot;&gt;UBSan&lt;/a&gt;, and
it saved the day, identifying this line of code as the problem:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int p = (MixBits(higherDigits ^ (0x55555555 * dimension)) &amp;gt;&amp;gt; 24) % 24;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;0x55555555&lt;/code&gt; is a signed integer and multiplying by &lt;code class=&quot;highlighter-rouge&quot;&gt;dimension&lt;/code&gt;, which was
an integer that starts at 0 and goes up from there, quickly led to
overflow, which is undefined behavior (UB) in C++.  In turn, &lt;em&gt;gcc&lt;/em&gt; was
presumably assuming that there was no UB in the program and optimizing
accordingly, leading in one case to an infinite loop and in another to a
bogus sample permutation.&lt;/p&gt;

&lt;p&gt;At least the fix was easy—all is fine with an unsigned integer, where
overflow is allowed and well-defined:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int p = (MixBits(higherDigits ^ (0x55555555u * dimension)) &amp;gt;&amp;gt; 24) % 24;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Leaving aside the joys of undefined behavior in C++, it was hard enough to
chase that bug down with it already narrowed down to a failing test.  If
the bug had been something like “images are slightly too dark with
gcc-10.3” (as could conceivably happen with repeated sample values,
depending on how they were being repeated), it surely would have been an
even longer and more painful journey. Score +1 for unit tests and -1 for
me.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re not done with testing! With the unit testing lecture over, next time
it will be on to some thoughts about writing effective assertions and how
end-to-end tests fit in for testing renderers.&lt;/p&gt;

&lt;h2 id=&quot;note&quot;&gt;note&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:atomicfloat&quot;&gt;
      &lt;p&gt;That capability isn’t provided by the C++ standard library
            since floating-point addition is not associative, so
            different execution orders may give different results.
            For pbrt’s purposes, that’s not a concern, so &lt;code class=&quot;highlighter-rouge&quot;&gt;AtomicFloat&lt;/code&gt;
            provides that functionality through atomic compare/exchange
            operations. &lt;a href=&quot;#fnref:atomicfloat&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Returning, now with intention, to write up some thoughts about how to effectively debug a renderer.</summary></entry><entry><title type="html">Swallowing the Elephant (Part 12): A Postscript On Disk Bandwidth</title><link href="https://pharr.org/matt/blog/2021/08/07/moana-bandwidth-note.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 12): A Postscript On Disk Bandwidth" /><published>2021-08-07T00:00:00-07:00</published><updated>2021-08-07T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/08/07/moana-bandwidth-note</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/08/07/moana-bandwidth-note.html">&lt;p&gt;At the ostensible &lt;a href=&quot;/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html&quot;&gt;end of these
updates&lt;/a&gt; about
pbrt-v4’s performance when rendering &lt;a href=&quot;https://www.disneyanimation.com/resources/moana-island-scene/&quot;&gt;Disney’s Moana Island
scene&lt;/a&gt;,
there was an unresolved question about why CPU utilization wasn’t better at
the very start when pbrt was parsing the scene description.  As a
refresher, with 64 threads on a 32-core AMD 3970X CPU and pbrt’s GPU-based
rendering path, it looked like this. (As before, the vertical dashed line
indicates when rendering begins.)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-64-truefin-rescaled.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Starting at the 17 second mark, low CPU utilization isn’t necessarily bad
since that’s when the GPU starts getting involved building acceleration
structures, but before that it’s all on the CPU.  For the first twelve or so
seconds, total CPU utilization is between 0.2 and 0.4, which corresponds to
roughly 13–26 of those 64 threads actually making something of themselves;
that’s not enough of them to be satisfying.&lt;/p&gt;

&lt;p&gt;It started to nag at me whether limited disk bandwidth might have something
to do with that—i.e., is the issue that threads are stalled waiting for
I/O?  I made a few measurements to try to answer that question and learned
enough along the way that here we go again.&lt;/p&gt;

&lt;h2 id=&quot;how-far-we-have-come&quot;&gt;How Far We Have Come&lt;/h2&gt;

&lt;p&gt;Three years ago when I &lt;a href=&quot;/matt/blog/2018/07/08/moana-island-pbrt-1.html&quot;&gt;first looked at pbrt’s performance with Moana
Island&lt;/a&gt; I was using a
Google Compute Engine instance with a spinny disk for benchmarks.  Nowadays
you might hope for around 100 MB/s of read bandwidth from such a disk.
pbrt-v4 reads a total of 27,766 MB from disk when loading this
scene and it takes a lot of 100 MBs to get through all of that.  Therefore,
when I was doing benchmarks then I was careful to flush the OS’s buffer
cache between runs so that the true cost of I/O was measured
and everything didn’t come out of RAM after the first time at rates much
better than 100 MB/s.&lt;/p&gt;

&lt;p&gt;This time around, I didn’t mention the disk on the system I used for
benchmarking and I didn’t worry about the buffer cache.  That wasn’t an
oversight, but was more of a “I’m pretty sure this doesn’t matter”&lt;sup id=&quot;fnref:ps&quot;&gt;&lt;a href=&quot;#fn:ps&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; sort of
thing, like which version of the Linux kernel was running or whether it was
DDR4 3200 or DDR4 3600 RAM in the system.  (For the record, 5.8.0 and the
former.)&lt;/p&gt;

&lt;p&gt;The disk I’m using now is an NVMe disk; a quick benchmark showed that it
delivers a peak of 2,022 MB/s of read bandwidth.  I didn’t think that could
be a bottleneck, though if you distribute those 2,022 MB/s evenly to 64
threads, it’s just 32 MB/s per thread.  Thinking about it in those terms
made me worry that bandwidth might be tight, so I decided to make some
direct measurements and see what they had to show.&lt;/p&gt;

&lt;h2 id=&quot;starting-position&quot;&gt;Starting Position&lt;/h2&gt;

&lt;p&gt;First, I measured pbrt’s disk bandwidth use over time to get a sense of
whether it ever approached the peak and to see how disk reads were
distributed over the course of loading the scene.  (This and following
measurements were made with an empty buffer cache, just to be safe.)
&lt;code class=&quot;highlighter-rouge&quot;&gt;iostat&lt;/code&gt; made that easy to do, though sadly it doesn’t seem to be able to
report I/O with less than one second granularity, which is more coarse than
one would like given 30 seconds time to first pixel.  In any case, here is
a graph of what it had to say; the disk’s measured maximum I/O bandwidth is
marked with a dashed horizontal line.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/moana-io.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;For the first 20 or seconds, pbrt is mostly parsing text *.pbrt scene
description files; it starts out consuming plenty of bandwidth but then
slows as there are fewer files left to get through.  The second wave
of I/O starting at 20 seconds corresponds to reading all of the PLY files for the
object instances in the scene.  The news in this graph is mostly good: pbrt
doesn’t seem to ever top out at the maximum bandwidth, suggesting that
it’s not I/O bound, though it’s close enough at 9 seconds there that it’s
not possible to be sure from these measurements.&lt;/p&gt;

&lt;p&gt;This data also makes it possible to compute an alternative speed of light
measurement for time to first pixel.  If we divide the total size of data
read, 27,766 MB, by the peak read bandwidth of 2,022 MB/s, we can see that we
can’t hope to have a time to first pixel under 13.7 seconds.  That’s already an
interesting result, as it shows that the &lt;a href=&quot;/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html#speed-of-light&quot;&gt;earlier speed of light
calculation&lt;/a&gt;
that only considered the CPU didn’t tell the whole story: then, 
neglecting I/O limits, we estimated 7.2 seconds as the best possible time
to first pixel.&lt;/p&gt;

&lt;p&gt;Another thing this graph shows is that pbrt is close enough to being I/O
bound at the start that there isn’t a lot of reason to worry about the
relatively low CPU utilization then.  We might improve
it some by finding more things to start reading sooner, but the benefit
would be limited since we would soon hit peak disk bandwidth and be limited
by that.  Further performance improvements would then require a better
balance of I/O requests over time.&lt;/p&gt;

&lt;h2 id=&quot;turning-the-bandwidth-screw&quot;&gt;Turning The Bandwidth Screw&lt;/h2&gt;

&lt;p&gt;The data already seemed fairly conclusive about not being I/O bound, but I
was curious about how performance varied with disk read bandwidth—how
crucial is that lovely abundant NVMe bandwidth to pbrt’s performance with
this scene?  One way to find out is to start reducing the amount of disk
read bandwidth available to pbrt and to see how that affects performance.&lt;/p&gt;

&lt;p&gt;Once you find &lt;a href=&quot;https://unix.stackexchange.com/a/393798&quot;&gt;the right trick&lt;/a&gt;
it’s surprisingly easy, at least on Linux, to use &lt;code class=&quot;highlighter-rouge&quot;&gt;systemd-run&lt;/code&gt; to launch a
process that has a limited amount of disk read bandwidth available to it.
I did a quick study, dialing the bandwidth down from the 2,000 MB/s that my
NVMe drive offers to the sad 50 MB/s that a middling spinning disk today
might provide.&lt;/p&gt;

&lt;p&gt;Here is a graph of pbrt-v4’s time to first pixel with the Moana Island
scene as a function of available disk bandwidth, running with both 8
threads on 4 cores and 64 threads on 32 cores. Note that the y axis has a
logarithmic scale, the better to fit the sadness that is a nearly 600
second time to first pixel given 50 MB/s.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-vs-read-bandwidth.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;There are a number of things to see in this graph.  First, it offers
further confirmation that pbrt-v4 is not bandwidth limited for this scene:
the fact that performance doesn’t immediately decrease as bandwidth starts
to decrease from 2,000 MB/s indicates that more bandwidth isn’t going to make things
faster.  Both lines seem to have hit their asymptote, though the 64 thread
one just barely so.&lt;/p&gt;

&lt;p&gt;This graph also shows how much bandwidth can decrease before performance is
meaningfully affected.  With 64 threads, you only have to go to 1400 MB/s
to slow down time to first pixel by 10%, but with 8 threads you can go all
the way to 800 MB/s before there’s a 10% drop.  This isn’t surprising—the
more threads you’ve got, the more bandwidth you’re capable of
consuming—but it’s nevertheless interesting to see how much farther one
can go with fewer threads.&lt;/p&gt;

&lt;p&gt;Finally, note that below 500 MB/s, the two curves are
effectively the same.  Here, too, there’s no big surprise: if you’re trying
to drink through a narrow straw, having more thirsty people waiting in line
on the end of it isn’t going to get the water through more quickly, to grossly
overstretch a metaphor.&lt;/p&gt;

&lt;h2 id=&quot;deflate-deflate-deflate&quot;&gt;DEFLATE, DEFLATE, DEFLATE&lt;/h2&gt;

&lt;p&gt;Compression algorithms make it possible to trade off bandwidth for
computation, so my last experiment was to look at performance with the
scene description compressed using &lt;code class=&quot;highlighter-rouge&quot;&gt;gzip&lt;/code&gt;.  Thanks to a recent &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/6577a325a4cb934efac3b10f3b33847cf0d93ea4&quot;&gt;patch from
Jim
Price&lt;/a&gt;,
pbrt-v4 now supports reading &lt;code class=&quot;highlighter-rouge&quot;&gt;gzip&lt;/code&gt;-compressed scene description files, and
&lt;a href=&quot;https://w3.impa.br/~diego/software/rply/&quot;&gt;RPly&lt;/a&gt;, the PLY file reader by Diego Nehab that
pbrt uses, already supported &lt;code class=&quot;highlighter-rouge&quot;&gt;gzip&lt;/code&gt;-compressed PLY files.
All of that made it easy to run the same experiments with a compressed
scene description.&lt;/p&gt;

&lt;p&gt;With the *.pbrt and PLY files compressed using &lt;code class=&quot;highlighter-rouge&quot;&gt;gzip -5&lt;/code&gt;, pbrt-v4 reads a
total of just 5,570 MB from disk—nearly 5x less than with the
uncompressed scene description.  Using &lt;a href=&quot;https://zlib.net/&quot;&gt;zlib&lt;/a&gt; for
decompression with 64 threads and the full NVMe disk bandwidth, pbrt takes
40 seconds to first pixel with a compressed scene—12 seconds slower than
with everything uncompressed.  Given that it wasn’t bandwidth-limited
before, that isn’t surprising—we have just increased the amount of CPU
work that needs to be done to get the scene into memory.&lt;/p&gt;

&lt;p&gt;Here is the graph of disk I/O consumption over those 40 seconds; it shows
that now there is plenty of headroom with never more than 500 MB/s of
bandwidth used.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/moana-io-gz.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;As we were going to press, I saw that Aras Pranckevičius just put up a nice
series of &lt;a href=&quot;https://aras-p.info/blog/2021/08/04/EXR-Lossless-Compression/&quot;&gt;blog posts about compression in
OpenEXR&lt;/a&gt;.
Those led me down all sorts of ratholes, and one of them reminded me about
&lt;a href=&quot;https://github.com/ebiggers/libdeflate&quot;&gt;libdeflate&lt;/a&gt;, a highly optimized
library that can decompress gzip-encoded files (among others).  It wasn’t too
much code to swap that in for zlib in pbrt and &lt;strong&gt;bam&lt;/strong&gt;: down to 34 seconds
to first pixel with a compressed scene.  And that’s actually only using
libdeflate for the *.pbrt files but still using zlib for the 1,152 MB worth
of compressed PLY files, since using libdeflate with RPly would have
required more complicated plumbing.&lt;/p&gt;

&lt;p&gt;Anyway, here’s a graph that shows time to first pixel with all three
options, again with 64 threads. libdeflate gets an asterisk, since it
isn’t being used for PLY files (and there is thus presumably some
performance being left on the floor.)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-vs-read-gz.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;There’s lots of good stuff to see there.  As advertised, libdeflate is
certainly faster than zlib. It starts being the fastest option overall at
around 1,300 MB/s of bandwidth.  From there on down, the additional CPU
work to do decompression is worth the disk bandwidth savings in return. (In
contrast, zlib doesn’t make up for its computational overhead until
around 1,000 MB/s.)&lt;/p&gt;

&lt;p&gt;Both decompressors have more or less constant performance all the way down
to roughly 300 MB/s from the disk.  Past there, their performance
converges: at that point, data is coming in so slowly that how quickly it’s
decompressed doesn’t make much difference.  We can also see that
compression is especially helpful way down at 50 MB/s, where it’s leads to
a spritely 127 seconds to first pixel—4.6x faster than the uncompressed
scene is with that little bandwidth.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;For once we have gotten through these investigations without finding any
surprising bottlenecks and so today has not brought any changes to pbrt’s
implementation, though I suspect pbrt will switch from zlib to libdeflate
fairly soon.&lt;/p&gt;

&lt;p&gt;Perhaps the most useful result from today is a more accurate estimate of
pbrt’s best possible performance when preparing to render this scene:
13.7 seconds given the disk I/O limits of the system.  With that limit
known, it’s easier to accept the 28 seconds to first pixel that the system
delivers today—apparently only 2x off the maximum attainable
performance—and to stop fiddling with the details.&lt;/p&gt;

&lt;p&gt;And yet… I hope that the attentive reader might quibble with the logic
behind that conclusion: with the compressed scene, we found ourselves with
a mere 5,570 MB of disk I/O, and that’s something this computer can deliver
in 2.75 seconds, which puts us once again 10x off the mark.  It seems that
part of speed of light is in how you define it, but nevertheless I think
it’s time to leave things where they lie for now.&lt;/p&gt;

&lt;h2 id=&quot;note&quot;&gt;note&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ps&quot;&gt;
      &lt;p&gt;The road to disastrous performance is paved with “pretty sure” assumptions about a system’s behavior, so that assumption was admittedly not wise. &lt;a href=&quot;#fnref:ps&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">A few concluding notes (this time for real) about the effect of disk bandwidth when preparing to render the Moana Island scene with pbrt-v4.</summary></entry><entry><title type="html">Swallowing the Elephant (Part 11): Once More Unto The Beach</title><link href="https://pharr.org/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 11): Once More Unto The Beach" /><published>2021-08-01T00:00:00-07:00</published><updated>2021-08-01T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/08/01/moana-once-more-unto-the-beach</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/08/01/moana-once-more-unto-the-beach.html">&lt;p&gt;There are two perspectives that one might take in assessing how far things
have come with pbrt-v4’s performance with &lt;a href=&quot;https://www.disneyanimation.com/resources/moana-island-scene/&quot;&gt;Disney’s Moana Island
scene&lt;/a&gt;: the
optimist’s and the pessimist’s.  As far as rendering performance goes, I
can’t find much to be pessimistic about.  The GPU especially has done well
on that front, to the point that time spent parsing the scene description
and preparing data structures for rendering—time to first pixel—is a
significant contributor to overall performance when it is used.&lt;/p&gt;

&lt;p&gt;About time to first pixel: here is the optimist’s view, which looks at
progress measured with respect to where we started.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Time&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Speedup&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v3 (&lt;a href=&quot;/matt/blog/2018/07/08/moana-island-pbrt-1.html&quot;&gt;July 2018&lt;/a&gt;)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2098s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-next (&lt;a href=&quot;/matt/blog/2018/08/03/moana-reader-mail.html&quot;&gt;August 2018&lt;/a&gt;)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;558s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.76x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v4 (&lt;a href=&quot;/matt/blog/2021/04/11/moana-time-to-first-pixel.html&quot;&gt;April 2021 start&lt;/a&gt;)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;410s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.12x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v4 (&lt;a href=&quot;/matt/blog/2021/04/11/moana-time-to-first-pixel.html&quot;&gt;April 2021 end&lt;/a&gt;)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;97s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;21.6x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v4 GPU (&lt;a href=&quot;/matt/blog/2021/07/27/moana-gpu-instances.html&quot;&gt;July 2021&lt;/a&gt;)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;59.6s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;35.2x&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;From that perspective, it’s been a smashing success, bringing a heavy scene
from something that is nearly intolerable to render to something that has a
bit of a hitch getting started, no big deal for what you get in return.&lt;/p&gt;

&lt;p&gt;For an alternative viewpoint, here is the graph of pbrt’s CPU utilization
over those 59.6 seconds up until the start of rendering, with all of those
improvements in there and measured with 64 threads on a 32-core AMD
3970X CPU. As before, the x axis is measured in seconds, a value of 1 on
the y axis represents all 64 threads running, and the time at which
rendering starts is indicated with a vertical dashed line.  Unlike the last
two posts, this graph starts from the very beginning, so parsing and
related work is back in there again.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-fin-64.svg&quot; alt=&quot;CPU utilization (64 threads, starting point)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;After what seemed like good progress from addressing post-parsing
bottlenecks, it’s disheartening to see how bad the complete graph
is—there’s still an enormous amount of space above the line, all of it
wasted potential.  Therefore, today it’ll be one more go at improving
parallelism and reducing time to first pixel.&lt;/p&gt;

&lt;p&gt;Before getting to work, let’s distract ourselves with an image.  Here’s the
beach view, again at 256 samples per pixel, rendered on both the CPU and
the GPU. My hacky &lt;a href=&quot;/matt/blog/2021/07/29/moana-rendered-on-the-gpu.html#preliminaries-textures-and-curves&quot;&gt;GPU Ptex
implementation&lt;/a&gt;
fares better here than in the roots view, though there are still issues on
the sand dunes and the tree trunks.&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; width: 100%; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;beachview&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('beachview'), {
  title: 'beachview', children: [
 { title: 'CPU', image: '/matt/blog/images/moana-beach-cpu-256spp.exr' },
 { title: 'GPU', image: '/matt/blog/images/moana-beach-gpu-256spp.exr' }
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/b6a3f681108d5ebd414b0aa0a5d4ba4c99eca8e4&quot;&gt;the version of pbrt that is today’s starting
point&lt;/a&gt;,
the RTX A6000 GPU renders its image in 36.3 seconds, taking a total of 105
seconds of wall-clock time including parsing the scene and setting up the
data structures.  The 32 core AMD 3970X takes 928 seconds to render it, or
1033 seconds including everything.  Rendering is slightly over 25x faster
with the GPU.&lt;/p&gt;

&lt;h2 id=&quot;speed-of-light&quot;&gt;Speed of Light?&lt;/h2&gt;

&lt;p&gt;That graph made it clear that time to first pixel could be better. But how
much better?  The area under the graph represents the amount of CPU work
that is done over the course of getting started and summing it up gives us
the total amount of CPU work required.&lt;/p&gt;

&lt;p&gt;It’s easy enough to modify pbrt’s logging code to track the total CPU
utilization, from which we can learn that it would be 463 seconds of
single-threaded work to get everything up and running.  Thus, in fantasy
perfect parallel scaling world, we would start rendering after 7.2 seconds
on a 32-core/64-thread system as long as the GPU didn’t become the
bottleneck.&lt;/p&gt;

&lt;p&gt;Put another way, the 59 second time to first pixel with 64 threads is
slightly more than 8x worse than speed of light performance.  Knowing that
pbrt was still that far off was a painful realization, but it was enough to
motivate giving the code more attention.  So what’s holding it back?&lt;/p&gt;

&lt;h2 id=&quot;serialization-everywhere&quot;&gt;Serialization Everywhere&lt;/h2&gt;

&lt;p&gt;A hindrance that comes from pbrt’s design is that we have thus far been
trying to parallelize within distinct phases of computation but never
across them.  It starts reading textures only after parsing is finished; it
starts creating lights only after textures are done; the BVHs for
non-instanced geometry are created before the BVHs for the object
instances, and so forth.  This constraint means that available parallelism
is often limited, making it harder to effectively scale up to use many CPU
cores.&lt;/p&gt;

&lt;p&gt;Concretely, within the first milliseconds of the start of parsing the Moana
Island scene, we know that there’s an environment light with its emission
specified by a PNG image; however, the slow (single threaded) reading and
decompression of that PNG doesn’t get started until more than 30 seconds
later when parsing has finished. Then, it’s pretty much the only work
available and all the other threads are unable to do anything useful.  Why
not start sooner and keep an otherwise-idle thread busy while other threads
are processing those few last large scene files?&lt;/p&gt;

&lt;p&gt;Honestly, pbrt’s startup phase had mostly been designed without parallelism
in mind; the biggest goal was that it be easily understood by the reader
and that it be simple enough to not use too many pages in the printed book.
Thus, the idea of “first there is a parsing phase”, “now there is a light
creation phase”, and so forth.  However, given the state of CPU utilization
in that earlier graph, I spent a while mulling over whether that design
should be revisited.  I was sure that there was performance to be had from
doing so, but I worried about making the system harder to understand and
debug.  At tension with the goal for pbrt to be understandable is an
aspiration to show best practices and to illustrate more broadly-useful
programming techniques; if that part of the system has really bad parallel
scaling, then that isn’t really best practices, is it?&lt;/p&gt;

&lt;p&gt;Something that helped tip the balance was the fact that C++17 has built-in
support for
&lt;a href=&quot;https://en.wikipedia.org/wiki/Futures_and_promises&quot;&gt;futures&lt;/a&gt;&lt;sup id=&quot;fnref:goog&quot;&gt;&lt;a href=&quot;#fn:goog&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and
has reasonably clean support for asynchronous tasks.  With the building
blocks in the core language, showing what the concepts are good for seemed
like it might be worthwhile in exchange for any added complexity.  And the
beauty of futures is that your thread will just stall if it tries to access
something before it’s ready; there’s not a risk of tricky race conditions.&lt;/p&gt;

&lt;p&gt;As I started exploring a redesign of that part of pbrt, I found that C++’s
language built-ins weren’t quite right out of the box: I wanted to run
asynchronous tasks using pbrt’s already-existing thread pool, which isn’t
supported by &lt;code class=&quot;highlighter-rouge&quot;&gt;std::async&lt;/code&gt;.  Fortunately, it was under 30 lines of code to
wrap up an asynchronous job into &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/2e6aace69d877295d33292dffa3239e04546d44b/src/pbrt/util/parallel.h#L315&quot;&gt;something that could run in pbrt’s task
system&lt;/a&gt;.
The other thing necessary was a small
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/2e6aace69d877295d33292dffa3239e04546d44b/src/pbrt/util/parallel.h#L291&quot;&gt;wrapper&lt;/a&gt;
around &lt;code class=&quot;highlighter-rouge&quot;&gt;std::future&lt;/code&gt; that would call into the job system to do work in the
current thread if a future that was waited on wasn’t ready; that avoided
deadlock, as would have been a problem otherwise with a fixed number of
threads in a thread pool.&lt;sup id=&quot;fnref:al&quot;&gt;&lt;a href=&quot;#fn:al&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;embracing-the-asynchronous-life&quot;&gt;Embracing The Asynchronous Life&lt;/h2&gt;

&lt;p&gt;With infrastructure that made it easy to kick off asynchronous work, it was
time to start putting it to use.  An easy first step was to
asynchronously create the &lt;code class=&quot;highlighter-rouge&quot;&gt;Media&lt;/code&gt; objects that represent participating
media; that doesn’t matter for the Moana Island scene, but it was the
simplest first thing to port over.  &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/117b1c135136633c3701f8bdc5b0c00d02a25f8b&quot;&gt;It wasn’t much
code&lt;/a&gt;
and, auspiciously, it worked the first time.&lt;/p&gt;

&lt;p&gt;Next up: lights, the known troublemaker.  Again, &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/3cdc3a9ef7af0ed61981add9da50909ec2746994&quot;&gt;not too much
code&lt;/a&gt;,
and more importantly, having written it, I don’t think there’s anything too
complex going on there: as soon as the renderer hears about a light in the
scene, it can kick off a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;RunAsync()&lt;/code&gt; to create it.  It holds on to
a vector of &lt;code class=&quot;highlighter-rouge&quot;&gt;Future&amp;lt;Light&amp;gt;&lt;/code&gt;s and then consumes the values returned in the
futures much later—usually well after the time they were actually
created.  That change alone improved time to first pixel with the Moana
Island scene by 5 seconds, which was more than enough encouragement to keep
going.&lt;/p&gt;

&lt;p&gt;Textures were next and they weren’t too tricky either, mostly &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/8e1c779f2e1afebf6a630bbc31d038c6bcbe94e4&quot;&gt;rearranging
preexisting texture creation
code&lt;/a&gt;.
They were good for another few seconds improvement, which brought pbrt to
just over 50 seconds time to first pixel.  Building the top-level
acceleration structures for non-instanced geometry asynchronously while
BVHs for instances were being created was &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/71316edd4e2280c23fad24d71d96aac6846e1f68&quot;&gt;another easy
one&lt;/a&gt;
and gave another second or so’s improvement on top of that.&lt;/p&gt;

&lt;p&gt;Here’s how things look with those changes.  Note that the
“Lights/Textures/Materials” category has disappeared into nothing, with all
of that work already done during parsing and ready by the time it is needed
afterward.  (That work is charged to parsing in the CPU utilization
reported in this graph.  For simplicity, we’ll report CPU utilization by
phases even as we blur the lines between them.)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-async-multi.svg&quot; alt=&quot;CPU utilization (64 threads, asynchronous light, texture, and BVH creation)&quot; /&gt; &lt;/p&gt;

&lt;h2 id=&quot;mind-the-parser&quot;&gt;Mind The Parser&lt;/h2&gt;

&lt;p&gt;Eyeballing that graph, we can see that we’re down to just over 10 seconds
of post-parsing work; nearly 40 seconds of parsing time remain, and that’s
where we ought to to look for further improvements.  I realized that I
hadn’t ever profiled pbrt-v4’s parsing and initial scene processing code
with the Moana Island scene; a quick run of
&lt;a href=&quot;https://perf.wiki.kernel.org/index.php/Main_Page&quot;&gt;perf&lt;/a&gt; delivered: nearly
20% of parsing time was spent in the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/2e6aace69d877295d33292dffa3239e04546d44b/src/pbrt/scene.cpp#L405&quot;&gt;method that is called to record each
use of an object instance in the
scene&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are 39 million instances in the scene so that method gets a workout,
but near 20% of total parsing time there seemed high.  Half of that was in
a single &lt;code class=&quot;highlighter-rouge&quot;&gt;std::vector::push_back()&lt;/code&gt; &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/2e6aace69d877295d33292dffa3239e04546d44b/src/pbrt/scene.cpp#L427&quot;&gt;method
call&lt;/a&gt;,
so I looked more closely at what was going on with it.&lt;/p&gt;

&lt;p&gt;That &lt;code class=&quot;highlighter-rouge&quot;&gt;vector&lt;/code&gt; is used to record the uses of object instances—for each
one, it needs to store both the name of the object instance being used as
well as a transformation matrix.  Even with 39 million instances, that
still seemed excessive.  I looked at the definition of the object that it
stores, &lt;code class=&quot;highlighter-rouge&quot;&gt;InstanceSceneEntity&lt;/code&gt;.  It was essentially:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;struct InstanceSceneEntity : public SceneEntity {
    AnimatedTransform *renderFromInstanceAnim = nullptr;
    Transform *renderFromInstance = nullptr;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So you’ve got your instance transformation in one of two flavors and then
whatever we get from &lt;code class=&quot;highlighter-rouge&quot;&gt;SceneEntity&lt;/code&gt;.  What does it have?&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;struct SceneEntity {
    std::string name;
    FileLoc loc;
    ParameterDictionary parameters;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There’s &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;, which here stores the name of the object instance, there’s
&lt;code class=&quot;highlighter-rouge&quot;&gt;loc&lt;/code&gt;, which stores its location in a scene description file, handy if we
need it for an error message, and then we’ve got ourselves a
&lt;code class=&quot;highlighter-rouge&quot;&gt;ParameterDictionary&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What’s a &lt;code class=&quot;highlighter-rouge&quot;&gt;ParameterDictionary&lt;/code&gt;? Unnecessary is what it is.  It’s a fairly
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/2e6aace69d877295d33292dffa3239e04546d44b/src/pbrt/paramdict.h#L97&quot;&gt;heavy
structure&lt;/a&gt;
that stores user-specified parameters for entities in the scene description
file—e.g., “this sphere has a float-valued parameter, radius, that has a
value of 2.5.”  It isn’t needed at all for object instances—it’s just
weight with lots of extra unused data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/98cbfcd7993555993fa9d5ba12e8cfeddc792f5c&quot;&gt;The
fix&lt;/a&gt;
to stop inheriting from &lt;code class=&quot;highlighter-rouge&quot;&gt;SceneEntity&lt;/code&gt; and to store &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;loc&lt;/code&gt;
directly in &lt;code class=&quot;highlighter-rouge&quot;&gt;InstanceSceneEntity&lt;/code&gt; changed 6 lines of code.  Time to first
pixel improved by 6 seconds, with the fix apparently contributing to
performance improvements in other places that were copying
&lt;code class=&quot;highlighter-rouge&quot;&gt;InstanceSceneEntity&lt;/code&gt; objects as well.  A bit more time was shaved off in a
follow-on change that &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/2e6aace69d877295d33292dffa3239e04546d44b&quot;&gt;tuned up the hash table used in
TransformCache&lt;/a&gt;.
Together those fixes brought pbrt to 44 seconds to first pixel:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-parserwork.svg&quot; alt=&quot;CPU utilization (64 threads, with parser improvements)&quot; /&gt; &lt;/p&gt;

&lt;h2 id=&quot;increasing-imports&quot;&gt;Increasing Imports&lt;/h2&gt;

&lt;p&gt;That 18 second tail of low CPU utilization at the end of parsing had become
untenable.  Looking at pbrt’s logs, it was easy to see that the “isBeach”
and “isCoral” models were laggards there.  A few
additional uses of the new-ish &lt;a href=&quot;/matt/blog/2021/04/11/moana-time-to-first-pixel.html#parsing-in-parallel&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;
statement&lt;/a&gt;
that allows parallel parsing were enough to put more threads working on
those files, which was enough to bring us to 28.7 seconds to first pixel,
now 73 times faster than pbrt-v3 was when &lt;a href=&quot;/matt/blog/2018/07/08/moana-island-pbrt-1.html&quot;&gt;all this
began&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Put it all together, and here is where things stand with 64 threads:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-64-truefin.svg&quot; alt=&quot;CPU utilization (64 threads, with more use of Import)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Here now, I’m happy to stop, even with some potential left above the line.
For the last ten or so seconds the GPU is busy building acceleration
structures, so non-full CPU utilization there is perfectly fine.  I am a
little surprised that the CPU utilization isn’t better at the very start of
parsing, though haven’t dug into that further.&lt;/p&gt;

&lt;p&gt;With 4 cores and 8 threads, the news is even better: CPU spends much
of its time at close to full utilization and it’s 53 seconds to first
pixel—better than it was with 32 cores when we started today.  Here is
the CPU utilization graph for that case:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;/matt/blog/images/ttfp-cpu-8-truefin.svg&quot; alt=&quot;CPU utilization (8 threads)&quot; /&gt; &lt;/p&gt;

&lt;p&gt;(This graph uses the same x axis scale as the other graphs in this
post but here the value 1 on the y axis corresponds to all 8 threads being
busy.)&lt;/p&gt;

&lt;p&gt;I was surprised to see pbrt spending a few seconds with low CPU utilization in
“Lights/Textures/Materials” there; presumably that is waiting for the
environment light source’s future, but I’m not sure why that work wouldn’t
have been finished earlier along the way.  I’ll also leave answering that
question for another time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As it usually goes with the Moana Island scene, it’s been quite a journey.
Three years after its release, the complexity it offers continues to be the
best kind of challenging, even after I think I’ve already learned all of my
lessons from it.&lt;/p&gt;

&lt;p&gt;Going back to that beach view from earlier in this post: at the start of
this post, it took a total of 105 seconds of wall-clock time to render with
the GPU at 256 samples per pixel from start to finish.  With &lt;a href=&quot;https://github.com/mmp/pbrt-v4/tree/2e6aace69d877295d33292dffa3239e04546d44b&quot;&gt;the version
of pbrt-v4 at the
end&lt;/a&gt;,
that’s down to 67 seconds, roughly evenly split between processing the
scene and doing actual rendering—a fine place for wrap up this go-round with
the island scene.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;notes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:goog&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;/matt/blog/images/google-wth.png&quot;&gt;What the heck, Google??!?&lt;/a&gt; &lt;a href=&quot;#fnref:goog&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:al&quot;&gt;
      &lt;p&gt;As was learned during the initial implementation of this feature. &lt;a href=&quot;#fnref:al&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">More attention to pbrt's performance when getting ready to render Disney's Moana Island scene finally gets us somewhere.</summary></entry><entry><title type="html">Swallowing the Elephant (Part 10): Rendering on the GPU—Finally</title><link href="https://pharr.org/matt/blog/2021/07/29/moana-rendered-on-the-gpu.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 10): Rendering on the GPU—Finally" /><published>2021-07-29T00:00:00-07:00</published><updated>2021-07-29T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/07/29/moana-rendered-on-the-gpu</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/07/29/moana-rendered-on-the-gpu.html">&lt;p&gt;We’re big fans of actually making pictures around here, so avoiding the
topic of rendering performance and focusing on performance while getting
ready for rendering may seem a little off.  A flimsy defense for that is
“vegetables before dessert”; we must attend to our responsibilities before
we go off and have fun.  A better justification is more or less Amdahl’s
law: as rendering time decreases, overall performance is increasingly
determined by the cost of rest of the work that happens before rendering.
That motivation should be more clear by the end of this post.&lt;/p&gt;

&lt;h2 id=&quot;preliminaries-textures-and-curves&quot;&gt;Preliminaries: Textures and Curves&lt;/h2&gt;

&lt;p&gt;Making pbrt-v4’s GPU rendering path capable of rendering Disney’s &lt;a href=&quot;https://www.disneyanimation.com/resources/moana-island-scene/&quot;&gt;Moana
Island
scene&lt;/a&gt;
required attending to two details that I hadn’t gotten around to until
recently: supporting both &lt;a href=&quot;https://ptex.us/&quot;&gt;Ptex textures&lt;/a&gt; and pbrt’s
curve shape on the GPU.  Both features were laggards that hadn’t yet been wired
up on the GPU and both are used extensively in the Moana Island scene.&lt;/p&gt;

&lt;p&gt;Ptex is a texture representation from Walt Disney Animation Studios that
works around the uv-mapping problem by splitting meshes into &lt;em&gt;faces&lt;/em&gt; (that
may themselves be collections of triangles or quads) and then applying a
plain old [0,1]&lt;sup&gt;2&lt;/sup&gt; parameterization to each face.  The Ptex library 
handles the details of things like loading textures on demand, caching
parts of them in memory, and texture filtering.&lt;/p&gt;

&lt;p&gt;To my knowledge, there isn’t a Ptex implementation that runs on the GPU.
One way to work around this problem is to round-trip to the CPU and service
Ptex requests there; that approach was taken by Chris Hellmuth when he
&lt;a href=&quot;https://www.render-blog.com/2020/10/03/gpu-motunui/&quot;&gt;rendered the Moana Island scene on the
GPU&lt;/a&gt;.  That approach
gives gold-standard results, but at the cost of synchronization and data
transfer between the two processors as well as the risk of the CPU being
the performance bottleneck.&lt;/p&gt;

&lt;p&gt;Ingo Wald &lt;a href=&quot;https://ingowald.blog/2020/10/26/moana-on-rtx-first-light/&quot;&gt;took a different
approach&lt;/a&gt; when
he got the Moana Island rendering on the GPU; his implementation resampled
each face’s texture at a low resolution and then packed the results into
large flat 2D textures.  That keeps everything on the GPU, but risks
blurred texture lookups due to insufficient resolution.&lt;/p&gt;

&lt;p&gt;And then there’s the approach I took: for each face, pbrt-v4 computes the
face’s average texture value and stores it in an array.  A texture lookup[sic] is
then a simple index into that array using the face index.  (Thus, it’s
basically Ingo’s approach with “low resolution” taken all the way to a
single pixel.)  The only thing defensible about my approach is that it’s
just &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/textures.cpp#L755&quot;&gt;a few lines of
code&lt;/a&gt;
to convert Ptex textures into this representation, and texture lookup is
near-trivial &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/textures.h#L989&quot;&gt;indexing into that
array&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I can’t say that I’m proud of that solution, but I can at least say that it
works great for objects that are far away where all you need is the top MIP
level anyway.  As we will see shortly, it is certainly not
production-ready.  I hope to get around to replacing it with something
better in the future, but for now it gets us up and rendering.&lt;/p&gt;

&lt;p&gt;The other thing to take care of was supporting curves on the GPU.
pbrt-v4’s built-in curve shape uses a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/shapes.cpp#L606&quot;&gt;recursive intersection
algorithm&lt;/a&gt;
that is a poor fit for the GPU; I didn’t even try running it there.  OptiX
does provide a curve primitive, highly optimized, though some plumbing
would be necessary to wire up pbrt’s curve representation to use it.
Impatient to get the scene up and rendering, I wrote a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/gpu/aggregate.cpp#L504&quot;&gt;simple function
that dices curves into bilinear
patches&lt;/a&gt;.
(This, too, is something to return to in the future.)&lt;/p&gt;

&lt;p&gt;To my delight, once those two additions were debugged, everything 
just worked the first time I tried rendering the Moana Island on the GPU.&lt;/p&gt;

&lt;h2 id=&quot;images-and-performance&quot;&gt;Images and Performance&lt;/h2&gt;

&lt;p&gt;Here again is the main view of the island, rendered on both the CPU and the
GPU with 256 samples per pixel at 1920x804 resolution.  The images are displayed using
&lt;a href=&quot;https://jeri.io/&quot;&gt;Jeri&lt;/a&gt;, which makes it possible to do all sorts of pixel
peeping right in the browser.  (Click on the tabs to switch between CPU and
GPU, or use the number keys after selecting the image. Hit ‘f’ to go
full-screen. You can also pan and zoom using the mouse.)  If you’d prefer to examine the EXRs directly, here they are:
&lt;a href=&quot;/matt/blog/images/moana-main-cpu-256spp.exr&quot;&gt;CPU&lt;/a&gt;, &lt;a href=&quot;/matt/blog/images/moana-main-gpu-256spp.exr&quot;&gt;GPU&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; width: 100%; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;mainview&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('mainview'), {
  title: 'mainview', children: [
 { title: 'CPU', image: '/matt/blog/images/moana-main-cpu-256spp.exr' },
 { title: 'GPU', image: '/matt/blog/images/moana-main-gpu-256spp.exr' }
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The differences between the image rendered on the CPU and the one rendered
on the GPU are entirely due to the differences in how Ptex textures and
curves are handled.  For Ptex, you can see the problems with the current
approach on the far-away mountainside as well as on the trunks of the palm
trees.  And then there’s a striking difference in the palm fronds; we’ll
return to that shortly, but it’s the GPU that has the more accurate result
there, not the CPU.&lt;/p&gt;

&lt;p&gt;Oh, and about rendering performance?  It’s 26.7 seconds on the
GPU (an NVIDIA RTX A6000) versus 326.5 seconds on the CPU (a 32 core AMD
3970X).  Work out the division and that’s 12.2x faster on the GPU.  If
you’d prefer a clean 2048 sample per pixel rendering, the GPU gets through
that in 215.6 seconds, once again over 12x faster than the CPU doing the same.&lt;/p&gt;

&lt;p&gt;And so it’s obvious why time to first pixel matters so much.  From start to
finish, that 256 sample per pixel rendering takes about 90 seconds of
wall-clock time.  Two thirds of it is spent getting things ready to render,
30% is rendering, and the rest is a few seconds of shutting things down at
the end.  With rendering being that fast, if you want to see that image
sooner, optimizing startup time can be a better place to focus than
optimizing rendering time.  Naturally, startup time matters less as the
number of pixel samples increases, but that has to go well into the
thousands of them before startup time starts to be insignificant.&lt;/p&gt;

&lt;p&gt;There is good news and bad news about memory: the scene needs “just” 29.0
GB of GPU memory to render.  I’m happy with that in absolute terms, but
unfortunately it limits how many GPUs can handle the scene.  It would be
nice to find a way to fit it in 24 GB, in which case it could be rendered
on an RTX 3090, but for now the full scene requires something along the
lines of an RTX A6000.  (As a workaround, removing the “isIronwoodA1” and
“isCoral” models gets it down under 24 GB with limited visual impact and,
bonus, takes time to first pixel down to 51 seconds.)&lt;/p&gt;

&lt;p&gt;As far as where the time is spent, pbrt-v4 offers a &lt;code class=&quot;highlighter-rouge&quot;&gt;--stats&lt;/code&gt; command-line
option that prints out various statistics after rendering finishes.  When
the GPU is used, it gives a summary of where the GPU spent its time during
rendering.  Here, sorted and summarized, is what it has to say about
rendering the main view of the Moana Island:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;  Total Time (ms)  &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;  Percentage&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tracing closest hit rays&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11976.57&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;45.4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Tracing shadow rays&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8441.60&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32.0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Material/BSDF evaluation and shading&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4294.59&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generating samples&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;638.44&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Generating camera rays&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;443.17&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.7%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Handling escaped rays&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;267.56&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Updating the film&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;127.35&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Handling emitters hit by indirect rays&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;97.47&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Other&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75.61&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;That’s 77.4% of the total runtime spent on ray intersection tests and
associated &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/wavefront/intersect.h#L49&quot;&gt;enqueuing of subsequent
work&lt;/a&gt;.
Most of the rest is in that 16.3% of material and BSDF work that is done at
each intersection.  It includes evaluating textures, sampling light
sources, and evaluating and sampling BSDFs.  For less complex scenes,
that’s where most of the runtime is normally spent.&lt;/p&gt;

&lt;p&gt;With apologies to the artists who spent untold hours on the textures for
this scene, here is another view of the island scene, this one the
“rootsCam” camera. (Direct links to the EXRs: 
&lt;a href=&quot;/matt/blog/images/moana-roots-cpu-256spp.exr&quot;&gt;CPU&lt;/a&gt;, 
&lt;a href=&quot;/matt/blog/images/moana-roots-gpu-256spp.exr&quot;&gt;GPU&lt;/a&gt;.)&lt;/p&gt;

&lt;div class=&quot;card-img-top&quot; style=&quot;display:block; width: 100%; padding-top: 47%;  position:relative;&quot;&gt;
&lt;div id=&quot;rootsview&quot; style=&quot;position: absolute; top: 0; left: 0; right: 0; bottom: 0;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
Jeri.renderViewer(document.getElementById('rootsview'), {
  title: 'rootsview', children: [
 { title: 'CPU', image: '/matt/blog/images/moana-roots-cpu-256spp.exr' },
 { title: 'GPU', image: '/matt/blog/images/moana-roots-gpu-256spp.exr' }
 ]});
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Again rendered at 256 samples per pixel, this took 32.1 seconds to render
on the GPU and 557.6 seconds on the CPU.  That’s 17.4x faster on the GPU,
with a similar breakdown of where the time was spent.&lt;/p&gt;

&lt;p&gt;With this viewpoint the shortcomings of pbrt-v4’s current approach for
Ptex on the GPU are even more obvious; not only do the sand dunes appear
faceted from lack of texture detail, but we have lost all of the fine
bump-mapping detail.  (Turns out, taking differences of a constant function
to compute shading normals doesn’t work out so well.)&lt;/p&gt;

&lt;p&gt;However, it is clear from these images that it is the GPU that is giving
the better result in those tufts of grass in the lower right.  The CPU’s
rendering path isn’t getting the self-shadowing correct down there, while
the GPU’s is.  (The same thing is happening in the palm fronds in the main
view.)  This discrepancy was unexpected and is something to chase down in
the future. I suspect the issue stems from the CPU curve implementation
needing a &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/4763b251b592a6ba95c811bc1877ff0ff1c21c13/src/pbrt/shapes.cpp#L719&quot;&gt;fairly large
epsilon&lt;/a&gt;
at ray intersections; this is necessary to avoid self-intersections since
the CPU’s curve shape orients itself to face the ray being traced.  On the
GPU, a much smaller epsilon is possible because true geometry is used for
curves.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The 12-17x speedups on the GPU are not based on a comparison of
perfectly-matching implementations, though other than the ray intersection
routines, curves, and Ptex, both otherwise run the same C++ code.  Each is
better and worse than the other in different ways: while the diced curve
representation used on the GPU turned out to be superior to the CPU’s curve
shape, the lack of proper Ptex texturing on the GPU at the moment is a
loss.&lt;/p&gt;

&lt;p&gt;One nice thing about the performance breakdown on the GPU is that there’s
plenty of headroom to do more shading work.  With 77% of runtime spent on
ray intersections and 16% on shading, even doubling the cost of shading
with a more complete Ptex implementation wouldn’t increase the overall
runtime very much.  I expect that the GPU’s speedup wouldn’t be too
different with those two differences harmonized.&lt;/p&gt;

&lt;p&gt;Next time we’ll come back to to review where pbrt-v4 stands in the time to
first pixel department and then end this series, at least for now, with
some renderer-design retrospection.&lt;/p&gt;</content><author><name></name></author><summary type="html">After our extended tour through where pbrt-v4 spends its time getting ready to render the Moana Island scene, we finally look at rendering, comparing performance and images from the CPU and GPU rendering paths.</summary></entry><entry><title type="html">Swallowing the Elephant (Part 9): We Got Instances</title><link href="https://pharr.org/matt/blog/2021/07/27/moana-gpu-instances.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 9): We Got Instances" /><published>2021-07-27T00:00:00-07:00</published><updated>2021-07-27T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/07/27/moana-gpu-instances</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/07/27/moana-gpu-instances.html">&lt;p&gt;&lt;a href=&quot;/matt/blog/2021/07/25/moana-gpu-part-1.html&quot;&gt;Last time around&lt;/a&gt; we finally
got started digging into &lt;a href=&quot;https://github.com/mmp/pbrt-v4&quot;&gt;pbrt-v4&lt;/a&gt;’s
performance with the Moana Island scene using its GPU rendering path.
Then, as today, the focus was limited to all of the processing that goes on
before rendering begins.  There was plenty left unresolved by the end,
including 16 seconds spent building BVHs for the object instances that
featured poor utilization on both CPU and GPU.&lt;/p&gt;

&lt;p&gt;Before we get into trying to improve that, here is the far-away view of the
Moana Island scene, again rendered on an NVIDIA RTX A6000.  As before,
mum’s the word on performance until next time, but once again, this didn’t
take too long.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/pbrt-v4-moana-gpu.jpg&quot; alt=&quot;Moana Island main view rendered with pbrt-v4 using the GPU&quot; /&gt;
&lt;i&gt;Moana Island main view rendered at 1920x804 resolution with pbrt-v4 on an NVIDIA RTX A6000 GPU with 2048 samples per pixel.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Now back to work.  In the 16 seconds that pbrt-v4 spends in its &lt;em&gt;Build
instance BVHs&lt;/em&gt; phase, it does the following three things for each geometric
object that is instanced repeatedly in the scene:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reads any geometry specified via PLY files from disk. (Any geometry not
specified in PLY files has already been read during regular parsing
of the scene description.)&lt;/li&gt;
  &lt;li&gt;Converts the geometry into the in-memory geometric representation that
OptiX takes as input to its BVH construction routines.&lt;/li&gt;
  &lt;li&gt;Has OptiX build its BVH.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two steps run on the CPU and the third runs on the GPU.&lt;/p&gt;

&lt;p&gt;There are a total of 312 such instance definitions and the work for one is
independent of the work for all of the others; this is a friendly problem
to parallelize.  Yet if we look at the CPU and GPU utilization graphs from
where we left off last time, the results are unimpressive during the
&lt;em&gt;Process instance BVHs&lt;/em&gt; phase:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-parallelize-optix-instance.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-parallelize-optix-instance.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(As before, the x axis is seconds, y axis is processor utilization, and the
dashed line indicates the start of rendering.)&lt;/p&gt;

&lt;p&gt;It starts out looking promising with 40% of the CPU in use, but after less
than two seconds of that, CPU utilization drops down to just a few cores
until all the instances are finished.  The GPU is occasionally fully
occupied, but it’s idle for much of the time.  Thus, we can’t blame all of
that CPU idling on threads waiting for the GPU.&lt;/p&gt;

&lt;h2 id=&quot;starting-with-the-obvious&quot;&gt;Starting With The Obvious&lt;/h2&gt;

&lt;p&gt;The natural place to start is to parallelize the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/3a09fc31cef26a83dd2ce505880ca4dac8e291b0/src/pbrt/gpu/aggregate.cpp#L1385&quot;&gt;outermost
loop&lt;/a&gt;
that does the three things outlined above; honestly, there’s no excuse for
it not having been parallel from the start.  The change is &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/2842083e37fd30d566091f0420d751628e492b80&quot;&gt;barely any work
at
all&lt;/a&gt;,
and in a world where the performance gods were feeling generous, that would
be the end of it.  The only thing that one might worry about in the
parallelization is contention on the mutex used to serialize updates to the
hash table, bit with just 312 instances, it seems
unlikely that will be a big problem.&lt;/p&gt;

&lt;p&gt;The good news is that this change did reduce time to first pixel; the bad
news is that it was only down to 68.5 seconds—an improvement of just 3.7
seconds.  I’m always happy to take a 5% improvement in overall performance,
but one has to feel a little mixed about that when it might have been much
more.  Here are the performance graphs—as before they start after parsing
has finished and the lights, materials, and textures have been created:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-ias-1.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-ias-1.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We can see that 3.7 second improvement; we can see that CPU utilization is
better throughout instance processing; and we can see that the GPU spends
less time idle. Yet, there’s nothing thrilling in the graphs: the CPU is
still sitting around not making much of itself and the GPU isn’t being
pushed very hard.&lt;/p&gt;

&lt;h2 id=&quot;no-luck-from-the-next-three-obvious-things&quot;&gt;No Luck From The Next Three Obvious Things&lt;/h2&gt;

&lt;p&gt;Parallelizing the outermost loop isn’t enough if there’s something that serializes
work in the middle of it.  I soon remembered such a thing in the
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/3a09fc31cef26a83dd2ce505880ca4dac8e291b0/src/pbrt/gpu/aggregate.cpp#L104&quot;&gt;function that launches the OptiX kernels to build
BVHs&lt;/a&gt;.
BVH construction there is serialized; not only is all work submitted to the
main CUDA command stream, but the CPU synchronizes with the GPU twice along
the way.  As a result, the GPU can only work on one BVH at a time, and
if another thread shows up wanting to build its own independent BVH, its work
is held up until the GPU finishes whatever it is already in the middle of.&lt;/p&gt;

&lt;p&gt;I decided that leaving the synchronization in wouldn’t be too terrible if
pbrt &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/4b74e6b6156d05c53a54df63da1bd3b121e4b2c9&quot;&gt;allowed each thread to independently submit work to the
GPU&lt;/a&gt;.
That was mostly a matter of taking a &lt;code class=&quot;highlighter-rouge&quot;&gt;cudaStream_t&lt;/code&gt; in that function’s
arguments and passing a different stream for each thread.  Thus, each
thread will wait for its own BVH to be built but it won’t be prevented
from starting its own work by other threads.  In turn, the GPU can work
on multiple BVHs in parallel, which is helpful when there are instances that
aren’t very complex.&lt;/p&gt;

&lt;p&gt;Sadly, and to my surprise, the performance benefit from that change was nil.&lt;/p&gt;

&lt;p&gt;I flailed a bit at that point, parallelizing two more inner loops in
instance BVH construction
(&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/a881617a1a9a49561022de53c2cf863cac0e0394&quot;&gt;1&lt;/a&gt;)
(&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/ad9a5ebe300d40e59c6c2c42b2651cc2169a026a&quot;&gt;2&lt;/a&gt;),
hoping that giving the CPU more available work would help.  From that, too,
there was no meaningful change in performance.  Time to get scientific again.&lt;/p&gt;

&lt;h2 id=&quot;too-much-performance-to-handle&quot;&gt;Too Much Performance To Handle&lt;/h2&gt;

&lt;p&gt;Giving up on an easy win from semi-informed guesses, I returned to my tried
and true performance bottleneck finder: running pbrt under the debugger,
interrupting execution when CPU utilization was low, and seeing what was
actually going on.  A quick scan of all of the threads’ backtraces at one
of these points showed that all 64 were in the the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/cf042ca90b398b36677cbd7f83fda43bb0078b58/src/pbrt/util/mesh.cpp#L23&quot;&gt;TriangleMesh
constructor&lt;/a&gt;.
(“That’s funny—we shouldn’t be spending much time there at all” was my
first reaction; that reaction is almost always good news when one is looking for
ways to improve performance.)&lt;/p&gt;

&lt;p&gt;Not only were all the threads in that constructor, but all but one was held
up waiting for the same mutex, which was held by the remaining one.  And
yet, there’s nary a mutex to be seen in that code…&lt;/p&gt;

&lt;p&gt;A closer look at the stack traces and it became clear that the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/a881617a1a9a49561022de53c2cf863cac0e0394/src/pbrt/gpu/memory.cpp#L31&quot;&gt;mutex in
pbrt’s GPU memory
allocator&lt;/a&gt;
was the point of contention; if 64 threads are trying to allocate of meshes
all at once, things will understandably go bad there.&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/20dd25e41cb61f3560e49193d1388e4058c425d3&quot;&gt;updated the allocator to use per-thread
slabs&lt;/a&gt;,
where each thread periodically goes out to the main allocator for a 1MB
chunk of memory but otherwise allocates memory from its own chunk
directly, no mutex required.  I assumed that this would be enough to
make that allocation much less of a bottleneck, but there’s no way to know
until you actually run the code.&lt;/p&gt;

&lt;p&gt;I could tell that I was getting somewhere when my computer locked up and
shut down when I ran pbrt with that fix.  I restarted it and tried again,
and was thrilled when my computer died once again.  Progress!&lt;/p&gt;

&lt;p&gt;To explain: I’ve been using a slightly under-powered power supply with this
computer for a while.  It’s enough to power the CPU at full utilization and
is enough to power the GPU at full utilization.  Both at the same time?  A
bit too much to ask for.  It hadn’t been much of a problem; I just got used
to not running big compilation jobs at the same time that the GPU was busy.
In most of my day-to-day work, it’s one processor or the other at work at a
time.  Those crashes were a good hint that I had gotten both processors to
be busy at once, which seemed promising.&lt;/p&gt;

&lt;p&gt;Unwilling to wait for a new power supply to be delivered in the mail, I
braved a trip to Best Buy for quick gratification and 1000 W of
future-proof power.  I could reliably measure performance after swapping
out the old power supply; time to first pixel was down to 64.9
seconds—another 3.6 seconds improvement—and graphs that were looking
better:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-cuda-mem-slabs.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-cuda-mem-slabs.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The extra good news in those graphs is that the first phase, &lt;em&gt;Process
non-instanced geometry&lt;/em&gt;, unexpectedly got one second faster—look at that
spike in CPU utilization there now! Apparently instance BVH construction
wasn’t the only thing bottlenecked on that mutex.&lt;/p&gt;

&lt;p&gt;And yet even with that fixed, CPU utilization there was still middling.&lt;/p&gt;

&lt;h2 id=&quot;its-always-a-mutex&quot;&gt;It’s Always a Mutex&lt;/h2&gt;

&lt;p&gt;Another round with the debugger as profiler and there was still lots of mutex
contention under the &lt;code class=&quot;highlighter-rouge&quot;&gt;TriangleMesh&lt;/code&gt; constructor.  A little more digging and
it became clear that the
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/3a09fc31cef26a83dd2ce505880ca4dac8e291b0/src/pbrt/util/buffercache.h#L34&quot;&gt;BufferCache::LookupOrAdd()&lt;/a&gt;
method was the culprit.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;BufferCache&lt;/code&gt; is something I added to pbrt after my first go-round with the
Moana Island; it uses a hash table to detect redundant vertex and index buffers in the scene
and only stores each one once.  It makes a difference—even though the
Moana Island comes in highly-instanced, &lt;code class=&quot;highlighter-rouge&quot;&gt;BufferCache&lt;/code&gt; saves 4.9 GB of
memory when used with it.&lt;/p&gt;

&lt;p&gt;What was its problem?  It’s bad enough that it’s worth copying a few lines
of code here:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    std::lock_guard&amp;lt;std::mutex&amp;gt; lock(mutex);
    Buffer lookupBuffer(buf.data(), buf.size());
    if (auto iter = cache.find(lookupBuffer); iter != cache.end()) {
       ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Not only do we have have our single contended mutex, but if you trace through the
&lt;code class=&quot;highlighter-rouge&quot;&gt;Buffer&lt;/code&gt; class and the hashing flow, it turns out that hash of the buffer
data is computed with the mutex held—totally unnecessary and awfully
rude, especially if one’s buffer covers many megabytes of memory.&lt;/p&gt;

&lt;p&gt;The
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/cf042ca90b398b36677cbd7f83fda43bb0078b58&quot;&gt;fix&lt;/a&gt;
makes three improvements: the hash is computed before the lock is acquired,
a reader-writer lock is used so that multiple threads can check for their
buffer in the cache concurrently, and the hash table is broken up into 64
shards, each protected by its own mutex.  In short, an attempt to atone for
the initial failure with plenty of parallelism prophylaxis.&lt;/p&gt;

&lt;p&gt;Survey says? 5.3 seconds faster, which brings us down to &lt;strong&gt;59.6 seconds&lt;/strong&gt; for
the time to first pixel.  An extra bonus is that the first phase, &lt;em&gt;Process
non-instanced geometry&lt;/em&gt;, sped up by another 0.8 seconds, bringing it down
to 3.0 seconds (versus the 4.9 seconds it was at the start of today’s
work).  The performance graphs are starting to go somewhere:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-shard-buffercache.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-shard-buffercache.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;There’s still plenty of CPU idle time in &lt;em&gt;Build instance BVHs&lt;/em&gt;, though one
might make the observation that CPU utilization and GPU utilization are
roughly inversely correlated there.  That fits with the fact that CPU threads
go idle while waiting for the GPU to build their BVHs, which is a
shortcoming that I think we will stick with for now in the interests of
implementation simplicity.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With the one-minute mark broken, my motivation started to wane.  The most
obvious remaining problems are low CPU utilization at the start of &lt;em&gt;Process
non-instanced geometry&lt;/em&gt; and at the end of &lt;em&gt;Build instance BVHs&lt;/em&gt;.  I believe
that those both correspond to the CPU working its way through a large PLY file in a
single thread; splitting those files multiple smaller ones files that could
be read in parallel would likely shave a few more seconds off.&lt;/p&gt;

&lt;p&gt;Next time, we’ll turn to rendering, covering some of the details related to
getting this scene to render on the GPU in the first place and finally
looking at rendering performance.&lt;/p&gt;</content><author><name></name></author><summary type="html">More work on performance with pbrt-v4 and the Moana Island scene, this time looking at getting object instances ready for rendering.</summary></entry><entry><title type="html">Swallowing the Elephant (Part 8): Meet The GPU</title><link href="https://pharr.org/matt/blog/2021/07/25/moana-gpu-part-1.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 8): Meet The GPU" /><published>2021-07-25T00:00:00-07:00</published><updated>2021-07-25T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/07/25/moana-gpu-part-1</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/07/25/moana-gpu-part-1.html">&lt;p&gt;It’s been over three months since my &lt;a href=&quot;/matt/blog/2021/04/11/moana-time-to-first-pixel.html&quot;&gt;last
post&lt;/a&gt; about rendering Disney’s
&lt;a href=&quot;https://www.disneyanimation.com/resources/moana-island-scene/&quot;&gt;Moana Island
scene&lt;/a&gt; with
&lt;a href=&quot;https://github.com/mmp/pbrt-v4&quot;&gt;pbrt-v4&lt;/a&gt;.  Back then, I promised a
further update that would discuss performance when rendering the scene on
the GPU, but here we are with months gone by and no further news.  Today,
finally, I’ll start to rectify that.&lt;/p&gt;

&lt;p&gt;Most of the delay was due to lack of motivation: for once, performance was
surprisingly good out of the box. Where we left off last time, pbrt-v4’s
CPU rendering path was down to 96.8 seconds of wall-clock time between the
start of parsing the scene description and the start of actual rendering
work.  With the GPU path selected and with no further attention to performance tuning,
rendering started after 83.3 seconds—1.16x faster.&lt;/p&gt;

&lt;p&gt;Of course, faster is to be expected, as roughly half of the pre-rendering
time on the CPU is spent building BVHs.  pbrt’s BVH construction code is
written for clarity rather than performance, is only sort-of parallelized,
and runs entirely on the CPU.  When rendering on the GPU, all of the BVH
construction is handled by highly-optimized code in OptiX, most
of it running on the GPU.  If using &lt;em&gt;that&lt;/em&gt; had been slower than pbrt’s CPU path,
then there surely would have been “interesting” things to discover.
However, faster was the expectation, and faster was what we got.&lt;/p&gt;

&lt;p&gt;Further sapping motivation for blogging, not only was system startup fast,
but rendering the scene itself on the GPU pretty much just worked the first
time.  Here’s the beach view, rendered using an NVIDIA RTX A6000 GPU.  I’m
going to save the topic of GPU rendering performance for a later post, but,
well… It’s most definitely &lt;em&gt;fast&lt;/em&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/pbrt-v4-moana-beach-gpu.jpg&quot; alt=&quot;Moana Island beach view rendered with pbrt-v4 using the GPU&quot; /&gt;
&lt;i&gt;Moana Island beach view rendered with pbrt-v4 on an NVIDIA RTX A6000 GPU with 1024 samples per pixel.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Good performance is always nice, but this series of blog posts has mostly
been about chasing down and fixing performance problems due to poor scaling
in the face of complexity; everything running well doesn’t leave
much to write about. With what I saw at first, I assumed that I would end
up with a short post without much technical content that instead declared
victory and showed a few pretty pictures.  Happily (in a way), once I
started actually looking at the data, there were all sorts of good surprises.&lt;/p&gt;

&lt;h2 id=&quot;the-value-of-logging-for-finding-stinky-things&quot;&gt;The Value of Logging For Finding Stinky Things&lt;/h2&gt;

&lt;p&gt;I’m all for a good debugger and a good profiler, but it’s surprising
how far one can go with basic instrumentation and logging.  For example,
if you give pbrt &lt;code class=&quot;highlighter-rouge&quot;&gt;--log-level verbose&lt;/code&gt; in its command-line arguments, it
prints all sorts of chatty information as it does its work, announcing that
it’s starting up the thread pool, telling you what it has found as far as
GPUs in the system, and giving updates about what it’s currently working
on.&lt;/p&gt;

&lt;p&gt;Here are a few lines of what it prints early on when rendering
the Moana island scene:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ tid 000 @     0.103s parser.cpp:745 ] Started parsing materials.pbrt
[ tid 000 @     0.103s parser.cpp:620 ] Finished parsing materials.pbrt
[ tid 000 @     0.103s parser.cpp:590 ] Started parsing isMountainA/isMountainA.pbrt
[ tid 000 @     0.103s parser.cpp:590 ] Started parsing isMountainB/isMountainB.pbrt
[ tid 000 @     0.103s parser.cpp:590 ] Started parsing isGardeniaA/isGardeniaA.pbrt
[ tid 000 @     0.103s parser.cpp:745 ] Started parsing isMountainA/objects.pbrt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In each line we get the id of the thread that issued the log message (here,
always the main thread), the elapsed time since pbrt started running, the
location in the source where the logging call was made, and then whatever
it has to tell us.  Here we can immediately see that the entirety of
&lt;code class=&quot;highlighter-rouge&quot;&gt;materials.pbrt&lt;/code&gt; was parsed nearly instantaneously.  We can
also see that multiple additional files are being parsed in parallel, each
getting started without pbrt waiting for the one before it to finish.  This all
is to be expected, so no surprises so far.&lt;/p&gt;

&lt;p&gt;As I was gearing up to start writing this post, I noticed the following when doing
a run with &lt;code class=&quot;highlighter-rouge&quot;&gt;--log-level verbose&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ tid 000 @    43.820s wavefront/integrator.cpp:140 ] Starting to create lights
[ tid 000 @    53.465s wavefront/integrator.cpp:229 ] Done creating lights
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;That’s ten seconds to create the light sources in the scene.  Those ten seconds
were especially evident when seen live—there’s “Starting to create
lights”, then no logging whatsoever for ten long seconds before pbrt comes
back, bright eyed and proud that it’s gotten all the lights taken care of.
That long pause was enough for me to realize something was off; I knew that
it had been no more than 5 or so seconds to create the lights for this
scene before, so there was surely something amiss.&lt;/p&gt;

&lt;p&gt;While a profiler could have cleared up what was happening in those ten
seconds, a simple sampling-based approach was enough: I re-ran pbrt (still
using an optimized build) with the debugger, waited for that long ten
second pause, then stopped execution and printed out the current stack
trace before letting it resume.&lt;sup id=&quot;fnref:sampling-profiling&quot;&gt;&lt;a href=&quot;#fn:sampling-profiling&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; A few quick iterations of
that was all it took to discover that much of that time was spent in two
functions that checked whether an image had any pixels with floating-point
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/cc8f68c38fc6abe2a9aee031bc75a457e517265a/src/pbrt/util/image.cpp#L199&quot;&gt;infinity&lt;/a&gt;
or
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/cc8f68c38fc6abe2a9aee031bc75a457e517265a/src/pbrt/util/image.cpp#L208&quot;&gt;not-a-number&lt;/a&gt;
values.&lt;/p&gt;

&lt;p&gt;I had added those checks after spending way too much time debugging an
unexpected infinity that turned out to be from an environment map light
source that had an infinite-valued pixel.  Now, the Moana island scene
includes an environment map that is 8k pixels square, otherwise known as 64
million pixels.  Oh, and those pixels are stored as 8-bit values, so
there’s no risk of funny floating-point values in any of them.  Perhaps looping over all of them, lovingly converting them from 8-bit sRGB to float, and then seeing if they were perhaps infinite was not the best use of cycles.&lt;/p&gt;

&lt;p&gt;It took two &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/f5668154f298ecef5a6fc5a1d62a9ec11e6abecf&quot;&gt;one-line
fixes&lt;/a&gt;
to early out in that case, and light creation time went back to the 5
seconds or so that it used to be.  The total time to first pixel went down
to 77.9 seconds—19 seconds faster than when rendering with the
CPU.&lt;/p&gt;

&lt;h2 id=&quot;basic-profiling-in-the-renderer&quot;&gt;Basic Profiling in the Renderer&lt;/h2&gt;

&lt;p&gt;Fixing that self-inflicted wound gave me some momentum; it was a small
taste of the satisfaction of making something slow run faster and that was
enough to give me motivation to dig in further.  However, the task is
trickier than it was before since pbrt is now doing work on both the CPU
and the GPU. It’s important to understand what each is up to; for example,
maybe it’s fine if the CPU is mostly idle at some point if the GPU is
working full-tilt.  However, if both are lounging around not doing much,
then perhaps we should see where the slacking lies.&lt;/p&gt;

&lt;p&gt;Sticking with my log-based methodology, I &lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/6e48cc048a393af74124b01f6bae6b8871d454ed&quot;&gt;added a &lt;code class=&quot;highlighter-rouge&quot;&gt;--log-utilization&lt;/code&gt;
command-line
option&lt;/a&gt;
to pbrt.  It causes an extra thread to be launched that measures current
system activity every 100ms and reports it to pbrt’s verbose log.  Thus,
when it’s running, you get updates ten times as a second about what’s going
on interspersed with pbrt’s regular logging output, which makes it easy to
connect to what pbrt is doing.  Here’s what it reports at the start of
light creation:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[ tid 000 @    43.213s pbrt/wavefront/integrator.cpp:140 ] Starting to create lights
[ tid 000 @    43.267s pbrt/util/log.cpp:175 ] CPU: Memory used 32101 MB. Core activity: 0.0155 user 0.0000 nice 0.0000 system 0.9845 idle
[ tid 000 @    43.267s pbrt/util/log.cpp:193 ] GPU: Memory used 1156 MB. SM activity: 0 memory activity: 0
[ tid 000 @    43.367s pbrt/util/log.cpp:175 ] CPU: Memory used 32101 MB. Core activity: 0.0140 user 0.0000 nice 0.0016 system 0.9845 idle
[ tid 000 @    43.368s pbrt/util/log.cpp:193 ] GPU: Memory used 1156 MB. SM activity: 0 memory activity: 0
[ tid 000 @    43.468s pbrt/util/log.cpp:175 ] CPU: Memory used 32101 MB. Core activity: 0.0155 user 0.0000 nice 0.0000 system 0.9845 idle
[ tid 000 @    43.469s pbrt/util/log.cpp:193 ] GPU: Memory used 1156 MB. SM activity: 0 memory activity: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Both CPU and GPU activity are reported where a value of 1 represents full
utilization.  The system I’m using has a 32 core/64 thread CPU, so all 64 of those
threads would need to be active and doing work to achieve a 1.
Here, we can see that we’ve got one thread keeping busy (1/64=0.0156), with
everything else sitting idle.  That’s not impressive, but it’s not
unexpected: there’s little parallelism in that part of
the system.&lt;/p&gt;

&lt;p&gt;Just as with the regular text logging, I often find it productive to
eyeball that output.  Though it’s not as fancy as a graphical profiler,
there’s nearly zero overhead to using it; sometimes, low friction is more
important than comprehensive data.  Reading such logs is actually
how I did all of the work described in this and the two following posts,
though it’s also easy enough to make graphs using that output as well.&lt;/p&gt;

&lt;h2 id=&quot;first-performance-graphs-and-setting-a-baseline&quot;&gt;First Performance Graphs and Setting a Baseline&lt;/h2&gt;

&lt;p&gt;As a baseline, here is where things stood with the light creation fix in
there, shown using separate graphs for the CPU and the GPU.&lt;sup id=&quot;fnref:gpu-lag&quot;&gt;&lt;a href=&quot;#fn:gpu-lag&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
These
graphs start after the scene description has been parsed and materials,
textures, and lights have been created. For these investigations we’ll
focus on the geometric work that happens after all that finishes, starting
47 seconds in.  Up until then both the CPU and GPU path run the same code,
which has probably received &lt;a href=&quot;/matt/blog/2021/04/11/moana-time-to-first-pixel.html&quot;&gt;enough
attention&lt;/a&gt; for now.&lt;/p&gt;

&lt;p&gt;Here, x axis is time in seconds and the y axis is processor utilization.
The start of rendering is indicated by a vertical dashed line.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-fiximage.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-fiximage.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Four phases of work that are shown in these graphs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Process non-instanced geometry&lt;/em&gt;: all of the non-instanced geometry (e.g., the mountains and the ocean surface) is converted to objects like pbrt’s &lt;code class=&quot;highlighter-rouge&quot;&gt;TriangleMesh&lt;/code&gt;, including reading geometry that stored in PLY files from disk. Once this geometry is in memory, pbrt has OptiX build BVHs for it.  (Triangles and non-triangles go into separate BVHs.)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Build instance BVHs&lt;/em&gt;: For each geometric object that is instanced (e.g., each type of shrub and flower), the geometry is read from disk and an individual BVH is built.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Process instance uses&lt;/em&gt;: For each use of a geometric instance, a structure is initialized that bundles up the handle to the instance’s BVH and the transformation matrix that places it in the scene.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Build top-level BVH&lt;/em&gt;: The final scene-wide BVH is built including both the non-instanced geometry and all of the instance uses.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Going back to the graphs, the only thing to be happy about is the final
stage of building the final top-level BVH: there’s not much left for the
CPU to do at that point and the GPU is nicely occupied during that phase.
For worse and for better, there’s plenty of unused computing capacity left
sitting idle in the rest of the graph.&lt;/p&gt;

&lt;h2 id=&quot;a-first-small-victory&quot;&gt;A First Small Victory&lt;/h2&gt;

&lt;p&gt;The nearly 6 seconds spent on &lt;em&gt;Process instance uses&lt;/em&gt; was one of the first
things that caught my eye, even though it’s not where most of the time is
spent.  If you look at &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/3a09fc31cef26a83dd2ce505880ca4dac8e291b0/src/pbrt/gpu/aggregate.cpp#L1427&quot;&gt;the corresponding
loop&lt;/a&gt;,
there’s not much to it; there’s a hash table lookup using the name of the instance
(e.g. “xgCabbage_archivecoral_cabbage0003_geo”).  That leads to the handle to
its BVH and then there’s just a bit of data movement, initializing an
&lt;code class=&quot;highlighter-rouge&quot;&gt;OptixInstance&lt;/code&gt; structure with the transformation matrix and a few other
things so that the instance use is part of the final scene BVH.&lt;/p&gt;

&lt;p&gt;Since that loop is not doing much that is computationally intensive, it
didn’t seem worth parallelizing when I first wrote it.  However, when you
have over 39 million instances, a little bit of data movement for each one adds up.
It’s easy enough to parallelize that loop:
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/757f00567cc11d2b9202daca21793eebaeb18a45&quot;&gt;(1)&lt;/a&gt;
&lt;a href=&quot;https://github.com/mmp/pbrt-v4/commit/1820fa7d618da0b51179261091b02279a2dd3dba&quot;&gt;(2)&lt;/a&gt;, 
and doing so brings &lt;em&gt;Process instance uses&lt;/em&gt; down from 5.9 seconds to 1.1 seconds of
wall-clock time, which brings us to 72.2 seconds for time to first pixel.  These
graphs show the improvement:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-cpu-parallelize-optix-instance.svg&quot; alt=&quot;CPU utilization&quot; /&gt;
&lt;br /&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-gpu-parallelize-optix-instance.svg&quot; alt=&quot;GPU utilization&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;(As in earlier posts, the scale of the x axis is the same as the baseline graph
in order to make it easier to compare successive graphs with each other.)&lt;/p&gt;

&lt;p&gt;CPU utilization during that phase is still a little spiky, but that phase
now achieves the best CPU utilization of all of them. With its total
time down to one second, it’s hard to worry too much more about it.&lt;/p&gt;

&lt;h2 id=&quot;looking-ahead&quot;&gt;Looking Ahead&lt;/h2&gt;

&lt;p&gt;It was a slow start, but we’ve gone from lack of motivation to two easy
fixes that shaved roughly ten seconds off of startup time.  Furthermore,
those have brought us to being “just” twelve seconds away from a
sub-one-minute time to first pixel.  &lt;em&gt;That&lt;/em&gt; would be exciting, and there’s
still got plenty of idle processor time in the graphs and plenty of code
still unexamined under the lens of Moana.  To that end, we’ll dig into the
instance BVH phase next time around.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;notes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:sampling-profiling&quot;&gt;
      &lt;p&gt;I’m not sure where I first learned about this trick.  I’m almost certain that I read a blog post extolling the idea somewhere years ago but am unable to find it again now. &lt;a href=&quot;#fnref:sampling-profiling&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gpu-lag&quot;&gt;
      &lt;p&gt;The GPU’s performance monitoring API reports its results averaged over an unspecified but up-to-one-second amount of previous time whereas the CPU numbers are calculated strictly based on activity in the last 100ms. This leads to some lag in the GPU results that manifests itself in activity sometimes being charged to the wrong stage; an example is “Process instance uses”, which doesn’t actually use the GPU at all. (I also believe that the spike in GPU activity at the start of “Build instance BVHs” should be charged to “Process non-instanced geometry.”) However, rather than futzing with the data, I have reported it as measured. &lt;a href=&quot;#fnref:gpu-lag&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Performance when rendering the Moana Island scene using pbrt-v4 on the GPU was surprisingly good at the start. And yet this is the first of three posts on the topic...</summary></entry><entry><title type="html">Swallowing the Elephant (Part 7): Time To First Pixel</title><link href="https://pharr.org/matt/blog/2021/04/11/moana-time-to-first-pixel.html" rel="alternate" type="text/html" title="Swallowing the Elephant (Part 7): Time To First Pixel" /><published>2021-04-11T00:00:00-07:00</published><updated>2021-04-11T00:00:00-07:00</updated><id>https://pharr.org/matt/blog/2021/04/11/moana-time-to-first-pixel</id><content type="html" xml:base="https://pharr.org/matt/blog/2021/04/11/moana-time-to-first-pixel.html">&lt;p&gt;With &lt;a href=&quot;/matt/blog/2021/04/02/moana-island-pbrt-fool-me-once.html&quot;&gt;memory use under
control&lt;/a&gt;,
today’s topic will be “time to first pixel” when rendering &lt;a href=&quot;https://www.disneyanimation.com/resources/moana-island-scene/&quot;&gt;Disney’s Moana
Island
scene&lt;/a&gt; with
pbrt—that is, how much time goes by between launching the renderer and
the start of actual rendering.  This measure covers all of the costs of
system startup, including parsing the scene, creating lights, shapes, and
textures, and building acceleration structures.&lt;/p&gt;

&lt;p&gt;Time to first pixel is a useful metric in that it is often the main
constraint on iteration time: have a bug in a ray intersection routine and
want to see if your fix took care of it?  Moved a light and want to see how
the image looks?  Time to first pixel is a big part of how quickly you get
those answers.&lt;/p&gt;

&lt;p&gt;Before we dig into the numbers, here is another view of the Moana Island
scene rendered with pbrt-v4:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/pbrt-v4-moana-beach.jpg&quot; alt=&quot;Moana Island beach view rendered with pbrt-v4&quot; /&gt;
&lt;i&gt;Moana Island beach view rendered with pbrt-v4. Rendering time at 1920x804 resolution with 1,024 samples per pixel was 63m58s on a 64 core AMD 3970X CPU.&lt;/i&gt;
&lt;/p&gt;

&lt;h3 id=&quot;foundations&quot;&gt;Foundations&lt;/h3&gt;

&lt;p&gt;The starting point was pretty ugly when I was first given access to the
Moana Island scene 2.5 years ago: &lt;a href=&quot;/matt/blog/2018/07/08/moana-island-pbrt-1.html#first-renderings&quot;&gt;pbrt-v3’s time to first pixel was
34m58s&lt;/a&gt;,
which is nothing short of horrific.  By the end of my efforts then, it was
&lt;a href=&quot;/matt/blog/2018/08/03/moana-reader-mail.html#parsing-floats-revisited&quot;&gt;down to
9m18s&lt;/a&gt;,
a 3.76x improvement.  At the time, that felt pretty good.  This is, after
all, a scene that exhibits the complexity of what is present in film
production today (or at least, what was present 5 or so years ago when
&lt;em&gt;Moana&lt;/em&gt; was made), and so it is to be expected that there will be some work
to be done before it’s ready to render.&lt;/p&gt;

&lt;p&gt;To get started, I measured time to first pixel with the latest version of
pbrt-v4 using a single thread–it was 6m50s.  Good news at the start for
once!  Here is a table that summarizes these timings and the respective
speedups:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Time&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Speedup&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v3 (original)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2098s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-next (2.5 years ago)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;558s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.76x&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pbrt-v4 (starting point today)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;410s&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.12x&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Where did that unexpected performance improvement come from?  Part of it is
that I ran the pbrt-v4 test on a modern CPU, while the earlier
measurements were on a Google Compute Engine instance with what is now a 5
or so year old CPU.  Thus, the latest measurement benefited from a higher
CPU clock rate and a few years of microarchitectural improvements.&lt;/p&gt;

&lt;p&gt;Another factor is an improvement to the surface area heuristic cost
computation in pbrt’s BVH construction algorithm.  In pbrt-v3 it used an
&lt;a href=&quot;https://github.com/mmp/pbrt-v3/blob/aaa552a4b9cbf9dccb71450f47b268e0ed6370e2/src/accelerators/bvh.cpp#L336&quot;&gt;O(n^2)
algorithm&lt;/a&gt;,
while it’s &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/b44bc261e52accffc1eb8b0da9a804d38319538d/src/pbrt/cpu/aggregates.cpp#L296&quot;&gt;an O(n) algorithm&lt;/a&gt;  in
pbrt-v4.
In my defense, n in this case is fixed at 12, which saves me from the
full infamy of
&lt;a href=&quot;https://randomascii.wordpress.com/2019/12/08/on2-again-now-in-wmi/&quot;&gt;Dawson’s&lt;/a&gt;
&lt;a href=&quot;https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/&quot;&gt;law&lt;/a&gt;,
though it’s still pretty indefensible.  Anyway, if I remember correctly,
improving that in pbrt-v4 roughly doubled the performance of BVH
construction, so that was surely part of it.&lt;/p&gt;

&lt;p&gt;For a starting point, here is a plot of CPU utilization over time with
single-threaded pbrt-v4 with the Moana island scene.  The horizontal axis
is time in seconds and the vertical is CPU utilization, measured with
respect to the 64 threads offered by an AMD 3970X CPU.  There’s not a lot
to see vertically, but the graph gives us a baseline and also shows where
the time is going: mostly parsing and BVH construction, both for instances
and for the top-level BVH.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-1thread.svg&quot; alt=&quot;CPU utilization with one thread&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(For this and following graphs, I’ve put light and texture creation into a
single category, since it’s about 8 seconds for both of them, most of those
spent reading the PNG for the environment light source.)&lt;/p&gt;

&lt;h3 id=&quot;about-all-those-idle-threads&quot;&gt;About all those idle threads…&lt;/h3&gt;

&lt;p&gt;Over the past year, I had already spent some time working on reducing
pbrt-v4’s time to first pixel.  That work was largely motivated by pbrt’s
GPU rendering path: it wasn’t unusual to spend more time loading the scene
description than rendering it with the GPU.  Optimizing startup was thence
the most effective way to speed up rendering—Amdahl’s law strikes again.&lt;/p&gt;

&lt;p&gt;That work was easier to do with pbrt-v4’s &lt;a href=&quot;/matt/blog/2021/04/02/moana-island-pbrt-fool-me-once.html#a-reorganization-of-the-parsing-code&quot;&gt;redesign of the scene parsing
code&lt;/a&gt;:
once the high-level &lt;code class=&quot;highlighter-rouge&quot;&gt;ParsedScene&lt;/code&gt; object is initialized, various
opportunities for parallelism are easily harvested.  With pbrt-v3, parsing
the scene description was intermingled with creating the scene data
structures, so there was less opportunity for extracting parallelism and
all of the work until the start of rendering was single-threaded.&lt;/p&gt;

&lt;p&gt;With pbrt-v4, it’s &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/b44bc261e52accffc1eb8b0da9a804d38319538d/src/pbrt/parsedscene.cpp#L907&quot;&gt;easy to parallelize loading
textures&lt;/a&gt;
when parameters for all of the ones to be loaded are at hand in a single
&lt;code class=&quot;highlighter-rouge&quot;&gt;vector&lt;/code&gt;.  Shapes are &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/b44bc261e52accffc1eb8b0da9a804d38319538d/src/pbrt/cpu/render.cpp#L125&quot;&gt;created in parallel as
well&lt;/a&gt;.
In practice, this means that if meshes are provided in PLY format files,
those can be loaded in parallel.  Finally, the &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/b44bc261e52accffc1eb8b0da9a804d38319538d/src/pbrt/cpu/render.cpp#L276&quot;&gt;BVHs for object
instances&lt;/a&gt;
are created in parallel, since they’re all independent.  This is all
opportunistic parallelism—&lt;i&gt;for&lt;/i&gt; loops over independent items that can be
processed concurrently.  It doesn’t scale well if there are only a few
items to loop over and it’s susceptible to load imbalance, but it’s
something, and something’s worth taking if it’s easy to do so.&lt;/p&gt;

&lt;p&gt;The BVH construction code has also been &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/b44bc261e52accffc1eb8b0da9a804d38319538d/src/pbrt/cpu/aggregates.cpp#L366&quot;&gt;(slightly)
parallelized&lt;/a&gt;:
sub-trees are built in parallel when there are many primitives.  This isn’t
the state of the art in parallel BVH construction, but it, too, is
something.&lt;/p&gt;

&lt;p&gt;Given those improvements and 64 threads, pbrt-v4 does better; here is a
graph of CPU usage over time until rendering begins.  Note that this graph
has the same scale as the earlier one, so we can directly see how much time
to first pixel has been reduced—it’s about 115 seconds less.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-64threads.svg&quot; alt=&quot;CPU utilization with 64 threads&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The big wins come from instance BVH construction and creating the final
top-level scene-wide BVH, which are sped up by factors of 4.2x and 2.4x,
respectively.  Neither is an exemplar of ideal parallel speedup, but again,
it’s not bad for not much work.&lt;/p&gt;

&lt;h3 id=&quot;parsing-in-parallel&quot;&gt;Parsing in parallel&lt;/h3&gt;

&lt;p&gt;It is evident from that graph that parsing performance must be improved in
order to make a meaningful further reduction in time to first pixel—with
BVH construction performance improved, parsing is about 5/6 of the total
time.  After a bit of thought, I realized that pbrt-v4’s new approach to
parsing and scene construction also offered the opportunity to parse the
scene description in parallel.&lt;/p&gt;

&lt;p&gt;For context, pbrt has always offered an &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt; directive in its scene
description files; it corresponds to &lt;code class=&quot;highlighter-rouge&quot;&gt;#include&lt;/code&gt; in C/C++ and is
semantically the same as expanding the text of the file inline at the point
where it is included.  This is a handy capability to have, but it
effectively requires serial processing of included files.  For example, if
one first &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt;s a file that has&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Material &quot;conductor&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;and then &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt;s a file that has&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Shape &quot;trianglemesh&quot; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;then the triangle mesh will have the “conductor” material applied to it.&lt;/p&gt;

&lt;p&gt;While one could perhaps imagine a more sophisticated implementation of
&lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt; that allowed parsing files in parallel and then patching things
up afterward, I decided to add a new directive, &lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;.  It’s the same
idea as &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt;—parse the given file and then add the stuff described in
it to the scene description—but its semantics are different.  While it
inherits the current graphics state—the current material, transformation
matrix, and so forth—at the start of its parsing, changes to the graphics
state do not persist afterward.  However, the shapes, lights, object
instances, and participating media that are specified in the file are added
to the scene.  In practice, most uses of &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt; can be replaced with an
&lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Thanks to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ParsedScene&lt;/code&gt; representation, we can &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/e643f698589562e44e36721f5893ba97249a71ef/src/pbrt/parser.cpp#L749&quot;&gt;kick off a new thread
to
parse&lt;/a&gt;
each &lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;ed file , have it &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/e643f698589562e44e36721f5893ba97249a71ef/src/pbrt/parsedscene.cpp#L448&quot;&gt;initialize a separate
&lt;code class=&quot;highlighter-rouge&quot;&gt;ParsedScene&lt;/code&gt;&lt;/a&gt;,
and then &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/e643f698589562e44e36721f5893ba97249a71ef/src/pbrt/parsedscene.cpp#L461&quot;&gt;merge that one
in&lt;/a&gt;
with the main &lt;code class=&quot;highlighter-rouge&quot;&gt;ParsedScene&lt;/code&gt; &lt;a href=&quot;https://github.com/mmp/pbrt-v4/blob/e643f698589562e44e36721f5893ba97249a71ef/src/pbrt/parser.cpp#L981&quot;&gt;when the thread has finished
parsing&lt;/a&gt;.
It’s a hundred or so lines of code in the end.&lt;/p&gt;

&lt;p&gt;Turning to the Moana scene, Disney’s original pbrt conversion of it has a
top-level file, &lt;code class=&quot;highlighter-rouge&quot;&gt;island.pbrt&lt;/code&gt;, that then has 20 &lt;code class=&quot;highlighter-rouge&quot;&gt;Include&lt;/code&gt; statements
for the main parts of the scene: the geometry of the mountains, the ocean
surface, the beach dunes, the various hero trees, and so forth.  All of
those can safely be brought in using &lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With that simple change, parsing is 3.5x faster and time to first pixel is
down to 123 seconds. Progress!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-import-top.svg&quot; alt=&quot;CPU utilization with top-level Import statements&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Parsing time has greatly improved, though for most of that phase only four
threads are running, trailing down to a single thread for the last few
seconds.  We have a good old load imbalance, where most of the imported
files are parsed quickly but then getting through the four heaviest ones is
the bottleneck.&lt;/p&gt;

&lt;p&gt;Each of those four has a ~5GB pbrt file to be parsed along the way.  I went
ahead and manually split each of those into 10 or so smaller files that are
themselves loaded via &lt;code class=&quot;highlighter-rouge&quot;&gt;Import&lt;/code&gt;.  With that, parsing has sped up by a total
of 6.1x and we’re down to 97 seconds of time to first pixel.  If 64 threads
are giving a 6.1x speedup, one might think that fewer cores might do well.
It is so: with 16 cores, it’s 124 seconds to first pixel, and with 8, it’s
149.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/matt/blog/images/ttfp-import-deep.svg&quot; alt=&quot;CPU utilization with multiple levels of Import statements&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The second half of the parsing phase is still just a few CPU cores chugging
along, but I ran out of motivation to keep manually splitting up the big
files; that’s the sort of thing that would ideally be done automatically by
an exporter anyway.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;As we wrap up now, pbrt-v4 is 21.6x faster in time to first pixel for the
Moana Island scene than pbrt-v3 was originally and 4.2x faster than it was
where things stood at the start of this write-up, all of that additional
improvement due to parallelism.  That’s satisfying progress, and I have to
admit that seeing 30 threads all simultaneously working on parsing the
scene description is a thrill; I wouldn’t have expected any parallelism in
that phase a few weeks ago.&lt;/p&gt;

&lt;p&gt;That said, even in the final graph there’s a lot more area above the curve
where CPUs are idle than there is below where they’re getting things done.
Eyeballing it, I’d guess that if the 64 CPU threads were used to their full
capabilities, it might be possible to have a 20 second time to first pixel.&lt;/p&gt;

&lt;p&gt;Getting to that point would require much more complexity in pbrt’s
implementation, however: it would likely end up kicking off work as soon as
it was available (“start creating this light in a separate thread”, etc.).
Having all of the object creation happening concurrently with parsing would
also introduce the risks of ugly bugs due to race conditions, which I’m not
sure is worth it, especially for a system with pbrt’s goals.  Therefore,
we’ll stop there for time to first pixel, at least for the moment.&lt;/p&gt;

&lt;p&gt;To wrap up these updates, next time we’ll look at pbrt-v4’s performance
rendering this scene, focusing on the GPU rendering path.&lt;/p&gt;</content><author><name></name></author><summary type="html">Digging into where all the time goes when parsing the Moana Island scene and getting ready to start rendering it.</summary></entry></feed>
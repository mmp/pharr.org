<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="icon" type="image/png" href="/matt/favicon.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Sampling in Floating Point (1/3): The Unit Interval</title>
  <meta name="description" content="Sampling algorithms can be subtle and tricky, as can be computations that are performed using floating-point arithmetic. This post starts a short series on t...">

  <link rel="stylesheet" href="/matt/blog/assets/main.css">
  <link rel="canonical" href="https://pharr.org/matt/blog/2022/03/05/sampling-fp-unit-interval.html">
  <link rel="alternate" type="application/rss+xml" title="Matt Pharr&#39;s blog" href="/matt/blog/feed.xml">
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/matt/blog/">Matt Pharr&#39;s blog</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
          
            
            
          
        </div>

      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Sampling in Floating Point (1/3): The Unit Interval</h1>
    <p class="post-meta">
      <time datetime="2022-03-05T00:00:00-08:00" itemprop="datePublished">
        
        Mar 5, 2022
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p><em>(The following assumes basic familiarity with the IEEE floating point
representation—sign, power of two exponent, and significand—but 
not necessarily expert-level understanding of it.)</em></p>

<p>Taking samples from various distributions is at the heart of rendering;
perhaps most importantly, it allows us to use importance sampling when
performing Monte Carlo integration, which gives us a powerful tool to
reduce error.  The associated sampling algorithms are generally derived
assuming that real numbers are afoot but are then implemented using
floating-point math on computers.  For sampling, the differences between
reals and floats usually doesn’t cause any problems, though if you look
closely enough there are a few interesting subtleties.  We’ll start this
short series on that topic today with what seems like should be the
simplest of problems: uniformly sampling a floating-point value between
zero and one.</p>

<h2 id="uniform-floats-by-dividing-by-an-integer">Uniform Floats by Dividing by an Integer</h2>

<p>Just about anywhere you look, from <em>Stack Overflow</em> to all four editions of
<em>Physically Based Rendering</em>, you’ll be told that it’s easy to sample a
uniform floating-point value in \([0,1)\): just generate a random \(n\)
bit unsigned integer and divide by \(2^{n}\).  Given real numbers, that’s
fine—the largest value your integer can take is \(2^n-1\) and dividing
by \(2^n\) gives a value that’s strictly less than one.<sup id="fnref:mult"><a href="#fn:mult" class="footnote">1</a></sup> With
32-bit floats (as we will exclusively consider today), there’s a nit: say that
\(n=32\) (as is used in pbrt).  After floating point rounding, one will
find that
\[
\frac{2^{32} - 1}{2^{32}} \rightarrow 1;
\]
so much for that non-inclusive upper bound.
The problem is that the spacing between the floats right below 1 is
\(2^{-24}\).  Because \(2^{-32}\) is much less than half that, \(1-2^{-32}\)
rounds to 1.  Even worse, all 128 floating-point values in \([2^{32}-128,
2^{32}-1]\) round to 1.</p>

<p>pbrt <a href="https://github.com/mmp/pbrt-v4/blob/fd3c25bf1062ab9a790a9ab5fbd4e84d813c2316/src/pbrt/util/rng.h#L129">works around that
problem</a>
by bumping any such 1s down to \(1-2^{-24}\), the last representable
float before 1. That gets things back to \([0,1)\) but it’s a stinkiness
in the code that in retrospect should have led to the algorithm used being
given more attention.</p>

<p>One way to avoid this issue and many of the following is to set \(n=24\),
in which case all values after division are valid float32 values and no
rounding is required.<sup id="fnref:petrik"><a href="#fn:petrik" class="footnote">2</a></sup> However, that gives slightly more than 16
million unique values; that’s a fair number of them, but there are actually
a total of 1,065,353,216 float32 values in \([0,1)\)—nearly a quarter
of all possible 32-bit floats.  Under that lens, those 16 million seem
rather few.</p>

<p>How much better do we do with \(n=32\)?  Although we start out with over
4 billion distinct integer values, if you divide each by \(2^{-32}\) to
generate samples in \([0,1)\) and count how many float32s are generated,
it turns out that those four billion yield only 83,886,081 distinct
floating-point values, or 7.87% of all of the possible ones between zero
and one.  Not only do we have multiple integer values mapping to the same
floating-point value all the way from \(2^{-8}=0.00390625\) to 1, but
between 0 and \(2^{-9}=0.001953125\), the spacing between floats is less
than \(2^{-32}\) and many floating-point values are never generated.</p>

<p>There’s another problem that comes with the choice of
\(n&gt;24\), nicely explained in the paper <a href="https://hal.archives-ouvertes.fr/hal-02427338"><em>Generating Random
Floating-Point Numbers by Dividing Integers: A Case
Study</em></a>, by <a href="https://frederic.goualard.net">Frédéric
Goualard</a>.  When the usual
round-to-nearest-even is applied after dividing by \(2^{32}\), a systemic
bias is introduced in the final values, clearly shown in that paper with
examples that use low floating-point precision.  Thus, it’s not just “we’re
not making the most of what we’ve been given”, but it’s “the distribution
isn’t actually uniform.”</p>

<p>The rounding problem is still evident with float32s and \(n=32\) bits; if
we consider all \(2^{32}\) floating-point values, we would expect for
example that all floats in \([0.5,1)\) would be generated the same number
of times.  (Indeed, we would expect 256 of each since we have \(2^{32}\)
values, the float32 spacing in that interval is \(2^{-24}\), and
\(2^{32} \cdot 2^{-24} = 256\).)  However, if we count them up, it turns
out that alternating floating-point values are generated 255 times and 257
times, all the way from 0.5 to 1.  That happens in many other intervals,
becoming its worst in the interval \([0.00390625, 0.0078125)\) where
alternating values are generated one and three times.<sup id="fnref:rounding"><a href="#fn:rounding" class="footnote">3</a></sup></p>

<p>Depending on one’s application, all of these issues may be no problem in
practice, and I wouldn’t make the argument that they are likely to cause
errors in rendered images.  Most of the time \(n=24\) and not worrying
about it is probably fine.  Yet IEEE has given us all that precision and it
seems wasteful not to make use of it, if it isn’t too much trouble to do
so…</p>

<h2 id="uniform-floats-by-sampling-a-geometric-distribution">Uniform Floats by Sampling a Geometric Distribution</h2>

<p>What might be done about these problems?  A remarkably elegant and
efficient solution dates to 1974 with Walker’s paper <a href="https://www.semanticscholar.org/paper/Fast-generation-of-uniformly-distributed-numbers-Walker/71ebd4c11bf15f87918325d92a5b476344b3c7a2"><em>Fast Generation of
Uniformly Distributed Pseudorandom Numbers with Floating-Point
Representation</em></a>,
which is based on the following observation (expressed here in terms of modern
IEEE float32s):</p>
<ul>
  <li>In the interval \([1/2, 1)\), there are exactly \(2^{23}\)
equally-spaced numbers that can be represented
in float32.</li>
  <li>In the interval \([1/4, 1/2)\), there are exactly \(2^{23}\)
equally-spaced numbers that can be represented
in float32.</li>
  <li>In the interval \([1/8, 1/4)\), there are exactly \(2^{23}\)
equally-spaced numbers that can be represented
in float32.</li>
  <li>And so on…</li>
</ul>

<p>We would like an algorithm that can generate all of those numbers but does
so in a way that gives a uniform distribution over \([0,1)\).  Walker
observed that this can be done in two steps: first by choosing an interval
with probability according to its width, and then by sampling uniformly
within its interval. (This algorithm is sometimes credited to Downey, who
seems to have independently derived it in an <a href="https://allendowney.com/research/rand/downey07randfloat.pdf">unfinished
paper</a> from
2007.)</p>

<p>Because each interval’s width is half that of the one above it, sampling an
interval corresponds to sampling a geometric distribution with
\(p=1/2\).  There’s thus an easy iterative algorithm to select an
interval: one can first randomly choose to generate the sample within
\([1/2, 1)\) with probability \(1/2\).  Otherwise, sample within
\([1/4,1/2)\) with probability \(1/2\) and so forth; then bottom out
when you hit the denorms.  Given an interval, the exponent follows and a
sample within an interval can be found by uniformly sampling a significand,
since values within a given interval are equally-spaced.</p>

<p>Choosing an interval in that way takes only two iterations in expectation,
but the worst case requires many more.  The associated execution divergence
is especially undesirable for processors like GPUs.  Walker had another
trick up his sleeve, however:</p>

<blockquote>
  <p>Pseudorandom integer numbers with a truncated geometric distribution may
be obtained by counting consecutive 1s or 0s in a binary random number,
drawn from a set having a uniform frequency distribution.</p>
</blockquote>

<p>In other words, generate a random binary integer and, say, count the number
of leading zero bits.  Use that count to choose an interval, where zero
leading zero bits has you sampling in \([1/2,1)\), one leading zero bit
puts you in \([1/4,1/2)\), and so forth.  Given an index \(i\) into the
intervals that starts at 0, the exponent is then \(-1 - i\).  Modern
processors offer bit counting instructions that yield such counts, so this
algorithm can be implemented very efficiently.</p>

<h2 id="from-theory-to-implementation">From Theory to Implementation</h2>

<p>With float32, the floating-point exponent factors over the \([0,1)\)
interval go from \(2^{-1}\) down to \(2^{-126}\) before the denorms
start.  Thus, 128 random bits may be required to choose the interval.
However, those intervals start becoming so small that one’s commitment to
possibly sampling every possible float might start to waver; the odds of
making it to one of the tiny ones becomes vanishingly small.</p>

<p>A <a href="http://marc-b-reynolds.github.io/distribution/2017/01/17/DenseFloat.html">blog
post</a>
by Marc Reynolds has all sorts of good insights on the efficient
implementation of this algorithm.  (More generally, <a href="http://marc-b-reynolds.github.io">Marc’s
blog</a> is full of great sampling and
floating-point content; highly recommended.)  He considers multiple
approaches (for example, successively generating as many random 32-bit
values as needed) and ends with a <a href="http://marc-b-reynolds.github.io/distribution/2017/01/17/DenseFloat.html#the-parts-im-not-tell-you">pragmatic
compromise</a>
that takes a single 64-bit random value, uses 41 bits to choose the
exponent, and uses the remaining 23 bits to sample the significand.  The
remaining \([0,2^{-40})\) interval is sampled uniformly.  As long as an
efficient count leading zeros instruction is used, it’s only slightly more
work than multiplying by \(2^{-32}\) and clamping; in practice, most of
the extra expense comes from needing to generate a 64-bit pseudorandom
value rather than just a 32-bit one.</p>

<!--
// Given uniform 64-bit integer 'u' return a uniform float on [0,1)
// * the interval [2^-40, 1) is dense (all representable values produced)
// * the interval [0, 2^-40) is equidistantly (2^-64) populated

float rng_pdense_f32(uint64_t u)
{
  uint32_t z = lzc_64(u);                   // change to (u|1) for intel pre LZCNT hardware

  if (z <= 40) {
    uint32_t e = 126-z;                     // compute the biased exponent
    uint32_t m = ((uint32_t)u) & 0x7fffff;  // explict significand bits
    return f32_from_bits(e<<23|m);          // construct the binary32
  }
  
  // The probabilty of reaching here is 2^-40. There are as many points
  // on this subinterval as the standard equidistance method produces
  // across the entire output range.

return 0x1.0p-64f*(float)((uint32_t)u);
}
-->

<h2 id="conclusion">Conclusion</h2>

<p>Unless you’re bottlenecked on sample generation, it’s worth considering
using an efficient implementation of Walker’s algorithm to generate
uniform random floating-point numbers over \([0,1)\).  It’s not much more
computation than the usual, it makes the most of what floating point
offers, and it eliminates a minor source of bias.  Plus, you get to
exercise the bit counting instructions and feel like that much more of a
hacker.</p>

<p>Next time we’ll look at uniformly sampling intervals of floating point
numbers beyond \([0,1)\).  After that, on to how low-discrepancy sampling
interacts with some of the topics that came up today as well as some
discussion about avoiding an unnecessary waste of precision when sampling
exponential functions.</p>

<h3 id="notes">notes</h3>
<div class="footnotes">
  <ol>
    <li id="fn:mult">
      <p>In practice, one multiplies by \(2^{-32}\) since dividing by a power of two and multiplying by its reciprocal give the same result  with IEEE floats. <a href="#fnref:mult" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:petrik">
      <p>If I remember correctly, Petrik Clarberg explained the superiority of \(n=24\) over \(n=32\) in this context to me a few years ago; it’s a point that I underappreciated at the time. <a href="#fnref:petrik" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:rounding">
      <p>On a processor where changing the rounding mode is inexpensive, it is probably a good idea to select rounding down in this case. For example, in CUDA, the multiplication by \(2^{-32}\) might be performed using <code class="highlighter-rouge">__fmul_rd()</code>. <a href="#fnref:rounding" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Matt Pharr&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li><a href="/matt/">homepage</a></li>
          
            <li><a href="mailto:matt.pharr@gmail.com">matt.pharr@gmail.com</a></li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>It seemed worth writing up at the time.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
